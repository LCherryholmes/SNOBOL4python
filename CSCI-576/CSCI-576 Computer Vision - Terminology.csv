Term,Definition,Example
3D pose estimation,"Determining an object or person’s spatial orientation and position in three dimensions from one or more images. It’s key for applications like motion capture, robotics, and AR.",Estimating a human skeleton’s joint angles from a single webcam frame for gesture control.
3D reconstruction,Rebuilding a three-dimensional model of a scene or object using multiple 2D images or depth maps. Common techniques include structure-from-motion and stereovision.,Creating a textured mesh of a statue from dozens of photos taken around it.
activity recognition,"Automatically identifying human actions (e.g., walking, jumping) from video sequences or sensor streams. It combines spatio-temporal feature extraction with classification models.",Flagging a person raising their hand in a security camera feed.
actual intensity,"The true, physical brightness value of a scene point before quantization or processing. It differs from perceived or camera-adjusted intensity and often serves as a ground-truth reference.","The raw sensor reading (e.g., 0.35 cd/m²) measured by a calibrated photometer."
AlexNet,"A landmark convolutional neural network that won the 2012 ImageNet challenge, reigniting deep learning for vision. It has five convolutional layers followed by three fully connected layers and uses ReLU activation.","Loading pretrained AlexNet in PyTorch to classify an image into one of 1,000 categories."
aliasing,Artifacts like jagged edges or moiré patterns that occur when high-frequency image details are undersampled. Prevented by antialiasing filters or by capturing at higher resolution.,Noticing “stair-step” edges on diagonal lines when downscaling an architectural blueprint.
analog image,"A continuous, non-digital representation of visual information—examples include film photographs or CRT displays. Intensities vary smoothly rather than in discrete levels.",A silver-halide print developed from a 35 mm camera negative.
Apollo missions,NASA’s lunar exploration programs (1961–1972) that captured high-resolution analog photographs of the Moon’s surface. These images were later digitized for scientific analysis.,Scanning Apollo 11 surface photos at high resolution for crater-size measurement.
artificial intelligence,"The discipline of creating machines or algorithms capable of tasks that normally require human intelligence, such as perception, reasoning, and learning. In vision, AI powers object detection, classification, and more.",Deploying a neural-network-based model to detect faces on a smartphone.
artificial light,"Man-made illumination—LEDs, fluorescents, incandescent bulbs—that affects color balance and exposure. Image-processing pipelines often include white-balance and exposure‐correction to compensate.",Auto-adjusting white balance when shooting indoors under tungsten lamps.
artistic effects,"Stylization techniques that transform photographs into “painterly” or “sketch” looks by combining edge detection, texture synthesis, and color remapping.",Applying neural style transfer so a portrait resembles a Van Gogh painting.
augmented reality,"Overlaying computer-generated graphics onto live camera views in real time, tightly registered with the real world. It relies on robust pose estimation and tracking.",Viewing 3D furniture models in your room through a smartphone AR app.
auto-detected features,"Interest points (corners, blobs) and descriptors (SIFT, ORB) that are automatically located and encoded for tasks like matching, tracking, and stitching.",Extracting ORB keypoints on two overlapping images to compute their homography.
autoencoders,"Neural nets that learn to compress inputs into a low-dimensional code and then reconstruct them, useful for denoising, dimensionality reduction, and anomaly detection. They consist of encoder and decoder parts.",Training an autoencoder to remove Gaussian noise from face images in PyTorch.
autonomous navigation,"The process by which a vehicle or robot plans and follows a path in its environment without human intervention, using sensor data (e.g., lidar, cameras, IMUs) and mapping algorithms. It continuously localizes itself and adjusts to dynamic obstacles.",A self-driving car using LiDAR point clouds and GPS to steer through city streets while avoiding pedestrians.
autonomous perception,"The suite of computer-vision and sensor-fusion techniques that enable a system to interpret its surroundings—identifying objects, estimating depth, and recognizing semantic context—without outside guidance.",A drone using onboard stereo cameras to detect and classify obstacles in real time.
autonomous system,"An integrated platform capable of sensing, reasoning, and acting in the real world entirely on its own, often combining navigation, perception, and decision-making modules.","An unmanned delivery robot that maps sidewalks, avoids hazards, and completes routes without human control."
average smoothing filters,"These filters replace each pixel’s value with the arithmetic mean of its neighbors, yielding a simple blur effect that reduces random noise. They are linear and easy to compute, but tend to smooth out edges and fine details.",Applying a 3×3 mean mask to reduce sensor noise in a low-light photograph.
Bartlane service,An early wire-photo (facsimile) transmission service developed in the 1920s that sent newspaper photos over telegraph and radio lines using the Bartlane phototelegraph process.,The Associated Press transmitting a front-page photograph via Bartlane service to distant newspapers.
Bartlane system,"The electro-mechanical apparatus invented by Edward Bartlane for scanning, encoding, and reconstructing images line by line over telegraph circuits, pioneering modern fax technology.",A 1930s Bartlane machine converting a photographic portrait to electrical signals and reassembling it hundreds of miles away.
binary image,"A digital image where each pixel can take only two possible values, typically 0 (background) or 1 (foreground), used to represent shapes or masks.",The silhouette of a scanned document where text pixels are white (1) and background is black (0).
bit depth,The number of bits used to encode the intensity of each pixel in an image; it determines the total number of distinct grey or color levels that can be represented.,An 8-bit grayscale image with bit depth = 8 offers 256 distinct intensity levels (0–255).
bit plane slicing,"Bit plane slicing isolates one or more binary bit-planes of an image, revealing how each bit contributes to overall appearance. By extracting high-order bits you highlight coarse features, while lower-order bits show fine detail and noise.",Extracting the 7th bit-plane of an 8-bit image to emphasize major intensity structures.
black & white,"A monochrome image where pixel intensities range continuously between black and white, but no color channels are present; also called a grayscale image.",A portrait photograph converted to shades of gray for artistic effect.
blind convolution,A convolution operation in which the filter (point-spread function) is unknown and must be estimated jointly with the underlying image—common in blind-deblurring tasks.,Restoring a motion-blurred photo by alternately estimating the blur kernel and the sharp image.
blurring,"The process of smoothing an image by attenuating its high-frequency components, resulting in a softening of edges and fine detail; often achieved with linear or nonlinear filters.",Applying a 5×5 Gaussian filter to remove sensor noise in a low-light photo.
blurring filters,"Blurring filters attenuate high-frequency content so that rapid intensity changes become smoother. Common kernels include box, Gaussian, and average; each yields different degrees and types of blur.",Convolving an image with a Gaussian kernel (σ=1.5) to remove background texture.
BMP file type,"A raster graphics file format (Bitmap) developed by Microsoft that stores uncompressed pixel data along with metadata (dimensions, bit depth), retaining full image quality at the expense of large file size.",Saving a Windows desktop screenshot as a .bmp file to preserve exact pixel values for later editing.
brightness adaptation,"The automatic adjustment of a camera’s exposure settings or a display’s tone mapping to cope with changes in ambient illumination, ensuring that scene details remain visible in highlights and shadows.",A smartphone camera dynamically changing shutter speed and ISO when moving from indoors to bright sunlight.
brightness discrimination,The capability of an observer (biological or algorithmic) to detect small differences in luminance between two adjacent regions; it underlies thresholding and contrast sensitivity studies.,"In a vision test, determining whether a 0.5% increase in patch luminance on a gray background is perceptible."
Butterworth lowpass filter,"A Butterworth lowpass filter provides a smooth, monotonic response in frequency domain with no ripples in the passband. Its roll-off rate is controlled by the filter order, balancing sharp cutoff and ringing artifacts.",Designing a 2nd-order Butterworth with cutoff D₀=50 to blur out details finer than that scale.
C/C++,"C and C++ are high-performance programming languages frequently used in image processing and computer vision for their low-level memory control and efficient execution. C provides procedural constructs, while C++ adds object-oriented abstractions and a rich standard library.",Writing a real-time Sobel edge detector in OpenCV using C++ for direct pixel access and multithreading.
candela,"The candela (cd) is the SI unit of luminous intensity, measuring the light emitted by a source in a specific direction per unit solid angle. It’s fundamental for calibrating camera exposure and display brightness in imaging systems.",Setting a camera’s exposure parameters based on an LED light source rated at 800 cd to achieve correct scene illumination.
Canny,"Often shorthand for the Canny edge detection algorithm, this term refers to a multi-stage process—Gaussian smoothing, gradient computation, non-maximum suppression, and hysteresis thresholding—that yields robust, well-localized edges with minimal spurious responses.","Using OpenCV’s Canny(src, dst, 50, 150) to extract crisp edges from a grayscale photograph."
Canny edge detector,"A benchmark edge-detection algorithm that first smooths an image with a Gaussian filter, computes gradient magnitudes and directions, applies non-maximum suppression to thin edges, then uses dual thresholds with connectivity analysis to finalize edge pixels.","Detecting road lane markings in dash-cam footage by applying Canny with σ=1.4, low threshold=100, high threshold=200."
CAT scan,"Computed tomography (CT) scans acquire multiple X-ray projections around a subject and reconstruct cross-sectional images, providing volumetric data for medical image analysis, segmentation, and 3D visualization.",Segmenting a CT scan of the chest to isolate lungs from surrounding tissue using thresholding and region growing.
CCD arrays,"Charge-coupled device arrays are light-sensitive sensors that accumulate and transfer charge across pixel elements, offering high sensitivity and low noise, especially in low-light applications like astronomy and fluorescence microscopy.",Capturing long-exposure astrophotographs with a cooled CCD camera to minimize thermal noise and reveal faint stars.
"CDF, cumulative distribution function",The CDF of an image histogram gives the cumulative probability that a pixel’s intensity is less than or equal to a given value. It underpins histogram equalization by mapping input intensities to output levels for contrast adjustment.,Building the CDF from a grayscale image to compute equalized output levels.
ciliary muscles,"In the human eye, ciliary muscles adjust the lens curvature to focus on objects at varying distances; understanding their behavior informs autofocus algorithms and blur modeling in camera systems.",Simulating defocus blur in a camera model by varying lens focal length analogously to ciliary muscle action.
class labels,"Discrete identifiers assigned to images or regions during supervised learning that denote the category or object present; they guide model training, evaluation, and performance metrics.","Labeling images in a dataset as “cat,” “dog,” or “bird” to train a convolutional neural network for classification."
class targets,"Ground-truth outputs used in training classification models, often encoded as one-hot vectors or integer indices corresponding to class labels; they drive loss calculation and backpropagation.","Encoding the label “car” as the one-hot vector [0,1,0,…,0] when computing cross-entropy loss in a neural network."
clustering,"An unsupervised technique that groups data points (pixels, feature vectors) into clusters based on similarity metrics, enabling tasks like color quantization and image segmentation without pre-labeled data.",Applying K-means clustering to the RGB values of an image to reduce it to its eight dominant colors for stylization
CMOS arrays,"Complementary metal-oxide-semiconductor (CMOS) arrays are solid-state image sensors in which each pixel has its own charge-to-voltage conversion and read-out circuitry. They offer lower power consumption and higher frame rates than CCDs, at the cost of slightly more fixed-pattern noise.",A smartphone camera module using a 48 MP CMOS sensor to shoot 4K video at 60 fps.
CMY,"CMY stands for cyan, magenta, and yellow, the subtractive primary colors used in printing and color reproduction. Mixing these inks on white paper subtracts red, green, and blue light, respectively, to form a full-color image.",Converting a digital RGB photograph to CMY separations for previewing magazine printing plates.
CMY color space,"The CMY color space represents colors by their cyan, magenta, and yellow components, each varying from 0 to 100 %. It’s a direct complement to RGB, used in prepress and commercial printing workflows.",Software converting sRGB images to CMY values so that a proof sheet matches printed output.
CMYK,CMYK adds a key (black) channel to the CMY model to improve depth and reduce ink usage. The black ink enhances shadows and detail without relying solely on combinations of three colored inks.,Feeding an RGB image into a print driver that outputs four separate CMYK plates for an offset press.
CMYK color space,"The CMYK color space specifies color as percentages of cyan, magenta, yellow, and black, facilitating precise ink-laydown control. It’s essential for color calibration in the printing industry.",Adjusting the black channel in Adobe Photoshop’s CMYK preview to prevent muddy shadows in a magazine ad.
CNNs,"Convolutional Neural Networks (CNNs) are deep learning architectures that use convolutional layers to automatically learn spatial hierarchies of features. They excel at vision tasks like classification, detection, and segmentation by sharing weights and reducing parameters.",Training a CNN in TensorFlow to classify images of dogs and cats with over 90 % accuracy.
COCO,"COCO (Common Objects in Context) is a large-scale dataset for object detection, segmentation, and captioning, containing over 330 K images with 80 object categories. It emphasizes complex everyday scenes with multiple, overlapping objects.",Fine-tuning a Mask R-CNN model on the COCO dataset to segment people and vehicles in street photographs.
color adjustment,"Color adjustment encompasses operations that modify hue, saturation, brightness, or contrast to correct or stylize an image. Techniques include white-balance correction, gamma adjustment, and channel mixing.",Increasing saturation by 20 % and shifting hue by +10° in Lightroom to make a sunset photo more vibrant.
color image processing,"Color image processing refers to algorithms that operate on multi-channel data, handling color conversions, enhancement, and analysis. It often involves transforming between color spaces (e.g., RGB→HSV) to simplify tasks like segmentation or white balance.",Converting an RGB satellite image to the Lab color space and equalizing its L channel to enhance land-water contrast.
color model,"A color model is a mathematical framework for representing colors as tuples of values—common examples are RGB, HSV, Lab, and CMYK. Each model has a specific geometry and use case, such as device display or human perception emulation.",Converting images to HSV to isolate hue values when detecting green vegetation in aerial surveys.
color shade,"A color shade is produced by adding black to a pure hue, resulting in darker variations while preserving the original hue’s character. Shades are used to create depth and contrast in imagery and design.",Generating darker navy shades by blending black into a base blue swatch for UI elements.
color space,"A color space defines a specific gamut and coordinate system (e.g., sRGB, Adobe RGB, ProPhoto RGB) for representing colors digitally or physically. It dictates how numerical triplets map to actual colors.",Saving an image in the Adobe RGB color space to preserve a wider gamut for professional printing.
color tint,"A color tint is produced by adding white to a pure hue, yielding lighter, pastel variations. In digital editing, tinting can also refer to applying a uniform hue overlay with adjustable opacity.","Applying a 20 % blue tint layer over a portrait to evoke a cool, moody atmosphere."
color tone,"Color tone describes the overall warmth or coolness of an image, influenced by hue and intensity distributions. Tone adjustments shift color balance to create mood or correct lighting imbalances.",Shifting midtones toward orange in Photoshop’s Color Balance tool to add a warm “golden hour” look.
component image,"A component image is one channel extracted from a multi-channel image, such as the R, G, or B plane of an RGB image, or the L, a, and b planes of Lab. It isolates specific information for targeted processing.",Extracting the V (value) channel from an HSV image to perform brightness-based thresholding.
compositing,"Compositing is the process of combining multiple images or layers into a single, cohesive frame using blending modes, masks, and alpha channels. It’s fundamental in VFX, photography, and UI design.",Overlaying a PNG of a rendered 3D car onto a background photo with alpha blending and shadow insertion.
Computer Vision,"Computer Vision is the scientific field focused on enabling machines to interpret and analyze visual data, extracting information such as shapes, motions, and semantics. It spans image processing, pattern recognition, and high-level reasoning.",Deploying a face-recognition system at airport security gates to authenticate passengers.
Computer Vision Toolbox,"Computer Vision Toolbox is a MATLAB add-on providing algorithms and tools for designing and testing vision systems, including feature detection, camera calibration, and 3D reconstruction.",Using the detectMSERFeatures function from Computer Vision Toolbox to segment text regions in scanned documents.
cones,"Cones are photoreceptor cells in the retina that respond to different wavelengths of light (short-, medium-, and long-wave), enabling color vision and high-acuity central vision under bright conditions.",Modeling the human LMS cone responses to simulate perceptual color shifts under different illuminants.
continuous,"In imaging, continuous refers to signals or intensity values that vary smoothly over space or time, such as analog photographs or continuous-tone images with infinite resolution.",Analyzing an analog film scan that preserves a continuous tonal range before quantization.
contrast,"Contrast is the difference in luminance or color that makes objects distinguishable; it can be measured globally (max–min range) or locally (e.g., local standard deviation). High contrast improves feature visibility.",Applying local contrast enhancement (CLAHE) to boost detail in foggy landscape photos.
contrast stretching,"Contrast stretching linearly expands the intensity range of an image to span the full available dynamic range. This scaling makes dark zones darker and bright zones brighter, improving visual separation of features.","Stretching intensities in [50,200] to fill [0,255], boosting mid-tone contrast."
convolution,"Convolution computes a weighted sum of pixel values in a sliding window defined by a kernel, producing filters such as blur, sharpen, or edge detection. It is the fundamental linear operation in spatial-domain image processing.",Applying a Sobel kernel via convolution to detect horizontal edges.
Convolutional Neural Network (CNN),"A Convolutional Neural Network (CNN) is a deep learning model that uses convolutional kernels to learn spatial feature hierarchies. Layers include convolutions, activations, pooling, and fully connected stages for tasks like classification or detection.",Implementing a CNN in Keras with three convolutional blocks for handwritten digit recognition.
correlation,"Correlation measures the similarity between an image region and a template by sliding the template over the image and computing dot products. Unlike convolution, it does not flip the kernel, and is often used for pattern and feature matching.",Matching a known symbol template to locate appearances within a scene.
CT scan,"A CT (computed tomography) scan captures cross-sectional X-ray images around a subject and reconstructs 3D volumes, widely used in medical diagnostics for bones, organs, and vascular structures.",Using a CT scan to generate a 3D volumetric model of a patient’s skull for surgical planning.
CUDA,"CUDA is NVIDIA’s parallel computing platform and API for harnessing GPU acceleration in general-purpose computing, notably speeding up large matrix operations in image and signal processing.",Running a custom CUDA kernel to accelerate convolutional filtering on 4K video frames in real time.
CV,"CV commonly abbreviates Computer Vision, the field that develops methods for machines to “see.” It also refers to libraries like OpenCV that provide vision and image-processing functions.",Using OpenCV (cv2 in Python) to detect faces in a webcam stream.
DC component,"The DC component of an image is its zero-frequency term in the Fourier domain, equal to the image’s average intensity. It appears as the central value in a shifted frequency spectrum.",Reading the center value of a 2D FFT (after shift) to measure overall brightness.
deblurring,"Deblurring aims to reverse blur artifacts caused by motion, defocus, or atmospheric turbulence by estimating and removing the underlying point-spread function. Techniques include Wiener filtering, blind deconvolution, and regularized inversion.",Applying non-blind Wiener deconvolution with a known Gaussian blur kernel to restore a shaky photograph.
decision trees,"Decision trees are hierarchical, tree-structured classifiers that split data based on feature thresholds, producing interpretable if-then rules. They’re used for pixel classification and coarse scene segmentation.","Using a decision tree to classify each pixel in a thermal image as “hot,” “warm,” or “cold.”"
deep learning,"Deep learning is a subfield of machine learning that trains multi-layer neural networks to automatically learn hierarchical feature representations from data. It powers state-of-the-art methods in vision, speech, and language.",Training a U-Net architecture for medical image segmentation from scratch using volumetric CT data.
deep learning pipeline,"A deep learning pipeline encompasses stages from data collection and annotation through preprocessing, model training, validation, and deployment. It ensures reproducibility and scalability of DL projects in production.","Building a pipeline that augments images, trains a Mask R-CNN on AWS GPU instances, and exports the model for edge use."
deep learning-based techniques,"Deep learning–based techniques refer to vision algorithms that rely on neural network architectures—CNNs, RNNs, Transformers—to learn features end-to-end, replacing handcrafted methods.",Using a YOLOv5 network for real-time object detection in video surveillance.
deep neural network,"A deep neural network is any neural model with multiple hidden layers enabling complex, non-linear feature extraction. Depth increases representational capacity but demands careful regularization and data.",Fine-tuning a 152-layer ResNet model pretrained on ImageNet for fine-grained flower species classification.
denoising,"Denoising removes unwanted noise from images while preserving edges and details. Approaches include spatial filters (median, bilateral), transform-domain methods (wavelet shrinkage), and DL models (DnCNN, Noise2Void).",Applying a pretrained DnCNN model to suppress Gaussian noise in low-light smartphone photos.
DenseNet,"DenseNet is a CNN architecture where each layer connects to every other layer in a feed-forward fashion, improving gradient flow and parameter efficiency. It has shown strong performance on classification and segmentation benchmarks.",Training a DenseNet-121 model in PyTorch to classify chest X-rays into normal and pathological categories.
DenseNet-201,"DenseNet-201 is a DenseNet variant with 201 layers, offering deeper feature extraction and often higher accuracy at the cost of increased training time and memory.",Benchmarking DenseNet-201 vs. DenseNet-121 on the CIFAR-100 dataset to compare accuracy and inference speed.
"DFT, discrete Fourier transform","The DFT decomposes a discrete image into a sum of sinusoids at discrete spatial frequencies, revealing periodic patterns and enabling frequency-domain filtering. It converts an M×N image into an M×N complex spectrum.",Computing fft2 of an image in MATLAB to inspect its texture frequencies.
digital image,"A digital image is a discrete two-dimensional array of pixel values, each representing a sampled intensity or color. It results from quantizing an analog scene or synthetic generation.","A 1024×768 JPG photo on your computer, where each pixel stores three 8-bit color components."
Digital Image Processing,"Digital Image Processing (DIP) encompasses algorithms and techniques for enhancing, analyzing, and transforming digital images, spanning point operations, spatial filtering, frequency analysis, and machine vision.",Using DIP methods like histogram equalization and edge detection to preprocess inputs for an OCR system.
DIP,"DIP is the common abbreviation for Digital Image Processing, the field that studies processing digital pictures by computer.",Enrolling in a university course titled “DIP: Digital Image Processing” covering Fourier transforms and filters.
discrete,"Discrete refers to values or signals defined only at separate, distinct points—e.g., pixels in an image or samples in time—rather than continuously.",Representing a photograph as a 512×512 grid of discrete intensity samples rather than a continuous function.
discrete quantity levels,Discrete quantity levels are the finite set of numeric values that a signal sample—such as a pixel—can assume after quantization. The number of levels is 2^bitDepth.,An 8-bit sensor produces 256 discrete grey levels (0–255) for each pixel.
discrete time intervals,"Discrete time intervals denote uniform sampling instants in time for temporally varying signals, such as video frames or line scans.","Capturing a high-speed event at 1,000 frames per second, each frame representing a uniform discrete time step."
DMSP instruments,"Defense Meteorological Satellite Program (DMSP) instruments, such as the Operational Linescan System (OLS), capture low-light images of Earth’s surface at night for meteorology and intelligence.",Analyzing DMSP-OLS night-light data to estimate urban population distribution.
DPI,"DPI (dots per inch) measures spatial printing or scanning resolution, defining how many individual dots or samples fit into one inch. Higher DPI yields finer detail at the cost of larger file sizes or slower scanning.",Setting a scanner to 600 DPI to digitize a printed photograph with high fidelity.
driverless cars,"Vehicles that autonomously perceive their environment, plan routes, and control motion using sensors (cameras, LiDAR, radar) and AI algorithms. They continuously fuse data to detect lanes, obstacles, and traffic signs without human intervention.",A Waymo car using deep learning and sensor fusion to navigate urban streets and avoid pedestrians.
drizzling technique,"A reconstruction method in astronomy that combines multiple undersampled, dithered images by remapping each exposure onto a finer output grid. It preserves spatial resolution and improves signal-to-noise by weighting and interlacing pixel data.",The Hubble Space Telescope applies drizzling to multiple dithered frames to create high-resolution deep-field images.
DSLR,"A digital single-lens reflex camera employs a mirror and prism system to deliver an optical viewfinder image and interchangeable lenses, capturing high-resolution data on a CMOS or CCD sensor. It offers full manual control over exposure, focus, and depth of field.",Shooting RAW landscapes with a Canon EOS 5D Mark IV DSLR and a 24–70 mm lens at f/8 for optimal sharpness.
DSP boards,"Hardware modules containing specialized Digital Signal Processors designed to execute filtering, Fourier transforms, and real-time image-processing pipelines at high throughput. They offload compute-intensive tasks from general-purpose CPUs.",Using a TI TMS320C6678 DSP board to perform real-time convolutional filtering in an industrial vision inspection system.
dynamic range,Dynamic range is the ratio between the highest and lowest measurable pixel intensities. A higher dynamic range allows representation of both very dark and very bright details.,A 12-bit sensor capturing values from 0 to 4095 yields a dynamic range of 4096:1.
dynamic range extension,Techniques that capture or display both very dark and bright scene regions by merging multiple exposures or adapting tone mapping. They prevent saturation in highlights and reveal detail across extreme contrast.,Creating an HDR image by merging bracketed exposures of a sunset to preserve detail in both the sky and shadowed foreground.
edge strength,"Edge strength quantifies the magnitude of intensity change at a pixel, commonly computed via gradient operators. Strong edges correspond to high gradient magnitudes and mark object boundaries.","Calculating the magnitude of the Sobel gradients Gx, Gy to highlight strong edges."
EfficientNet,"A family of CNNs that use a compound scaling method to uniformly scale network depth, width, and resolution, achieving state-of-the-art accuracy with fewer parameters. They balance model size and performance systematically.",Fine-tuning EfficientNet-B0 on a plant-disease dataset to deploy a lightweight classifier on a mobile app.
electro-microscopic image,"An image produced by an electron microscope (SEM or TEM) where a focused electron beam scans or transmits through a specimen, revealing ultrastructural details at nanometer scale. Intensities correspond to electron scattering or transmission.",Imaging a virus using transmission electron microscopy at 100 000× magnification to study capsid structure.
electromagnetic spectrum,"The full range of electromagnetic radiation wavelengths, from gamma rays to radio waves. Different imaging modalities (X-ray, infrared, ultraviolet) exploit specific bands for applications like medical diagnostics and remote sensing.",Using near-infrared (700–900 nm) aerial imagery to assess vegetation health in agriculture.
EmguCV,"A .NET wrapper for OpenCV that lets C# and VB.NET applications call OpenCV functions for image acquisition, processing, and vision tasks without unmanaged code.",Building a C# WinForms app with EmguCV to detect and draw rectangles around faces from a webcam stream.
EPS file type,"Encapsulated PostScript is a vector graphics format containing PostScript code and optional embedded bitmaps, ideal for resolution-independent printing and scaling.",Exporting a company logo as an EPS file to ensure crisp print output at any size in Adobe Illustrator.
eye tracking,"The measurement of gaze direction and fixation points using cameras and infrared illumination. It analyzes pupil and corneal reflections to infer where a user is looking on a display or in a scene, enabling studies of attention and interaction.",Running a usability test where participants’ gaze heatmaps on a website are recorded to optimize page layout.
face recognition,"The process of identifying or verifying individuals by matching detected facial features against a database of known faces. It involves detection, alignment, feature representation, and classification stages.",Unlocking a smartphone with its built-in face recognition system that matches a live scan to stored templates.
face recognition hard features,"Hand-crafted, interpretable facial attributes such as distances between eyes, nose-mouth ratios, and shape metrics, used in classical recognition pipelines. They contrast with deep-learned embeddings.",Calculating inter-pupillary distance and mouth-chin distance as input features for an early eigenfaces classifier.
FAST,"A corner detection algorithm (“Features from Accelerated Segment Test”) that examines circular pixel segments around a candidate point for intensity contrasts, yielding very fast interest point detection suitable for real-time systems.",Detecting FAST keypoints at 500 fps on live video in an augmented reality application to anchor virtual objects.
Faster R-CNN,A two-stage object detector that first proposes regions via a Region Proposal Network (RPN) and then classifies and refines bounding boxes using a CNN. It improved speed and accuracy over earlier R-CNN models.,"Training Faster R-CNN on COCO to detect and localize pedestrians, cars, and bicycles in street scenes."
feature detection,"Algorithms that locate distinctive structures—corners, edges, blobs—in images to serve as anchors for matching, tracking, and recognition. Popular detectors include Harris, FAST, and SIFT.",Using the Harris corner detector to find keypoints for stitching two overlapping photos into a panorama.
feature engineering,"The manual design and creation of informative descriptors from raw data to improve classical model performance. In vision, it spans color histograms, texture measures, and shape attributes before classifier training.",Computing Haralick texture features from image patches to classify types of wood surfaces.
feature extraction,"The process of transforming raw image data into numerical descriptors (keypoint vectors, histograms, CNN embeddings) that compactly capture salient information for tasks like classification and retrieval.",Extracting 256-dimensional SIFT descriptors around detected keypoints as input to a bag-of-visual-words model.
feature extractor,"A module (hand-crafted or learned) that processes images to produce feature vectors representing texture, shape, or semantic embeddings. Examples include HOG, SIFT, and CNN backbones.",Using the convolutional base of ResNet-50 as a pretrained feature extractor in a transfer learning pipeline.
feature matching,"Pairing feature descriptors between two images by comparing distance metrics (Euclidean, Hamming) to establish correspondences. It is essential for panorama stitching, 3D reconstruction, and tracking.",Matching ORB descriptors from two drone images to compute the homography for orthomosaic creation.
feature vectors,"Numeric arrays encoding properties of image regions or entire images, serving as inputs to classifiers, clustering, or retrieval systems. Their dimensionality depends on the descriptor method used.",A 128-dimensional SIFT descriptor vector representing gradient orientations around a keypoint.
features,"Distinctive image characteristics—edges, corners, blobs, textures—that algorithms detect and represent to distinguish objects and patterns. Robust features resist scale, rotation, and illuminance changes.",Extracting SIFT keypoints that remain stable under viewpoint and lighting variations for object recognition.
feed-forward NN,A neural network architecture where information flows in one direction from input through hidden layers to output without loops or feedback. Each layer applies linear weights and nonlinear activations to learn hierarchical mappings.,A three-layer feed-forward network mapping 28×28 pixel inputs to 10 softmax outputs for digit classification.
filter transfer function,"The filter transfer function H(u, v) describes how each frequency component in the Fourier domain is modified by a filter. Its magnitude and phase responses determine the filter’s smoothing, sharpening, or distortion characteristics.","Plotting H(u,v) of a Gaussian LPF with σ=10 to visualize its smooth attenuation."
fovea,"The small, central region of the human retina with high cone density responsible for sharp central vision and color discrimination. Foveated imaging models mimic this by allocating more resolution near the gaze point for efficiency.",Implementing foveated rendering that increases pixel density around a user’s gaze position tracked by an eye-tracker.
FPGAs,"Field-Programmable Gate Arrays are reconfigurable hardware platforms that implement custom parallel logic for algorithms like real-time filters and neural network inference, offering low latency and power efficiency.",Deploying a CNN inference engine on a Xilinx FPGA board to accelerate object detection at the network edge.
frequency domain,"In the frequency domain, an image is represented by its sinusoidal frequency components rather than spatial intensities. This view makes it easy to design and apply filters based on frequency content.",Displaying log-magnitude of the FFT of a photo to reveal periodic interference.
frequency filtering,"Frequency filtering applies a mask or transfer function to the DFT of an image to selectively attenuate or amplify frequency bands. After filtering, the inverse DFT returns the processed image to the spatial domain.",Using a notch filter to remove a 60 Hz periodic noise pattern in the spectrum.
frequency rectangle,A frequency rectangle (ideal bandpass) selects a rectangular region of frequencies in the spectrum while zeroing out others. It creates sharp cutoffs but can introduce ringing artifacts in the spatial domain.,Applying an ideal bandpass mask passing frequencies between 20–50 cycles/image width.
fully-connected layer,"A neural network layer where each input neuron connects to every output neuron via learnable weights, enabling global feature integration. They commonly appear at the end of CNNs for classification tasks.",Adding a 512-unit fully connected layer after convolution blocks in a deep classifier to produce softmax scores.
Gabor filters,"Linear filters defined by sinusoidal plane waves modulated by Gaussian envelopes, tuned to specific frequencies and orientations. They model visual cortex receptive fields and capture texture and edge patterns.",Convolving an image with a bank of Gabor filters at multiple orientations to extract texture features for fingerprint analysis.
gamma correction,"A non-linear power-law transform applied to pixel intensities to account for display or camera gamma characteristics, ensuring perceived brightness matches stored values. It compensates for the eye’s nonlinear luminance response.",Applying s = 255·(r/255)^0.45 to linear image data before displaying on an sRGB monitor to correct brightness.
gamma-ray band,"The portion of the electromagnetic spectrum with photon energies above ~100 keV, used in specialized imaging (gamma-ray cameras) and astrophysical observations. Detection requires scintillators or semiconductor sensors.",Capturing gamma-ray images of radioactive tracer distribution in a SPECT medical scanner for diagnostic imaging.
Gaussian blur,"A smoothing operation that convolves an image with a two-dimensional Gaussian kernel, attenuating high-frequency details while preserving overall structure. The kernel’s standard deviation controls blur strength.","Using OpenCV’s GaussianBlur(img, (5,5), sigmaX=1) to reduce noise before applying edge-detection."
Gaussian lowpass filter,"This filter uses a Gaussian-shaped transfer function in frequency domain to smoothly attenuate high frequencies. It yields a blur with no ringing, controlled by the Gaussian standard deviation.",Designing a Gaussian LPF with σ=5 pixels and applying via ifft2(H.*F).
gaussian noise,"Gaussian noise adds random values drawn from a normal distribution to each pixel, modeling sensor or electronic noise. It is spread evenly across frequencies and is commonly used to test filter performance.",Adding noise with mean=0 and σ=10 to a clean image to benchmark a denoising filter.
gesture recognition,"The interpretation of human movements—hand poses, body gestures—from video or sensor data to control interfaces or trigger commands. It combines detection, tracking, and classification of spatio-temporal patterns.",Employing a depth sensor to recognize swipe gestures for slide navigation in a conference presentation.
GIF file type,Graphics Interchange Format is a raster image format supporting up to 256 colors and simple frame-based animation. It uses LZW compression and preserves exact pixel data but lacks full alpha transparency.,Creating a looping GIF of a user interface mockup to showcase hover effects on a design portfolio website.
GIS,"Geographic Information Systems integrate spatial imagery and vector data with georeferenced coordinates to analyze, visualize, and model geographic phenomena. They support map overlays, terrain analysis, and remote-sensing workflows.",Overlaying satellite imagery and street networks in QGIS to plan new urban transit routes.
gradient direction,"Gradient direction at a pixel is the angle of maximum intensity change, computed via arctan of vertical and horizontal derivatives. It indicates edge orientation.","Computing atan2(Gy, Gx) after Sobel filtering to color-code edge orientation."
granularity,"The level of detail or resolution at which image analysis is performed—pixel-level, patch-level, or object-level. Fine granularity captures minute features; coarse granularity focuses on larger structures and patterns.",Segmenting an image into 100×100-pixel superpixels (coarse granularity) before applying fine-grained object classification.
graphic designer,"A professional who uses image-processing and design software to create visual content, combining typography, imagery, and layout principles. Their work relies on DIP tools for color correction, retouching, and compositing to meet aesthetic and functional goals.","Designing a marketing poster in Adobe Photoshop, employing layer masks and color grading to achieve a cohesive look."
grayscale,"Images composed exclusively of shades of gray, where each pixel holds an intensity value without color. Converting RGB to grayscale reduces dimensionality and simplifies many processing tasks.",Converting a color photograph to grayscale via Y = 0.299 R + 0.587 G + 0.114 B before running the Canny edge detector.
grey levels,"Grey levels are discrete intensity values that a pixel can take, typically from 0 (black) to L–1 (white) where L is the number of levels (e.g., 256). They define the quantization of continuous brightness.",An 8-bit image has 256 grey levels ranging from 0 to 255.
grey-level slicing,Grey-level slicing highlights a specified intensity range by mapping it to high output values while suppressing others. It emphasizes features within that range and is useful in medical imaging.,Mapping intensities 100–150 to white and all others to black to isolate specific tissues.
hand-crafted features,"Manual descriptors designed using domain knowledge to capture salient image characteristics such as edges, textures, and shapes. Examples include SIFT, HOG, and SURF, which powered vision pipelines before deep learning dominated.",Using SIFT keypoint descriptors to match features between two overlapping photographs for panorama stitching.
Harris,"The Harris corner detector identifies interest points by measuring local intensity gradients and computing a corner response function, robust to rotation and moderate illumination changes. It’s widely used for selecting stable keypoints.",Detecting building corners in an aerial image to serve as anchors for image registration.
HCI,"Human–Computer Interaction is the multidisciplinary study of how people design, implement, and use computational systems with graphical or natural interfaces. It informs the ergonomics and usability of vision-based applications.",Designing a touch-friendly photo-editing app interface that uses gesture controls for filter selection.
HDR imaging,"High Dynamic Range imaging combines multiple exposures of the same scene to extend the captured brightness range, preserving detail in both shadows and highlights. Tone-mapping operators then compress this range for display.",Merging bracketed exposures of a sunset into an HDR image that retains detail in the bright sky and dark foreground.
HEIC file type,"HEIC (High Efficiency Image File Format) stores images and image sequences using HEVC compression, offering better quality-to-size ratios than JPEG and supporting features like transparency and depth maps.",Saving iPhone photos as .HEIC to reduce storage while preserving high image fidelity and metadata.
hidden layers,Layers in a neural network situated between the input and output that learn intermediate representations by applying weights and activations. The number and size of hidden layers determine the model’s capacity and abstraction level.,A feed-forward network with two hidden layers of 128 and 64 neurons trained to classify handwritten digits.
high pass filtering,High pass filters attenuate low-frequency (smooth) components while preserving or boosting high frequencies. They enhance edges and detail by removing slow variations in intensity.,Applying a Laplacian kernel to sharpen building edges in an architectural photo.
high-level image processing spectrum,"The top end of the processing pipeline focused on semantic interpretation—tasks like object recognition, scene understanding, and activity analysis. It builds on low- and mid-level processing to deliver actionable insights from images.",Using a CNN to classify and count vehicles in traffic camera footage for urban planning.
highboost filter,"A highboost filter adds a scaled high-pass filtered image back to the original, amplifying fine details without losing the overall brightness. The boost factor controls the degree of sharpening.",Using A=1.5 so that output = original + 0.5*(original − blurred).
histogram equalization,"A global contrast enhancement technique that redistributes pixel intensities so the output histogram is approximately uniform, improving visibility in images with poor dynamic range.",Applying OpenCV’s equalizeHist on a dim indoor photo to brighten shadows and enhance details.
histogram equalization,Histogram equalization redistributes pixel intensities so the output histogram approximates a uniform distribution. This global contrast enhancement makes features more distinct in images with poor dynamic range.,Running OpenCV’s equalizeHist on a low-contrast X-ray to reveal bone structures.
HOG,"Histogram of Oriented Gradients computes gradient orientation histograms over local cells to encode shape and texture information, robust to illumination changes and small deformations. It’s often used in pedestrian detection.",Extracting HOG descriptors from image windows and classifying them with an SVM to detect people in street scenes.
Hough transform,"A voting-based method that maps image points into a parameter space to detect geometric shapes (lines, circles) by finding accumulations of votes corresponding to shape parameters.",Detecting lane markings in road images by applying the Hough line transform to edge-detected pixels.
HSI color space,"A color model representing pixels by Hue, Saturation, and Intensity, separating chromatic content from brightness. It facilitates operations like color-based segmentation and brightness adjustment independent of hue.",Converting an RGB image to HSI and thresholding the hue channel to segment ripe fruit in an agricultural scene.
HSV color space,"A model that describes colors in terms of Hue, Saturation, and Value (brightness), aligning more closely with human perception and easing tasks like color selection and adjustment.",Using the HSV space to isolate and track a red ball in video by thresholding hue between 0° and 10°.
Hubble telescope,"A space-based observatory orbiting above Earth’s atmosphere that captures high-resolution images in visible, ultraviolet, and near-infrared bands. Its data has driven breakthroughs in astronomy and cosmology.",Processing Hubble Deep Field exposures with drizzling and cosmic-ray removal to reveal distant galaxies.
hue component,"The angular attribute of a color in HSI/HSV spaces representing the dominant wavelength (e.g., red at 0°, green at 120°, blue at 240°). It defines pure color irrespective of brightness or saturation.","In HSV, a pixel with hue = 60° corresponds to pure yellow, used to highlight yellow traffic signs."
Hugging Face,"A platform and open-source community providing state-of-the-art machine-learning models, datasets, and tools—initially for NLP but now also hosting vision and multimodal architectures.",Downloading a pretrained Vision Transformer from the Hugging Face Hub for fine-tuning on a flower classification task.
Human Computer Interface,"The hardware and software mechanisms through which users interact with a computer system, including keyboards, touchscreens, and voice-based controls. In vision systems, HCIs enable user-guided annotation and feedback loops.",Using a tablet’s touchscreen GUI to draw bounding boxes around objects for dataset labeling.
human vision,"The biological process by which the eye and visual cortex perceive, process, and interpret light stimuli, featuring non-linear response curves, variable spatial acuity, and color opponency. Insights from human vision guide computational models.",Implementing a retinal-contrast model to simulate human sensitivity when designing a tone-mapping algorithm.
identity transformation,An identity transformation leaves every pixel’s intensity unchanged. It serves as a baseline in point processing and confirms that subsequent operations are working correctly.,Applying f(r)=r to verify that a point-processing routine is initialized properly.
"IDFT, inverse discrete Fourier transform",The IDFT reconstructs a spatial-domain image from its DFT spectrum. It sums weighted complex exponentials to recover original pixel intensities after frequency-domain processing.,Using MATLAB’s ifft2 on a filtered spectrum to obtain the sharpened image.
Illumination source,"The physical light emitters or reflectors (sun, lamps, LEDs) that light a scene; their spectral distribution, intensity, and geometry influence image appearance and must be modeled or compensated in vision tasks.",Calibrating a studio setup with 5500 K LED panels to ensure consistent color rendering in product photography.
"ILPF, ideal lowpass filter","An ideal lowpass filter perfectly passes frequencies below a cutoff radius and completely blocks those above it. While mathematically sharp, it causes ringing artifacts in the spatial domain.","Applying a binary mask with cutoff D₀=30 in the frequency plane, then ifft2."
image acquisition,"The process of capturing images from real-world scenes or objects via sensors (cameras, scanners, microscopes), including considerations of resolution, sampling rate, and noise characteristics.",Grabbing live frames from an industrial machine-vision camera at 120 fps for defect detection.
image alignment,"Registering two or more images by estimating and applying geometric transformations (translation, rotation, scale) so they share a common coordinate frame for analysis or fusion.",Aligning multispectral satellite images using feature matching and RANSAC to create a composite map.
image averaging,"Image averaging combines multiple exposures of the same scene by computing the mean pixel value at each location. This reduces random noise while preserving signal, especially in low-light imaging.",Averaging 10 time-lapse frames of a dim star field to improve SNR.
image compression,"Reducing the data size of an image by encoding it with fewer bits, using lossless (PNG) or lossy (JPEG, HEVC) algorithms, which trade off fidelity for storage or bandwidth savings.",Saving photos as JPEG at 80% quality to upload quickly while maintaining acceptable visual quality.
image displays,"Hardware devices (LCD, OLED, projectors) that render digital images for human viewing, each with specific color gamut, brightness range, and response characteristics requiring calibration.",Showing medical scans on a DICOM-calibrated monitor to ensure consistent grayscale rendering for diagnostics.
image enhancement,"A set of techniques (contrast adjustment, denoising, sharpening) applied to images to improve visual quality or to reveal hidden details, often as a preprocessing step for analysis.",Applying an unsharp mask filter to a landscape photo to bring out fine textural details in foliage.
image enhancement,"Image enhancement applies spatial or frequency transforms to make certain features more visible to human observers or to ease subsequent analysis. Techniques include contrast adjustment, sharpening, and noise reduction.","First equalizing the histogram, then applying unsharp masking to a satellite image."
image filters,"Operators that modify pixel neighborhoods to perform tasks like smoothing, sharpening, and edge detection; they include linear kernels (Gaussian blur) and nonlinear methods (median filter).",Using a 3×3 median filter to remove salt-and-pepper noise from a scanned document image.
image formation,"The physical process by which optics (lenses, mirrors) project a scene onto a sensor or film, governed by imaging equations and affected by aberrations, depth of field, and illumination geometry.",Modeling a pinhole camera projection in simulation to understand perspective distortion.
image processing,"Image processing is the set of operations that modify or analyze pixel data to improve quality or extract information. It spans point operations, spatial filtering, frequency analysis, and machine-vision algorithms.",Using OpenCV to detect and track vehicles in traffic footage.
image processing software,"Applications and libraries (OpenCV, MATLAB, Photoshop) that implement algorithms for acquisition, transformation, analysis, and visualization of images in research and industry.","Using MATLAB’s Image Processing Toolbox to call imadjust, imfilter, and bwlabel for segmentation tasks."
image processing spectrum,"The continuum of operations from low-level (pixel intensity transforms, filtering), through mid-level (segmentation, feature extraction), to high-level (recognition, interpretation) tasks in vision systems.","Building a pipeline that applies Gaussian blur (low-level), edge linking (mid-level), and object classification (high-level)."
Image Processing Toolbox,"A MATLAB add-on providing functions for image analysis, enhancement, filtering, segmentation, and geometric transformations, with a user-friendly interface and integration with Simulink.",Using imrotate and imwarp from Image Processing Toolbox to correct image skew in scanned documents.
image restoration,"Techniques that invert degradations (blur, noise, missing data) to recover an approximation of the original image, including deconvolution, inpainting, and denoising methods based on physical models and regularization.",Applying Wiener deconvolution with a known blur kernel to deblur a motion-blurred photograph.
image sensors,"Devices (CCD, CMOS) that convert incident light into electrical signals, characterized by parameters like quantum efficiency, noise, dynamic range, and pixel pitch, which determine image quality.",Capturing low-light fluorescence microscopy images with a cooled CCD sensor to reduce thermal noise.
image sharpening,"Enhancing edges and fine details by boosting high-frequency components, using methods like unsharp masking, high-pass filters, or Laplacian operators to make features more distinct.",Applying an unsharp mask with radius=2 px and amount=1.5 in Photoshop to crisp up a portrait image.
image stitching,"Combining multiple overlapping images by aligning, warping, and blending them to produce a seamless panorama or mosaic, handling exposure differences and parallax.",Creating a 180° panorama by matching features across three handheld shots and blending seams smoothly.
image transforms,"Mathematical operations that map image data into alternative domains (Fourier, Wavelet, DCT) or coordinate systems (Radon, log-polar) to facilitate analysis, compression, or filtering.",Computing the 2D DCT of an image block for JPEG compression.
ImageNet,"A large-scale image database with over 14 million labeled images across 20 000+ categories, widely used to pretrain deep neural networks and benchmark vision models.",Fine-tuning a pretrained ResNet-50 on a custom dataset after initializing from ImageNet weights.
imaging system,"The complete assembly of optics, sensors, electronics, and software that captures, processes, and displays images, designed for applications like microscopy, remote sensing, or medical diagnostics.","An endoscopic imaging system consisting of illumination fiber, lens assembly, CMOS sensor, and processing board."
impulse noise,"Impulse noise, also called salt-and-pepper noise, randomly sets pixels to minimum or maximum values. It often arises from faulty sensors or transmission errors and is best removed with nonlinear filters like median.",Simulating salt-and-pepper noise by randomly setting 5% of pixels to 0 or 255.
indexed image,"A representation where each pixel holds an integer index that refers to a row in a separate colormap, enabling compact storage and palette-based rendering with up to 2^m colors for an m-bit index.",Displaying an 8-bit indexed GIF where the pixel data matrix indexes a 256×3 RGB colormap.
indexed image - color map matrix,The N×3 matrix of RGB triplets defining the palette entries for an indexed image; each row corresponds to one color used when rendering the image data indices.,"In MATLAB, a 256×3 double matrix where row i gives the RGB color for index i in an 8-bit indexed image."
indexed image - data matrix,"The M×N matrix of integer indices that reference rows in the colormap matrix, specifying which palette color each pixel should display.",A 480×640 matrix of values 1–128 that indexes into a 128-color map for poster-style rendering.
industrial inspection,"Automated vision-based quality control processes on production lines that detect defects, measure tolerances, and verify assembly using cameras, lighting, and analysis algorithms in real time.",A machine-vision system that inspects X-ray images of assembled PCBs to find missing or misaligned components.
infrared band,"The portion of the electromagnetic spectrum from roughly 700 nm to 1 mm, used in thermal imaging, night vision, and remote sensing; different sub-bands (NIR, SWIR, MWIR, LWIR) reveal material properties and heat signatures.",Using a LWIR thermal camera to detect hotspots in industrial equipment for preventative maintenance.
input intensity,Input intensity is the original grey-level value of a pixel before any transformation. It is the reference for all point and neighborhood operations.,r = 120 in an 8-bit image denotes a medium-bright pixel.
intensity resolution,"The number of distinct gray or color levels a sensor or image can represent per channel, determined by bit depth; higher resolution yields finer gradations in brightness.",A 12-bit sensor offering 4096 intensity levels for capturing subtle shading in medical X-rays.
intensity transform,"An intensity transform is a mapping from input pixel values to new output values, applied independently at each pixel. It includes linear, logarithmic, power-law, and piecewise functions for contrast or brightness adjustment.",Applying s=2·r to double brightness across all pixels.
internal image plane,"The conceptual plane inside a camera model where light rays converge to form the image; in pinhole models it’s at the focal distance, while thick-lens models have principal planes.",Simulating pinhole camera projection by sampling scene points onto the internal image plane at z = f.
inverse logarithm transformation,"Inverse logarithmic transforms expand high-intensity values more than low ones, brightening details in lighter areas. It’s the reverse of a log transform and can enhance bright regions without saturating dark ones.",Using s = exp(r/10) mapping to boost highlights in a sunset photo.
Isopreference curve,"A psychophysical contour in a multi-dimensional attribute space (e.g., brightness vs. contrast) along which a human observer has equal aesthetic or functional preference. It guides display calibration and image enhancement design.",Plotting combinations of tone-mapping strength and contrast adjustment that observers rate as equally pleasing
Java,"A high-level, platform-independent, object-oriented programming language widely used for building cross-platform applications, including image processing and computer vision. It includes libraries like JavaCV and OpenIMAJ for vision tasks.",Using JavaCV in a Spring application to apply real-time face detection on video streams.
JAX,"A Python library by Google for high-performance numerical computing that combines a NumPy-like API with automatic differentiation and XLA compilation, enabling accelerated machine learning and vision research. Its functional style supports composable transformations.",Implementing a convolutional neural network in JAX to accelerate training on a TPU for image classification.
jitter,"Small, rapid variations in timing or spatial sampling during image acquisition or display that can cause blurring, ghosting, or misalignment artifacts. In video systems, jitter arises from frame timing instability and mechanical vibrations.",Observing motion blur in high-speed footage due to sensor jitter on a handheld drone camera.
JPEG,A lossy image compression standard (Joint Photographic Experts Group) that reduces file size by discarding high-frequency components imperceptible to the human eye. It uses discrete cosine transform (DCT) and quantization to balance compression ratio and visual quality.,Saving a digital photograph as a 90%-quality JPEG to share on a website without noticeable degradation.
JPEG file type,"A file format (.jpg, .jpeg) using the JPEG compression algorithm to store still images efficiently. It supports 8-bit per channel color and is universally compatible across devices and software.",Archiving holiday photos in the .jpeg format to ensure broad compatibility and reasonable file size.
k-NN,k-Nearest Neighbors is a non-parametric classification/regression algorithm that assigns labels based on the majority vote (or average) of the k closest data points in feature space. It’s simple to implement but scales poorly with large datasets.,Classifying handwritten digits by comparing a test image’s pixel feature vector to its 5 nearest neighbors in the training set.
Keras,"A high-level neural network API in Python that runs on top of TensorFlow, Theano, or CNTK, designed for rapid prototyping and easy model building. It provides user-friendly layers, optimizers, and pre-trained models tailored for vision and NLP tasks.",Defining and training a simple CNN in Keras to classify cat vs. dog images in just a few lines of code.
LANDSAT,"A series of Earth-observing satellites that capture multispectral imagery across visible, infrared, and thermal bands, providing data for environmental monitoring, agriculture, and land-use studies since 1972. Its open archive supports remote sensing research.",Analyzing Landsat 8 imagery to map urban expansion by computing NDVI changes around a city over time.
Laplacian filters,"Laplacian filters compute a weighted sum of second-order derivatives around each pixel, highlighting regions of rapid intensity change. They are used for edge detection and image sharpening.",Convolving with the 3×3 kernel [0 –1 0; –1 4 –1; 0 –1 0] to accentuate edges.
law enforcement,"Agencies and processes that apply image processing and computer vision for surveillance, forensics, and public safety—such as face recognition, license-plate reading, and crime-scene reconstruction. Deployment is governed by ethical and privacy considerations.",Using automatic license plate recognition to detect stolen vehicles at highway toll plazas in real time.
LBP (Local Binary Patterns),"A texture descriptor that thresholds each neighbor pixel against a central pixel and encodes the result as a binary number, capturing micro-patterns for classification tasks. LBP is simple, rotation-invariant, and robust to slight illumination changes.",Computing LBP histograms on facial images as input to an SVM for real-time face authentication.
learned features,"Data-driven representations automatically extracted by machine learning models—especially deep networks—that capture hierarchical, task-specific patterns without manual design. They often outperform hand-crafted features in complex vision tasks.",Using activations from a pretrained ResNet’s penultimate layer as feature vectors for image retrieval.
LeNet-5,"A pioneering CNN architecture by Yann LeCun et al. for handwritten-digit recognition, consisting of alternating convolution and pooling layers followed by fully connected layers. It demonstrated the power of learned features for vision in 1998.",Training LeNet-5 on the MNIST dataset to achieve over 99% accuracy in digit classification.
lens,"An optical component that focuses or diverges light rays to form images on sensors or film, characterized by focal length, aperture, and distortion properties. In cameras, lens choice determines field of view, depth of field, and aberrations.",Using a 50 mm f/1.8 prime lens on a DSLR to capture portraits with shallow depth of field.
light receptors,"Photoreceptive elements in sensors (pixels in CCD/CMOS) or biological rods/cones that convert incoming photons into electrical signals, determining sensitivity, dynamic range, and spectral response.",Adjusting the Bayer filter pattern on a CMOS sensor to optimize color sensitivity for sRGB capture.
linear transformation,A linear transformation scales and shifts pixel values by a constant multiplier plus an offset. It includes contrast stretching and brightness adjustment.,Using s = 1.2·r + 10 to slightly boost contrast and overall brightness.
Linux,"An open-source operating system kernel used extensively in scientific computing and embedded vision platforms for its stability, performance, and extensive support of libraries like OpenCV and ROS. Its modularity enables deployment on diverse hardware.",Running a ROS-based robot vision stack on Ubuntu Linux to perform real-time obstacle detection.
local histogram processing,"Local histogram processing computes histogram-based transforms within a moving window around each pixel. It adapts contrast enhancement to local image characteristics, revealing regional detail.",Applying CLAHE (Contrast Limited Adaptive HE) on a foggy landscape.
logarithm transformation,"A logarithm transformation compresses the dynamic range by mapping input intensities via a log function. It enhances dark values more than bright ones, making shadow detail more visible.",s = c·log(1 + r) with c=255/log(1+255) to reveal details in dark areas.
low pass filtering,Low pass filtering attenuates high-frequency components so that slow intensity variations pass through. It smooths an image and reduces noise at the cost of blurring edges.,Convolving with a uniform 5×5 averaging mask to blur textures.
low-level image processing spectrum,"The set of basic pixel-wise and neighborhood operations—point transforms, filtering, and convolution—that directly manipulate image intensities for tasks like noise removal and smoothing. These operations form the foundation for higher-level algorithms.",Applying a 3×3 median filter and histogram normalization to raw microscope images as preprocessing.
luminance component,"The brightness channel (Y) in color spaces like YUV or YCbCr, representing perceived light intensity independent of chromatic information. Many compression and enhancement algorithms operate primarily on luminance for efficiency and visual fidelity.",Enhancing the Y channel of a YCbCr image via contrast stretching before recombining with chroma for display.
lux,"The SI unit of illuminance measuring luminous flux per unit area (lumens per square meter), used to quantify scene lighting conditions and set exposure parameters in imaging systems. Cameras and sensor designs reference lux for sensitivity calibration.",Setting camera exposure time after measuring studio illumination at 500 lux to avoid underexposure.
Mac OS,"Apple’s Unix-based operating system for Mac computers, offering tight hardware–software optimization for graphics and vision tasks through frameworks like Core Image and Metal, alongside cross-platform libraries such as OpenCV.",Developing an image-editing plugin using macOS Core Image filters in Swift for a photo-retouching app.
machine language pipeline,"The structured workflow in a machine learning or deep learning project—data acquisition, preprocessing, feature extraction, model training, evaluation, and deployment—ensuring repeatable and scalable development of vision systems.","Implementing a pipeline that reads images, applies augmentation, extracts features with a CNN, and serves predictions via REST."
machine learning,"A field of AI where algorithms learn patterns from data to make predictions or decisions, widely used in vision for classification, regression, clustering, and anomaly detection across supervised, unsupervised, and reinforcement paradigms.",Training a random forest on color and texture features to classify healthy vs. diseased plant leaves.
machine learning algorithm,"A specific computational procedure (e.g., SVM, random forest, K-means) that learns from data by optimizing an objective function, producing models that generalize to unseen inputs. The choice depends on problem type, data size, and interpretability requirements.","Using K-means clustering to segment MRI brain scans into white matter, gray matter, and cerebrospinal fluid."
machine learning techniques,"Broad strategies like deep learning, ensemble methods, and transfer learning that enhance performance through architectural design, data handling, or algorithm combinations. They address challenges such as overfitting, data scarcity, and domain adaptation.",Applying transfer learning by fine-tuning a MobileNet pretrained on ImageNet for a small medical-image dataset.
machine perception,"Computational interpretation of sensory data (vision, audio) to form an environmental model for tasks like recognition, tracking, and scene understanding. Perception systems integrate sensing, algorithms, and knowledge representation.",A robot using stereo vision and SLAM to map an indoor factory floor and avoid obstacles autonomously.
MATLAB,"A high-level programming environment with built-in support for matrix operations, visualization, and specialized toolboxes (Image Processing, Deep Learning) that facilitate rapid prototyping of DIP and CV algorithms with interactive debugging and visualization.","Loading an image in MATLAB, applying imbinarize, and using bwlabel to count connected components in one script."
median smoothing filters,"Median filters replace each pixel with the median of its neighborhood, effectively removing impulse noise while preserving edges. They are nonlinear and often outperform linear averages on salt-and-pepper noise.",Applying a 3×3 median filter to remove pepper speckles from a scanned document.
medical visualization,"The application of DIP and computer graphics to render and interact with medical imaging data (CT, MRI, ultrasound) for diagnosis, surgical planning, and education, employing techniques like volume rendering and interactive segmentation.",Generating a 3D volume-rendered model of a patient’s heart from CT slices to guide surgical planning.
microwave band,"The portion of the electromagnetic spectrum from 300 MHz to 300 GHz, used in radar imaging, wireless communication, and remote sensing via SAR (Synthetic Aperture Radar), which captures surface structure regardless of lighting conditions.",Processing X-band SAR images to detect changes in forest biomass over time.
mid-level image processing spectrum,"The set of algorithms that extract structural information—edges, regions, and features—via segmentation, morphological operations, and transformations, bridging pixel-level processing and semantic analysis.",Segmenting a microscopy image into cell regions using watershed and region-growing before classification.
MLPs,"Multi-Layer Perceptrons are feed-forward neural networks with one or more hidden layers, using fully connected nodes and non-linear activations to approximate complex functions. They lack the spatial invariance of CNNs but serve in many regression and classification tasks.",Training an MLP with two hidden layers to predict pixel-wise color corrections in RAW-to-RGB conversion.
morphological processing,"Non-linear operations based on set theory (erosion, dilation, opening, closing) that probe an image with a structuring element to extract or suppress shapes and connectivity, useful for cleaning up binary masks and extracting geometrical features.",Removing small speckle noise from a binary mask using morphological opening with a 3×3 square element.
motion estimation,"The process of determining pixel or block displacements between consecutive frames, used in video compression, optical flow, and stabilization. It yields motion vectors that describe apparent movement in the scene.",Using Lucas–Kanade optical flow to estimate and visualize object motion in traffic surveillance video.
MRI,Magnetic Resonance Imaging is a medical modality using strong magnetic fields and radiofrequency pulses to non-invasively generate high-resolution cross-sectional images of soft tissues based on proton density and relaxation properties.,Segmenting a brain tumor in an MRI scan by thresholding T2-weighted images and refining boundaries with active contours.
natural light,"Sunlight or ambient outdoor illumination characterized by a broad, continuous spectrum and dynamically varying intensity. Vision pipelines adapt to natural light changes via auto-exposure, white balance, and HDR techniques.",Capturing a landscape at dawn and using automatic white-balance correction to compensate for the bluish shadow cast.
NDVI maps,"Normalized Difference Vegetation Index maps visualize plant health by computing (NIR – Red)/(NIR + Red) per pixel from multispectral imagery, highlighting chlorophyll activity and biomass density for agriculture and ecology applications.",Generating NDVI maps from Sentinel-2 images to monitor crop stress across a farming region.
negative images,Negative images invert pixel intensities so that dark becomes light and vice versa. They aid in enhancing details in X-rays and other applications where bright structures are better seen on a dark background.,Transforming an X-ray film to its negative to highlight bone structures.
negative transformation,This point transformation computes the complement of each pixel’s intensity by subtracting it from the maximum possible value. It produces a photographic negative effect.,s = 255 – r for an 8-bit grayscale image.
neighborhood,"A neighborhood is a small, typically square, region of pixels centered on a target pixel. Filters and local statistics are computed over this region.",A 3×3 neighborhood used for computing local mean around each pixel.
neighborhood size,"The neighborhood size (e.g., 3×3, 5×5) defines how many pixels around the center are involved in local operations. Larger neighborhoods yield stronger smoothing but risk losing fine detail.",Choosing a 7×7 window for stronger smoothing in a noisy photo.
neural network,"A computation model inspired by biological neurons, consisting of interconnected nodes (neurons) arranged in layers, which learn complex mappings via weight adaptation. Neural networks underpin modern deep learning in vision, speech, and language tasks.",Deploying a CNN-based neural network to detect manufacturing defects on an assembly line in real time.
neural style transfer,A technique that recombines the content of one image with the style of another by optimizing an image to match deep feature representations in a pretrained CNN. It uses content and style loss functions derived from convolutional layers.,Transforming a selfie into a Van Gogh–style painting by iteratively updating pixels to match style and content targets.
night-time lights of the world,"Global datasets from DMSP-OLS and VIIRS sensors capturing visible light emissions at night, used to study urbanization, population density, and economic activity patterns after dark.",Using VIIRS night-light composites to model electricity consumption and estimate urban population distribution.
NOAA,"The U.S. National Oceanic and Atmospheric Administration provides earth observation data including satellite imagery, weather forecasts, and environmental monitoring for climate, oceanography, and remote sensing research.",Downloading MODIS sea surface temperature data from NOAA servers to analyze coastal upwelling patterns.
noise reduction,"Techniques for suppressing random variations in pixel intensities (noise) using spatial (Gaussian, median) or transform (wavelet shrinkage) methods, enhancing image quality while preserving edges.",Applying non-local means denoising to reduce sensor noise in low-light astrophotography.
noise removal,"Methods to eliminate impulsive or structured noise—such as salt-and-pepper or striping—using median filters, morphological filters, or specialized algorithms, restoring cleaner images for analysis.",Using a 3×3 median filter to remove salt-and-pepper speckles from a scanned historical document.
nth power transformation,"The nth power transform raises normalized pixel values to the n-th power, compressing or expanding contrast depending on n. Values of n>1 darken an image, while fractional n brighten it.",Using r² (n=2) to darken mid-tones and boost contrast in bright areas.
nth root transformation,"The nth root transform is the inverse of the power transform, stretching lower intensities more than higher ones when n>1. It is useful for revealing shadow detail without blowing out highlights.",Applying √r (n=0.5) to brighten dark regions in a dim image.
NTSC color space,"The analog television color-encoding system used in North America, defining luma (Y) and chroma (I, Q) components. It has a limited gamut and has largely been superseded by digital standards but remains relevant for simulating broadcast signals.",Converting RGB images to NTSC YIQ color space to simulate broadcast color fidelity on vintage TV hardware.
NumPy,"A core Python library for numerical computing that provides N-dimensional array objects, broadcasting, and vectorized operations. NumPy forms the foundation for scientific computing and image processing in the Python ecosystem.","Using NumPy to convert a PIL image to an array, apply a 5×5 convolution via slicing, and convert back to an image."
Nyquist principle,"The sampling theorem that states a signal must be sampled at least twice its highest frequency to avoid aliasing, guiding sensor design and reconstruction filters in imaging systems. It ensures accurate representation of continuous scenes when digitized.",Ensuring a camera’s pixel pitch satisfies the Nyquist rate for the optical system’s maximum spatial frequency to prevent aliasing.
object classification,"The task of assigning a category label to an entire image or a pre-detected region, identifying what object type is present. Classification models learn discriminative features for each class but do not provide object location.","Using a CNN to label a 128×128 crop as “dog,” “cat,” or “bird” in a wildlife photo dataset."
object detection,Locating and categorizing multiple objects within an image by outputting bounding boxes and class labels. Detection networks predict both what objects are present and where they lie spatially.,"Running YOLOv5 on street footage to draw boxes around pedestrians, cars, and cyclists in real time."
object recognition,The combined process of detecting objects and identifying their class or specific instance. Recognition systems handle unknown poses and variations to match objects against known templates or learned embeddings.,A logo recognition app that finds and names brands on product packaging in a retail scene.
object segmentation,"Partitioning an image into pixel-accurate regions corresponding to individual objects (instance segmentation) or classes (semantic segmentation). It produces masks rather than boxes, enabling precise object boundaries.",Applying Mask R-CNN to segment each person in a crowd image with a unique colored overlay.
offset printing,"A commercial printing technique where inked images are transferred (offset) from metal plates to rubber blankets and then onto paper. It enables high-volume, consistent reproduction of text and images with fine detail.",Printing monthly magazines by first etching page layouts onto aluminum plates and running them through an offset press.
ONNX,"The Open Neural Network Exchange is an open format for representing deep learning models across frameworks like PyTorch and TensorFlow. ONNX enables interoperability, allowing a model trained in one library to run in another environment without retraining.",Exporting a PyTorch object-detection model to ONNX and loading it in NVIDIA TensorRT for optimized inference on edge devices.
opacity,"A parameter (0 to 1) defining a layer or pixel’s non-transparency in compositing, where 0 is fully transparent and 1 is fully opaque. Opacity controls how underlying content shows through when layers are blended.",Setting a Photoshop layer’s opacity to 0.5 so the background image remains partially visible beneath a text overlay.
OpenCL,"An open standard for writing parallel code that runs on CPUs, GPUs, and accelerators across vendors. OpenCL kernels accelerate compute-heavy tasks—like convolution and feature extraction—by leveraging device-specific parallelism.",Implementing a Sobel filter in OpenCL to run on an AMD GPU and achieve real-time edge detection on HD video frames.
OpenCV,"An open-source computer vision library offering hundreds of optimized algorithms for image/video I/O, filtering, feature detection, and machine learning. It supports C++, Python, Java, and hardware acceleration for real-time applications.",Using OpenCV’s cv2.Canny() and cv2.findContours() in Python to detect and outline cells in microscope images.
OpenGL,"A cross-platform API for 2D/3D graphics rendering, used in visualization and GPU-accelerated image processing. Programmable shaders let you implement custom filters, transformations, and volume rendering.",Writing a GLSL fragment shader in OpenGL to apply a real-time Gaussian blur to a live camera feed in a graphics application.
optic nerve,The bundle of retinal ganglion cell axons that carries electrical visual signals from the eye to the brain’s visual cortex. Damage or disease affecting the optic nerve leads to vision loss patterns analyzed in medical imaging.,Simulating optic-nerve fiber loss on OCT scans to study glaucomatous damage progression.
optical flow,"A field of apparent motion vectors estimated between successive frames by analyzing intensity changes. Optical-flow methods (e.g., Lucas–Kanade, Farneback) support video stabilization, motion tracking, and action recognition.",Computing dense Farneback optical flow on dash-cam video to visualize and measure vehicle movement.
optical path,The product of physical distance traveled by light and refractive index along its trajectory through lenses or media. Optical-path differences create interference or phase shifts used in holography and interferometric imaging.,Designing a Mach–Zehnder interferometer to measure minute optical-path changes caused by temperature variations.
ORB (Oriented FAST & Rotated BRIEF),"A fast feature detector and descriptor that combines FAST keypoint detection with orientation-aware BRIEF descriptors. ORB is efficient, rotation-invariant, and suitable for real-time SLAM and image stitching.",Using OpenCV’s cv2.ORB_create() to detect and match keypoints between overlapping drone photos to create a mosaic.
output digitized image,The final digital image obtained after sampling (spatial quantization) and quantization (intensity discretization) of an analog source. This image is stored as a raster of pixels with bit-depth–defined values for processing and display.,Scanning a 35 mm film frame at 600 dpi to produce a 24-bit TIFF for archival and editing.
PCA,"Principal Component Analysis is a linear transform that projects high-dimensional data onto orthogonal axes of maximum variance. It reduces dimensionality and decorrelates features, often used for compression or preprocessing in vision tasks.",Applying PCA to reduce 128-dimensional SIFT descriptors to 32 dimensions before clustering for visual-vocabulary construction.
perceived intensity,"The subjective brightness an observer experiences, which follows a nonlinear relation to physical luminance (e.g., gamma curves). Vision models and display pipelines apply gamma correction to approximate human perceptual response.",Gamma-correcting a linear HDR image by applying a 1/2.2 exponent so midtones appear perceptually linear on an sRGB display.
PET scan,"Positron Emission Tomography is a functional imaging modality where injected radiotracers emit positrons that annihilate into gamma photons, which are detected to reconstruct metabolic activity maps. PET is widely used in oncology and neurology.",Segmenting a brain PET scan to highlight regions of elevated glucose uptake indicative of tumor activity.
photodiodes,"Semiconductor devices that convert incident photons into proportional electrical current. Linear over a wide dynamic range, photodiode arrays serve as sensors in scanners, spectrometers, and certain high-speed cameras.",Capturing document lines on a sheet-fed scanner by reading voltage outputs from a linear photodiode array.
photoreceptors,"Biological light-sensing cells in the retina—rods for low-light (scotopic) vision, cones for color (photopic) vision. Their spectral and spatial responses inform algorithms for adaptive tone mapping and color constancy.",Simulating rod and cone responses to design an image enhancement pipeline that preserves scotopic visibility.
Photoshop,"Adobe Photoshop is a widely used commercial application for raster-based image editing, compositing, color grading, and retouching. It offers layers, masks, filters, and scripting for creative and professional workflows.",Removing blemishes in a portrait by using Photoshop’s healing brush and layer-based dodge and burn adjustments.
piecewise-linear transformation,"Piecewise-linear transforms map intensity ranges to new values via multiple linear segments, enabling custom contrast adjustments. Examples include contrast stretching with controlled midrange mapping.","Mapping [0–80]→[0–50], [80–160]→[50–200], [160–255]→[200–255] to emphasize midtones."
PIL (Pillow),"Pillow is the friendly fork of the Python Imaging Library, providing Python APIs for opening, manipulating, and saving many image formats. Functions include resizing, cropping, filtering, and pixel-level access.",Using Pillow’s Image.resize() and Image.filter(ImageFilter.GaussianBlur) to batch-process and smooth a photo library.
Pixel,"The smallest addressable element in a digital image, representing a single color or gray-level sample. An image’s resolution and bit depth determine how many pixels and intensity levels it contains.","A 1920×1080 display has about 2.07 million pixels, each storing 8-bit values for red, green, and blue channels."
PNG,"Portable Network Graphics is a raster format supporting lossless compression and optional alpha transparency. PNG preserves exact pixel values, making it ideal for graphics, screenshots, and images requiring sharp edges or overlay compositing.",Exporting a web graphic with crisp text and semi-transparent logo elements as a .png file to maintain quality.
PNG file type,"Files with the .png extension adhere to the PNG specification, storing image data in chunks with zlib-based compression, color palettes, and metadata (gamma, ICC profiles). They are universally supported in browsers and editors.",Saving MATLAB plots to “figure.png” to embed in a report with transparent background and lossless detail.
point processing,"Point processing involves operations on individual pixels based solely on their original values. It includes identity, negative, thresholding, and basic intensity transforms.",Thresholding each pixel with T=128 to create a binary mask.
power law transformations,"Power law (gamma) transforms map input intensities using a power function s = c·r^γ, controlling brightness and contrast nonlinearly. They are fundamental in display calibration and image enhancement.","Using γ=0.5, c=255 to apply gamma correction and brighten a dim image."
primary colors,"Fundamental hues from which other colors are derived through mixing; they differ by medium. In additive systems, the primaries are red, green, and blue, while in subtractive (pigment) systems they are cyan, magenta, and yellow (with black added in CMYK).",Mixing RGB light at varying intensities on a screen versus combining cyan/magenta/yellow inks in print.
primary colors of light,"The additive primaries—red, green, and blue—whose combinations at different intensities span most perceivable colors. Display devices emit these to synthesize full-color imagery.","Calibrating a monitor so that equal intensities of R, G, and B channels produce a neutral gray or white."
primary colors of pigment,"The subtractive primaries—cyan, magenta, and yellow—absorb (subtract) complementary light wavelengths when mixed on paper. Black (key) ink is often added to improve contrast and depth (CMYK process).","Designing a brochure in CMYK where cyan, magenta, yellow, and black separations are generated for press plates."
Python,"A high-level, interpreted language with readable syntax and a rich ecosystem (NumPy, SciPy, OpenCV) for numerical computing and rapid prototyping of image-processing and vision algorithms.",Writing a Python script using OpenCV’s cv2.VideoCapture() to read webcam frames and display edge-detected output.
PyTorch,An open-source deep learning framework featuring dynamic computation graphs and GPU acceleration. PyTorch integrates with TorchVision to provide pretrained models and utilities for vision research and deployment.,Fine-tuning a ResNet-50 in PyTorch on a custom medical-image dataset for tumor classification.
quantity domain,"The set of permissible values a variable can assume—e.g., pixel intensities in an image. Digital images have discrete quantity domains determined by bit depth (e.g., 0–255 for 8-bit channels).",Converting a floating-point HDR image into an 8-bit PNG by clamping values into the 0–255 domain.
quantization,"Mapping continuous signal values (spatial position or intensity) to a finite set of discrete levels, introducing quantization noise. In imaging, quantization occurs in analog-to-digital conversion and color depth reduction.",Reducing a 16-bit raw image to 8-bit JPEG by quantizing each channel to 256 levels.
radio band,"Portions of the electromagnetic spectrum from kHz to GHz frequencies, used in radar remote sensing and wireless data transmission. Radar imaging leverages radio wavelengths to penetrate clouds and map terrain or moisture.",Processing C-band SAR data from a Sentinel-1 satellite to monitor flood extents in rainy conditions.
Ranger 7 probe,NASA’s 1964 lunar probe that transmitted the first clear close-up TV images of the Moon’s surface before impact. Analog video frames were later digitized for scientific study of lunar geology.,Converting Ranger 7’s analog lunar TV frames into digitized TIFFs to analyze crater morphology.
RAW file type,"A minimally processed camera file format that stores sensor readings with full bit depth and metadata, enabling maximum flexibility in post-capture adjustments. RAW files require special software to demosaic, color correct, and tone-map.","Importing a Nikon .NEF RAW file into Lightroom to adjust exposure, white balance, and highlight recovery non-destructively."
real-time ALPR system,"An Automated License Plate Recognition pipeline that captures video frames, detects plate regions, segments characters, and applies OCR at video-rate speeds. ALPR integrates detection, segmentation, and recognition modules for live monitoring.",Deploying a highway toll-collection camera that reads license plates at 30 fps to automate billing and law enforcement.
ResNet,"A convolutional neural network architecture that uses residual (skip) connections to enable training of very deep networks by mitigating vanishing gradients. ResNet variants (e.g., ResNet-50) serve as powerful feature extractors in transfer learning.",Using a ResNet-50 pretrained on ImageNet to extract 2048-dimensional features for a downstream classification task.
ResNet-152,"A deeper ResNet model with 152 layers, providing enhanced representational capacity and improving accuracy on large datasets at the expense of increased computation and memory.",Benchmarking ResNet-152 vs. ResNet-50 on fine-grained bird species recognition to assess trade-offs between accuracy and speed.
resolution,"The amount of spatial detail an imaging system can capture, measured in pixels (e.g., 3840×2160) or optics as line pairs per millimeter (lp/mm). Higher resolution yields finer detail but increases data size and processing cost.",Choosing a 4K (3840×2160) camera for aerial mapping to resolve small features in urban environments.
retina,The light-sensitive neural layer at the back of the eye containing rods and cones; it transduces photons into electrical signals that travel via the optic nerve. The retina’s nonuniform cell density (fovea vs. periphery) inspires foveated imaging models.,Implementing foveated rendering that increases sampling density around a user’s gaze tracked by an eye-tracker.
RGB,"An additive color model representing images as three channels—red, green, and blue—each typically 8 bits deep. Color display devices combine these primaries at varying intensities to reproduce a wide gamut.","Storing a photo as a 24-bit bitmap where each pixel has 8-bit R, G, and B values."
RGBA,"An extension of RGB that adds an alpha channel for per-pixel transparency, enabling blending and compositing effects. The alpha value ranges from 0 (transparent) to 255 (opaque) in 8-bit images.",Overlaying a semi-transparent RGBA PNG sprite onto a game background to render shadows and effects.
RNNs,"Recurrent Neural Networks are architectures with cyclic connections that allow information persistence across sequence steps, suitable for video and time-series modeling. Variants include LSTM and GRU, which address gradient-vanishing challenges.",Training an LSTM-based RNN on frame-level embeddings to predict future frames in a surveillance video.
rods,"Photoreceptor cells in the retina that are highly sensitive to low light, enabling scotopic (night) vision but lacking color discrimination. Rod response models inform algorithms for enhancing dark or infrared imagery.",Enhancing night-vision footage by simulating rod sensitivity curves to boost low-light contrast without color artifacts.
ROI cropping,Extracting a rectangular region of interest from an image to focus processing on relevant areas and reduce computational load. ROI cropping is a standard preprocessing step in detection and recognition pipelines.,Cropping the region around a detected face before passing it to a facial-landmark estimator to speed up the network.
rotating drum scanners,"High-resolution scanners that wrap documents around a precision drum and use a rotating sensor bar to capture images with uniform motion and lighting. They achieve very high DPI and geometric accuracy for maps, technical drawings, and archival materials.",Digitizing large engineering blueprints at 2400 dpi on a rotating drum scanner to ensure precise line reproduction
salt and pepper noise,"Salt and pepper noise randomly sets pixels to extreme low or high values, appearing as white and black speckles. It is a type of impulse noise often caused by faulty data transmission.",Adding salt-and-pepper noise at 2% density to test median filter performance.
sampling,"The conversion of a continuous signal or scene into discrete measurements at regular spatial or temporal intervals. In imaging, sampling defines how continuous light intensity is captured as an array of pixels; proper sampling rate avoids aliasing and preserves detail.",Scanning a film negative at 600 dpi so that each one-hundredth-inch in the original maps to one pixel in the digital image.
saturation component,"In color models like HSI or HSV, saturation measures the purity or intensity of a hue, ranging from 0 (gray) to 1 (fully vivid). Adjusting saturation changes how “washed out” or “rich” colors appear without altering brightness or hue.",Increasing the saturation component of a photograph in Photoshop to make the reds and blues more vibrant without affecting luminance.
scene element,"A distinct object or region in an image—such as a person, vehicle, or building—that constitutes part of the visual scene. Identifying scene elements is a key step in recognition and understanding tasks to parse complex imagery into meaningful parts.","Detecting and labeling cars, pedestrians, and traffic lights as scene elements in an autonomous-driving camera feed."
scene understanding,"High-level interpretation of an image or video that goes beyond detecting objects to infer relationships, context, and intent—combining semantics, geometry, and sometimes language. It underpins applications like autonomous navigation and robotics by building a coherent model of the environment.","An autonomous drone’s system analyzing a warehouse scene to recognize shelves, forklifts, and human workers, then planning safe flight paths."
scikit-image,"A Python library built on NumPy and SciPy that provides a collection of image-processing algorithms, including filtering, morphology, segmentation, and geometric transformations. It emphasizes readability, ease of use, and integration with the scientific Python stack.",Using skimage.filters.sobel to compute the edge magnitude of a grayscale image before segmentation.
SciPy,"An open-source Python library for scientific computing that includes the scipy.ndimage submodule offering functions for filtering, interpolation, and morphological operations on N-dimensional images. It complements NumPy and integrates well with other Python imaging tools.",Applying scipy.ndimage.gaussian_filter to smooth a noisy microscopy image as a preprocessing step.
secondary colors,"Colors produced by mixing two primary colors. In additive (light) systems, red+green yields yellow, green+blue yields cyan, and red+blue yields magenta. In subtractive (pigment) systems, cyan, magenta, and yellow are the secondaries used in printing.",Combining red and blue light on an LED screen to produce magenta in a graphical effect.
segmentation,"The process of partitioning an image into meaningful regions or objects based on attributes like color, intensity, or texture. Segmentation can be semantic (class-based), instance (object-based), or panoptic (both), and is foundational for tasks like object recognition and medical analysis.",Using Otsu’s thresholding to segment cells from the background in a stained microscope slide image.
segmentation,"Segmentation partitions an image into regions or objects based on criteria like intensity, texture, or edge continuity. It is the first step in many vision tasks, separating foreground from background.",Using Otsu’s thresholding to segment cells from microscope imagery.
semi-supervised learning,"A machine-learning paradigm that leverages a small amount of labeled data together with a larger pool of unlabeled data to improve model performance, often through consistency regularization or pseudo-labeling. It reduces annotation burden while maintaining accuracy.",Training an image classifier with 1 000 labeled images and 10 000 unlabeled images by assigning pseudo-labels to the unlabeled set during training.
sharpening,"Enhancing edge contrast and fine details by boosting high-frequency components in the image spectrum. Common methods include unsharp masking, high-pass filtering, and Laplacian operators; sharpening improves perceived clarity but can amplify noise.","Applying unsharp masking (radius=2 px, amount=1.2) in Photoshop to make the texture of fabric stand out in a product photo."
sharpening filters,"Sharpening filters enhance edges and fine details by amplifying high-frequency components. Common examples include Laplacian, unsharp masking, and highpass kernels.","Applying unsharp masking (Gaussian blur radius=2, amount=1.2) to bring out text details."
Shi-Tomasi,"A corner-detection algorithm that improves on Harris by selecting points where the smaller eigenvalue of the local gradient covariance matrix exceeds a threshold. It yields robust, well-distributed keypoints under rotation and illumination changes.","Using OpenCV’s goodFeaturesToTrack with the Shi-Tomasi criterion (qualityLevel=0.01, minDistance=10) to detect corners in a building facade."
SIFT (Scale-Invariant Feature Transform),"An algorithm that detects keypoints invariant to scale and rotation by identifying extrema in Difference-of-Gaussians scale space, then computing 128-dimensional gradient-orientation descriptors. SIFT features are robust to affine distortions and illumination changes.",Matching two photos taken from different viewpoints of a sculpture by extracting and matching SIFT descriptors to compute the relative pose.
SIFT keypoints,"Scale-space extrema of the Difference-of-Gaussians that represent distinctive image locations (blobs, corners) at multiple scales. Each keypoint is characterized by its position, scale, and orientation, making it stable under various transformations.",Detecting ~500 SIFT keypoints on a natural scene image; visualizing them as circles whose radius encodes scale and a line indicates orientation.
smoothing filters,"Smoothing filters reduce noise and detail by averaging, median, or Gaussian operations within a neighborhood. They attenuate high frequencies while preserving larger structures.",Using a Gaussian filter (σ=1) to gently blur a portrait background.
Sobel,A discrete differentiation operator that computes approximate horizontal and vertical gradients by convolving the image with two 3×3 kernels. Sobel’s smoothing in one direction improves noise robustness; combining gradients yields edge magnitude and direction.,Applying cv2.Sobel in Python to extract the gradient magnitude of a road scene before thresholding to detect lane markings.
sobel filters,Sobel filters compute approximate first derivatives in horizontal and vertical directions using 3×3 kernels. They produce two gradient images that can be combined into edge magnitude and orientation.,Convolving an image with Sobel X and Y to detect road lane edges in driver-assist systems.
spatial differentiation,"Spatial differentiation computes image derivatives in the x and/or y directions to detect edges and transitions. First-order gives gradients, second-order highlights zero crossings.",Computing the Laplacian (second derivative) after Gaussian smoothing to find blobs.
spatial domain,The spatial domain refers to the image plane where pixel intensities are directly manipulated. Operations like convolution and neighborhood statistics occur here.,Directly applying a 5×5 sharpening kernel to enhance printed text scans.
spatial filtering,Spatial filtering applies a kernel or mask directly to image pixels in the spatial domain. It includes both linear (convolution) and nonlinear (median) methods.,Running a 3×3 median filter to remove salt-and-pepper specks from a scanned photo.
spatial first derivative,"The spatial first derivative approximates the gradient of intensity, indicating rate and direction of change. It is used for edge detection with operators like Sobel or Prewitt.",Using Prewitt operator to compute Gx and Gy for detecting object contours.
spatial resolution,"The smallest spatial detail that can be resolved by an imaging system, often expressed in pixels per inch (ppi), dots per inch (dpi), or line pairs per millimeter (lp/mm). Higher spatial resolution captures finer structures but increases data size and processing load.",Using a satellite sensor with 0.5 m ground sampling distance (GSD) to resolve vehicles in aerial imagery.
spatial second derivative,"The spatial second derivative measures the rate of change of the gradient (Laplacian), detecting points where intensity changes most abruptly. It is sensitive to noise and often preceded by smoothing.",Applying Laplacian of Gaussian (LoG) to detect circular features in cell images.
specialized image hardware,"Dedicated devices—such as GPUs, FPGAs, DSPs, and ASICs—designed to accelerate image-processing and computer-vision operations with massive parallelism, low latency, and efficient power use. They offload compute from general-purpose CPUs for real-time performance.",Running a ResNet inference engine on an NVIDIA Jetson Xavier module to achieve 30 fps object detection on a drone.
supervised learning,"A learning approach in which a model is trained on input–output pairs (features and labels) to learn a mapping from data to target outputs. It underlies most classification and regression tasks, requiring annotated datasets for training and validation.",Training a CNN on a dataset of labeled face images to recognize identities in a security application.
supervised learning pipeline,"A structured workflow that orchestrates data collection, labeling, preprocessing, feature extraction or augmentation, model training, hyperparameter tuning, validation, and deployment for supervised-learning tasks. It ensures reproducibility and scalability in production systems.","Building a pipeline that reads images from storage, augments them, trains a TensorFlow model, evaluates on a hold-out set, and exports the best checkpoint for serving."
SURF (Speeded-Up Robust Features),A fast interest-point detector and descriptor that approximates Hessian-matrix blob detection with integral images and computes orientation-invariant descriptors based on Haar wavelet responses. SURF offers similar robustness to SIFT with improved speed.,Using OpenCV’s xfeatures2d.SURF_create to match keypoints between frames for real-time panorama stitching.
SVG file type,"Scalable Vector Graphics is an XML-based vector format where images are defined by geometric primitives (paths, shapes, text) and styling attributes. SVGs are resolution-independent, easily editable, and ideal for diagrams, logos, and web graphics.",Exporting a flowchart from Inkscape as diagram.svg so it can scale without quality loss on a website.
SVMs,Support Vector Machines are supervised classifiers that find the hyperplane maximizing the margin between classes in feature space. They can be extended with kernel functions to handle non-linear separations and are effective for small- to mid-sized datasets with well-engineered features.,Training an RBF-kernel SVM on HOG descriptors of pedestrian patches to classify “person” vs. “background” in a surveillance system.
television band,Portions of the radio-frequency spectrum—VHF (30–300 MHz) and UHF (300 MHz–3 GHz)—allocated for analog and digital TV broadcasting. These bands also influence wireless camera links and interference considerations in video transmission systems.,"Using a UHF wireless video transmitter at 2.4 GHz to send drone footage to a ground station, avoiding TV-broadcast channels."
TensorFlow,"An open-source framework by Google for building and training machine-learning and deep-learning models using dataflow graphs. TensorFlow supports CPUs, GPUs, TPUs, and provides high-level APIs (Keras) alongside lower-level control for research and production.",Defining and training a U-Net in TensorFlow 2.x for medical-image segmentation of brain MRI scans.
TensorRT,"NVIDIA’s high-performance deep-learning inference SDK that optimizes trained models via layer and precision fusion, dynamic tensor memory, and hardware-specific kernels to maximize throughput and minimize latency on GPUs.",Converting a TensorFlow-trained SSD-Mobilenet model to a TensorRT engine to achieve real-time object detection on an NVIDIA DGX.
three-dimensional image,"Volumetric data represented as a 3D grid of voxels (volume pixels), capturing internal structure in modalities like CT, MRI, or 3D reconstructions from multiple views. True 3D images enable analysis along any plane or rendering of isosurfaces.",Visualizing a 512×512×256 CT volume of a human thorax in a 3D viewer for surgical planning.
thresholding,Thresholding converts a grey-level image to binary by comparing each pixel to a cutoff value. It segments objects from background when intensities are distinct.,Converting an aerial image to black/white using T=128 to separate buildings from roads.
TIFF file type,"Tagged Image File Format is a flexible container that supports multiple pages, high bit depths (8–32 bits/channel), lossless compression, and extensive metadata tags, making it a standard for scientific imaging and archiving.","Saving high-dynamic-range microscopy data as a 16-bit, multipage TIFF stack for downstream analysis in ImageJ."
time domain,"A representation of signals or image sequences in terms of variation over time. Video processing and temporal filtering operate in the time domain to detect motion, flicker, or temporal patterns before or after conversion to frequency or other domains.",Analyzing frame-to-frame intensity changes to detect flicker in a time-lapse video of plant growth.
tomography,"A technique that reconstructs cross-sectional images from projection data acquired at multiple angles around an object, using algorithms like filtered backprojection or iterative reconstruction. Tomography is the basis of CT, PET, and electron tomography.",Performing a CT reconstruction of a small animal by processing 360 X-ray projections around its axis with filtered backprojection.
traditional computer vision pipeline,"A pre-deep-learning workflow consisting of image acquisition, low-level filtering, mid-level feature detection (edges, corners), geometric modeling, and rule-based decision logic. These handcrafted steps rely on explicit algorithms rather than learned representations.","Detecting lane lines by smoothing with a Gaussian filter, applying Canny edges, using Hough transform to find lines, and filtering by slope."
traditional computer vision techniques,"Handcrafted algorithms—thresholding, morphological ops, template matching, feature descriptors (SIFT, HOG)—that process images via explicit rules and mathematical formulas. They often require domain expertise and parameter tuning, contrasting with end-to-end learned methods.",Using normalized cross-correlation to template-match and locate a company logo in scanned documents.
traditional neural network,"Early artificial neural networks with only a few layers (shallow architectures) composed of fully connected perceptrons. They lack spatial invariance and deep hierarchical feature learning, limiting performance on complex vision tasks before the advent of deep learning.",Training a three-layer perceptron on raw pixel vectors for simple digit recognition on the MNIST dataset with ~92% accuracy.
transmission electron microscopy,"An imaging modality where a focused beam of high-energy electrons is transmitted through an ultrathin specimen, interacting with atomic structures to form high-resolution images down to the subnanometer scale. TEM reveals internal morphology and crystal structures.",Capturing ultrastructural details of virus particles at 1 nm resolution in a biological research lab.
two-dimensional image,"A planar grid of pixels indexed by two spatial coordinates (rows and columns), representing intensity or color values at discrete positions. The vast majority of digital photographs and video frames are 2D images stored in formats like PNG or JPEG.",Displaying a 1920×1080 RGB JPEG photograph on a monitor as a 2D array of 8-bit color pixels.
ultrasound,"A real-time imaging modality that sends high-frequency sound waves into the body and measures echoes to form cross-sectional or Doppler images. Ultrasound is safe, portable, and widely used in obstetrics, cardiology, and industrial non-destructive testing.",Using a 5 MHz linear ultrasound probe to visualize fetal anatomy during prenatal checkups.
ultraviolet band,"The portion of the spectrum from roughly 10 nm to 400 nm, used in UV-enhanced imaging and fluorescence microscopy to reveal material properties and biological structures invisible under visible light. UV imaging often requires specialized sensors and filters.",Capturing UV fluorescence of plant leaves under 365 nm illumination to map stress-induced chlorophyll changes.
Unix,"A family of multitasking, multiuser operating systems known for stability, security, and scripting capabilities. Unix and its derivatives (Linux, macOS) are popular platforms for developing and deploying image-processing and vision applications on workstations and servers.",Running a Python-based image-analysis pipeline on a Linux cluster with Slurm for batch processing of satellite imagery.
unsharp masking,Unsharp masking sharpens an image by subtracting a blurred (unsharp) version from the original and adding a scaled difference back. It emphasizes edges without extreme boosts that cause noise amplification.,Using kernel size=5 and amount=1.5 to sharpen a landscape photo without over-boosting noise.
unsupervised learning,"A learning paradigm where models infer patterns or structures from unlabeled data, performing tasks such as clustering, density estimation, or dimensionality reduction. Unsupervised methods are valuable for exploring unknown datasets or pretraining representations.",Applying K-means to cluster texture patches into distinct material categories for later manual annotation
VGG16,"A convolutional neural network architecture developed by the Visual Geometry Group at Oxford, featuring 16 weighted layers with small (3×3) convolution kernels and deep, uniform structure. It excelled in ImageNet classification and popularized very deep networks for transfer learning.",Using a pretrained VGG16 to extract 4096-dimensional feature vectors from photographs for clustering similar images.
video stabilization,The process of reducing unwanted camera shake and jitter in video by estimating global inter-frame motion and applying corrective transforms or cropping. Stabilization yields smoother playback and more professional footage.,A smartphone app that crops and warps each frame based on estimated camera motion to produce a shake-free vacation video.
video tracking,"Continuously locating and following one or more objects across successive video frames by combining detection, motion estimation, and data association. It underpins surveillance, sports analytics, and autonomous navigation.",Tracking each player on a basketball court in broadcast footage to generate movement heatmaps and performance statistics.
VIIRS instruments,"The Visible Infrared Imaging Radiometer Suite aboard NOAA and Suomi-NPP satellites, capturing high-resolution day/night imagery across multiple spectral bands for weather forecasting, environmental monitoring, and night-light studies.",Generating global night-time light maps from VIIRS Day/Night Band data to analyze urbanization and energy use trends.
virtual reality,"An immersive technology that renders a computer-generated, stereoscopic 3D environment, tracked by head-mounted displays and controllers to allow interactive exploration and manipulation of virtual scenes.",A VR training simulator for firefighters that responds to user movements and renders dynamic smoke and heat effects in real time.
visible band,The portion of the electromagnetic spectrum (~400–700 nm) detectable by the human eye and standard digital cameras. Imaging in this band captures natural colors and contrast under ambient or artificial illumination.,Calibrating a DSLR’s white balance using a Macbeth color checker under daylight to ensure accurate visible-band color capture.
vision specialist,"A professional with expertise in image processing and computer vision algorithms who designs, implements, and optimizes systems for tasks like detection, recognition, and analysis across industries.",Hiring a vision specialist to develop a machine-vision inspection system that rejects defective parts on an assembly line.
watermarking,"Embedding visible or imperceptible patterns—often in frequency or spatial domains—into images or video to assert ownership, track distribution, or detect tampering. Robust schemes survive common processing like compression.",Inserting an invisible spread-spectrum watermark into stock photos so unauthorized copies can be traced online.
watershed algorithm,"A segmentation approach that treats gray levels as topography and simulates flooding from marker points until watersheds meet, delineating catchment basins. Marker-controlled variants prevent over-segmentation by guiding flood sources.",Separating touching cell nuclei in a microscopy image by placing foreground and background markers and running watershed.
WebP file type,"An image format by Google using VP8-based compression for both lossy and lossless modes, supporting transparency and animation with smaller file sizes than JPEG or PNG. It’s widely adopted for web graphics to speed up page loads.",Saving icons and illustrations as .webp files to reduce website bandwidth while preserving visual fidelity.
weighted smoothing filters,Weighted smoothing filters assign different weights within a neighborhood—often Gaussian—to emphasize closer pixels during averaging. They offer better edge preservation than uniform averaging.,Convolving with a 5×5 Gaussian kernel so centre pixels contribute more to the average.
Wiener deconvolution,A restoration filter that inverts known blur (point-spread function) in the frequency domain while balancing noise amplification via a signal-to-noise ratio parameter. It achieves optimal trade-off between deblurring and noise suppression.,Restoring a motion-blurred image by applying Wiener deconvolution with an estimated linear blur kernel and known noise level.
Windows,"Microsoft’s dominant desktop operating system, providing extensive APIs (Win32, DirectX) and hardware support for image-processing and vision applications. It integrates with libraries like OpenCV, DirectML, and CUDA for accelerated computing.",Building a C# WPF application on Windows that uses EmguCV to detect and annotate faces in webcam streams.
x-ray band,"Electromagnetic radiation with wavelengths roughly 0.01–10 nm, used in medical imaging (radiography, CT) and nondestructive testing. X-ray imaging relies on differential absorption by materials to reveal internal structures.",Reconstructing bone fractures from digital x-ray radiographs in a hospital’s PACS system for diagnostic review.
YCbCr color space,"A digital color model separating luminance (Y) from chrominance (Cb, Cr), enabling chroma subsampling (e.g., 4:2:0) for efficient video compression and broadcast standards while preserving perceived detail in the luminance channel.",Encoding a Blu-ray video stream in YCbCr 4:2:0 format to reduce bandwidth by halving color resolution without grayscale loss.
YIQ color space,"The analog TV color model used in NTSC broadcasts, where Y is luminance and I (in-phase) and Q (quadrature) encode chrominance. It optimizes bandwidth by allocating more resolution to luminance, matching human visual sensitivity.",Simulating NTSC signal quality by converting RGB test images to YIQ and back to assess chroma fidelity on legacy TVs.
YOLO,"“You Only Look Once” is a family of real-time object detectors that predict bounding boxes and class probabilities in a single neural network pass. Its unified architecture offers high speed with competitive accuracy, ideal for live detection tasks.",Running YOLOv5 on a Jetson Nano to detect and label traffic signs in a live roadside camera feed at 25 fps.