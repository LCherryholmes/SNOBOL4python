Component,Name,Article,,
FPNet,Frequency Perception Network,Frequency Perception Network for Camouflaged Object Detection,,
CFM,Correction Fusion Module,Frequency Perception Network for Camouflaged Object Detection,,
CFM,"Correction Fusion Module, direct lineage",Camouflaged Object Detection via Context-aware Cross-level Fusion,,
CFM,"Correction Fusion Module, inspiration",Context-aware Cross-level Fusion Network for Camouflaged Object Detection,,
Foundational,,Detecting Camouflaged Object in Frequency Domain,,
LF,Loss Function,"F3Net Fusion, Feedback and Focus for Salient Object Detection",,
"NCD, SINet",Neighbor Detection Decoder,Concealed Object Detection,,
Octave Convolution,,"Drop an Octave
Reducing Spatial Redundancy Convolutional Neural Networks with Octave Convolution",,
PVT,Pyramid Vision Transformer,Pyramid Vision Transformer a Versatile Backbone for Dense Prediction without Convolutions,,
RFB,Receptive Field Block,Receptive Field Block Net for Accurate and Fast Object Detection,,
SAM,Spatial Attention Mechanism,Convolutional Block Attention Module,,
FPM,Frequency Perception Module,Frequency Perception Network for Camouflaged Object Detection,,
Component Level,Component,,,
1,FPNet (Frequency Perception Network),,,
1.1,Feature Extraction Backbone,,,
1.1.1,PVT (Pyramid Vision Transformer),,,
1.1.1.1,overall 4 stages,,,
1.1.1.2,patch embedding layer,,,
1.1.1.3,"L1, L2, L3, and L4 Transformer Encoding Layers",,,
1.1.1.3.1,attention layer,,,
1.1.1.3.2,feed-forward layer,,,
1.1.1.4,"1st stage, flattened 4x4 patches to a linear projection",,,
1.1.1.5,"1st stage, transformer with L1 levels",,,
1.1.1.6,"F1, feature map, 4x4",,,
1.1.1.7,"F2, feature map, 8x8",,,
1.1.1.8,"F3, feature map. 16x16",,,
1.1.1.9,"F4, feature map, 32x32",,,
1.1.1.10,"Feature pyramid, F1, F2, F3, F4",,,
1.1.1.11,progressive shrinking strategy,,,
1.1.1.12,Spatial-Reduction Attention,,,
1.1.2,"X1 feature map, rich detail information",,,
1.1.3,"X2 feature map, higher level semantic information",,,
1.1.4,"X3 feature map, higher level semantic information",,,
1.1.5,"X4 feature map, higher level semantic information",,,
1.2,Frequency-guided Coarse Localization Stage,,,
1.2.1,"FPM, Frequency-perception Module
frequency-domain feature extraction on high-level features
automatically separate features into high-frequency and low-frequency parts",,,
1.2.2,"NCD, Neighborhood Connection Decoder, feature fusion decoding",,,
1.2.3,"S1, coarse COD map, high-level features are embedded into CFM",,,
1.2.4,Octave Convolution,,,
1.3,Detail-preserving Fine Localization Stage,,,
1.3.1,"CFM, Correction Fusion Module
progressively achieves prior-guided correction and fusion across layers
fuses adjacent layer features and a coarse camouflaged mask to produce fine output",,,
1.3.2,"RFB, Receptive Field Block, low-level high-resolution feature optimization",,,
1.3.3,"SAM, Spatial Attention Mechanism",,,
1.3.4,"Soutput, the final COD result, output of SAM and CFM combined",,,
1.3.5,"Sg, coarse mask ",,,
1.3.6,Prior-guided Correction,,,
1.3.7,Channel-wise Correlation Modeling,,,
1.4,Loss Function,,,
Component,Term,Definition,Example,
SAM,[·;·] (concatenation operator),"Notation for concatenating tensors along a specified axis, here channel axis in spatial attention.",[F_avg; F_max] yields a 2×H×W tensor from two 1×H×W maps.,
SAM,⊗ (element-wise multiplication operator),Symbol denoting Hadamard (element-wise) product between attention and feature maps.,Mc ⊗ F applies channel attention map to feature map.,
FPNet,1×1 convolution,"A convolutional operation with a 1×1 kernel that linearly projects between channel dimensions without changing spatial resolution. It is commonly used for channel reduction, expansion, or fusion of feature maps.",Using a 1×1 conv to reduce a 256-channel feature map to 64 channels before frequency fusion.,
RFB,1×1 convolution,A pointwise conv using 1×1 kernels that merges channel information or reduces/increases channel count without affecting spatial size.,A 1×1 conv reduces 1536 channels to 512 after branch concatenation.,
Octave Convolution,1×1 convolution,A pointwise convolution that mixes information across channels without affecting spatial dimensions.,Using a 1×1 conv to reduce a 256-channel tensor to 64 channels before applying a 3×3 conv in a bottleneck block.,
FPNet,3×3 convolution,A standard convolutional filter of size 3×3 that captures local spatial patterns and edge information. Stacking multiple 3×3 layers increases the effective receptive field while keeping parameter count moderate.,Applying a 3×3 conv to extract texture details around a camouflaged frog’s outline.,
RFB,3×3 convolution,"The standard convolution using a 3×3 kernel, balancing context and efficiency.","In RFB, the mid-scale branch applies a 3×3 conv at dilation=3 over the input map.",
Octave Convolution,3×3 convolution,A local convolution with a 3×3 kernel that extracts spatial features while preserving much of the input resolution when stride=1.,Replacing all 3×3 vanilla convolutions in MobileNetV1 with OctConv (α = 0.5) to cut computation in half for low-frequency maps.,
Octave Convolution,3D convolution,"A convolutional operation that spans three dimensions (height, width, time) for video data, extending 2D conv to handle temporal information.",I3D inflates a 2D ResNet by replacing each 3×3 conv with a 3×3×3 conv to process 16-frame video clips.,
RFB,5×5 convolution,A convolution with a 5×5 kernel capturing broader context but with more parameters.,The large-scale branch in RFB applies 5×5 conv with dilation=5 on conv7 feature map.,
FPNet,ablation analysis,"Experimental methodology where one removes or alters specific modules (e.g., frequency-perception module) to quantify their individual contribution to overall performance.",Comparing FPNet’s performance with and without octave convolutions.,
FPNet,ablation study,A set of experiments where specific modules or settings are removed to assess their individual contributions. The paper likely performs ablation to justify the frequency-perception module and fusion strategies.,Disabling octave convolution to measure the drop in camo-frog detection accuracy.,
PVT,absolute positional bias,"Learnable scalar offsets added to raw attention scores based on token positions, enhancing the model’s ability to distinguish spatial relationships. Bias values help differentiate positions more flexibly than fixed embeddings.",PVT can add an absolute positional bias matrix to the QK^T scores before softmax to improve positional sensitivity.,
PVT,absolute positional bias,"A trainable scalar added to each query–key dot-product based on absolute token positions, distinguishing locations during attention.","In some PVT variants, an absolute bias matrix of shape (N×N) is added to the QKᵀ scores before softmax to encode fixed grid positions.",
PVT,absolute positional embedding,A grid of trainable vectors added element-wise to token embeddings to encode each patch’s spatial location. This compensates for the loss of position information after patch partitioning.,"PVT learns a 56×56×64 tensor of positional embeddings for Stage 1 and adds it to all 3,136 patch tokens before feeding them into the first Transformer encoder.",
RFB,activation function,"A non-linear transform (e.g. ReLU, Sigmoid) applied after conv to introduce non-linearity.","RFB uses ReLU, computing max(0,x) on each activation.",
RFB,activation map,The output feature map after applying a non-linear activation (e.g. ReLU).,ReLU maps negative conv outputs to zero before passing to next layer.,
SINet,activation map,A spatial map of neuron activations indicating the presence and strength of learned features at each location. Activation maps can be visualized as heatmaps to interpret where a network focuses.,"The search stage produces an activation map that highlights regions with potential camouflaged objects, guiding the identification stage to focus on those areas.",
SINet,Adam,An adaptive moment estimation optimizer that computes individual learning rates for each parameter using running averages of gradients and squared gradients. It often converges faster than vanilla SGD.,"With Adam’s adaptive updates, SINet quickly adjusts weights in the RF module to capture subtle texture patterns of camouflaged fish.",
FPNet,Adam optimizer,"An adaptive gradient method combining momentum and per-parameter learning rates based on first and second moments of gradients. Adam converges quickly in practice and handles sparse gradients well, making it a popular default for deep learning.","Training FPNet with Adam and default β₁=0.9, β₂=0.999 settings for fast convergence.",
SINet,alpha-matting annotation,"Fine-grained labeling that captures soft boundaries between foreground and background via per-pixel alpha values in [0,1], used for tasks like compositing and image editing.","COD10K includes alpha-matting masks requiring ~60 minutes per image, enabling SINet to be evaluated on soft-boundary accuracy.",
FPNet,anchor box,Predefined bounding boxes of various sizes and aspect ratios tiled across the image for proposal generation.,"Using anchors of sizes {32, 64, 128} px at ratios {1:1, 1:2, 2:1} to cover small to large camo-objects.",
RFB,anchor matching,"Synonym for default-box matching, referring to the IoU-based pairing of pre-defined anchor boxes to ground-truth boxes.","In SSD, anchor matching each iteration yields about 200 positive anchors out of 8732 total anchors per image.",
PVT,anchors-free detection,"Object detectors that don’t rely on pre-defined anchor boxes; instead, they predict object centers, offsets, or keypoints directly.","FCOS could be combined with PVT by predicting center heatmaps and regressions on PVT’s 28×28 feature map, eliminating anchors.",
PVT,AP50,Average Precision at IoU = 0.50; a simpler detection metric focusing on coarse localization.,"PVT-Small+RetinaNet scores 61.2 AP50, showing strong detection performance at a loose IoU criterion.",
PVT,AP75,AP at IoU = 0.75; emphasizes more precise bounding-box localization.,PVT-Small+RetinaNet’s AP75 of 43.7 highlights its ability to localize objects tightly.,
SINet,Apex,"NVIDIA’s library for mixed precision and distributed training in PyTorch, enabling automated loss scaling and FP16 support.","SINet’s training script imports apex’s amp to wrap the optimizer, achieving a 1.8× speed-up on COD10K segmentation.",
SINet,apex-mode O1,"A training mode in Apex that uses FP16 arithmetic for most operations and FP32 for dynamic loss scaling, balancing speed and numerical stability.",Training SINet under opt_level='O1' reduces memory per image from ~419 MB to ~305 MB on an RTX TITAN while preserving convergence behavior.,
SINet,apex-mode O2,"A more aggressive Apex mode that applies FP16 throughout except for batch‐norm and embedding layers, offering further speedup at potential precision trade-offs.","Switching to opt_level='O2' yields a ∼2× throughput boost when training SINet on large‐scale COD10K batches, at the cost of careful tuning.",
FPNet,aspect ratio,"The width-to-height ratio of an anchor or detection box, important for matching object shapes.",Defining an anchor with aspect ratio 2:1 to capture elongated camouflaged snakes.,
RFB,aspect ratio,"The width-to-height ratio of default boxes or bounding boxes; common values include {1:1, 2:1, 1:2, 3:1, 1:3}.","SSD300 uses aspect ratios 1:1, 2:1, 1:2 on conv4_3 map corresponding to small objects.",
RFB,Atrous convolution,"Another name for dilated convolution, where “holes” (zeros) are inserted between kernel weights to grow RF.","The RFB middle branch applies a 3×3 atrous conv with rate=3, sampling every 3rd neighbor.",
FPNet,Atrous spatial pyramid pooling (ASPP),"A context aggregation module employing parallel dilated convolutions at multiple rates. ASPP captures multi-scale context without reducing feature map resolution, aiding scenarios with varying object scales.","Using ASPP with dilation rates {1, 6, 12, 18} to gather context for camouflaged bird detection.",
RFB,Atrous spatial pyramid pooling (ASPP),A module applying multiple parallel atrous convolutions at different rates to capture multi-scale context in segmentation.,"ASPP might run 1×1, 3×3 (rate=6), 3×3 (rate=12), 3×3 (rate=18) in parallel.",
PVT,attention head,"One of the parallel subspaces in MHSA, each with its own projection matrices for queries, keys, and values, enabling focus on different features.","In an 8-head attention block, the 768-dim embedding splits into eight 96-dim heads, each computing separate attention weights.",
PVT,attention head splitting,Dividing the embedding dimension into multiple equal-sized “heads” so that each head performs its own attention computation. This allows the model to capture diverse relational patterns in parallel.,A 256-dim token in PVT-Base is split into 4 heads of 64 dims each for multi-head self-attention.,
FPNet,attention map,"A feature weighting map generated by attention mechanisms. In visual transformers or attention modules, these maps highlight relevant spatial regions or channels, guiding the model to focus on informative cues.",Visualizing self-attention weights that highlight a hidden frog on a leaf.,
SAM,attention map generation,The procedure by which channel and spatial attention maps are computed from feature maps.,"Generating Mc and Ms via pooled descriptors, MLP, and convolution.",
SAM,attention mechanism,"A computational strategy that adaptively weights feature responses to focus on informative parts of the data while suppressing uninformative ones. In CBAM, it consists of channel and spatial sub-modules. Attention mimics human visual focus to boost model performance and interpretability.",Self-attention in transformers weighs word embeddings; CBAM’s attention weighs feature map channels and pixels.,
PVT,attention score matrix,The matrix of raw similarities (before softmax) between every query and key vector pair. Its size is (N_queries × N_keys) and dictates how value vectors are aggregated.,"For Stage 2’s 28×28 grid, PVT forms a 784×784 score matrix per head prior to applying softmax.",
SINet,attribute label,"Flags assigned to each image/object indicating challenges like occlusion, small size, or out-of-view, used for subset evaluation.",An image of a partially hidden lizard behind leaves is tagged with the “Occlusion” attribute.,
RFB,average pooling,"Computing the average activation within each pooling window, smoothing responses.",Global average pooling on a 7×7 map yields one value per channel.,
FPNet,average pooling,"A downsampling operation that computes the average value within each window. Compared to max-pooling, it preserves more context but can blur sharp edges and reduce contrast in boundary regions.",Reducing feature map size by averaging each 3×3 neighborhood in a satellite image.,
Octave Convolution,backbone architecture,"The core CNN design (e.g., ResNet, DenseNet) onto which specialized layers or blocks (like OctConv) are plugged.",Integrating OctConv into a ResNet-50 backbone by replacing all 3×3 convolutions in each residual block.,
SINet,backbone network,"A pretrained convolutional neural network (e.g. ResNet-50) used to extract multi-scale feature maps (commonly denoted C3, C4, C5). These hierarchical features encode image patterns from low-level edges up to high-level semantic cues.","SINet uses ResNet-50 as its backbone, taking the output of its third, fourth, and fifth residual blocks (C3/C4/C5) to feed into the RF module and Search/Identification branches.",
SINet,background,The complement of the foreground: pixels not part of the object(s) of interest. Properly modeling background helps the network suppress false positives when objects blend into surroundings.,SINet learns to assign low confidence (pi≈0) to background textures such as bark or foliage that visually mimic the target object.,
RFB,backpropagation,The algorithm for computing gradients of the loss w.r.t. parameters via reverse-mode automatic differentiation.,Gradients from multi-task loss flow back through all RFB branches.,
SINet,backpropagation,"The gradient-based method for computing derivatives of the loss w.r.t. each model parameter, enabling end-to-end learning.","After each batch’s forward pass, SINet backpropagates L_total to update RF, SA, and PDC parameters via the Adam optimizer.",
RFB,base network (backbone),The core feature extractor (e.g. VGG16) providing intermediate feature maps to subsequent detection modules.,RFB Net uses VGG16 up to conv7 as its backbone.,
FPNet,batch normalization,"A layer that normalizes activations within each mini-batch to zero mean and unit variance, then scales and shifts with learnable parameters. It accelerates convergence, stabilizes training, and provides mild regularization against overfitting.",Normalizing feature maps after each convolution in a ResNet backbone during training.,
RFB,batch normalization,"Normalizing activations per mini-batch to zero mean, unit variance, accelerating training.",Adding BatchNorm after each conv reduces internal covariate shift.,
SINet,batch normalization,"A normalization layer that standardizes mini‐batch activations by subtracting the batch mean and dividing by the batch standard deviation, followed by learned affine scaling and shifting. This stabilizes training, allows higher learning rates, and acts as a regularizer.","Every convolution in SINet’s backbone and RF module is followed by batch normalization, ensuring stable gradient propagation and faster convergence during training on COD10K.",
FPNet,batch size,Number of samples processed per gradient update. Larger batches stabilize gradients but increase memory usage.,Training FPNet with a batch size of 8 on 512×512 images to fit within 11 GB of GPU memory.,
RFB,batch size,"The number of training samples processed in one forward/backward pass, affecting GPU memory and gradient noise.",SSD uses batch size = 32 across 4 GPUs (8 images each).,
SINet,batch size,The number of training samples processed before the model’s weights are updated once. Larger batches smooth gradient estimates but require more memory.,"SINet uses a batch size of 8 images on a 24 GB RTX TITAN, balancing GPU memory constraints with stable gradient updates.",
FPNet,benchmark dataset,"A publicly available dataset used to evaluate and compare model performance. In COD: COD10K, CHAMELEON, and CAMO are standard benchmarks.",Running FPNet on the COD10K test split to report MAE and F-measure scores.,
FPNet,benchmark protocol,"Standardized training/testing procedures on COD10K, CHAMELEON, and CAMO datasets, ensuring fair comparisons across methods.",Using the same train/test splits and data augmentation pipeline as prior COD studies.,
FPNet,benchmark protocol,"Prescribed procedure for training and evaluating on COD10K, CHAMELEON, and CAMO (train/test splits, pre-processing, metrics) to ensure fair comparisons across methods.",Following the COD10K paper’s train/test splits and metric computation exactly.,
FPNet,benchmark protocol,"Standardized procedure for training, testing, and reporting results on COD datasets. It ensures fair comparisons across methods by fixing splits and metrics.","Following COD10K’s official train/test split and reporting MAE, F-measure, and E-measure.",
SINet,big object (BO),"An attribute where the object-to-image area ratio exceeds 0.5, testing the model’s handling of dominant foregrounds.",A close-up of a large camouflaged leaf insect covering most of the frame receives the “BO” tag.,
FPNet,bilinear interpolation,A non-learnable upsampling technique that computes new pixel values via a weighted average of the four nearest neighbors. It preserves smooth gradients but cannot learn task-specific reconstruction patterns.,Upscaling a low-res prior mask to match the encoder feature size using bilinear interpolation.,
FPNet,bilinear interpolation,A non-learnable upsampling method that computes values by weighted averaging of four neighbors. It preserves smooth transitions but lacks task-specific adaptation.,Upscaling a 64×64 coarse mask to 128×128 in the decoder via bilinear interpolation.,
SINet,bilinear Interpolation,"A non‐learnable method that upsamples images or feature maps by computing output values via linear interpolation along both spatial dimensions, offering smooth resizing.","To enlarge the coarse search map, SINet uses bilinear interpolation, ensuring a smooth transition between pixel values when scaling from low to high resolution.",
SINet,binary cross-entropy (BCE) loss,A pixel-wise loss for binary classification that measures the distance between predicted probabilities and GT labels using the cross-entropy formula: –[y log(p)+(1–y) log(1–p)]. It’s the foundation of many segmentation objectives.,"Each identification output pixel (pi) is compared with its GT label (yi∈{0,1}) via BCE; misclassified camouflaged regions (yi=1 but pi<0.5) incur high penalty, pushing the network to correct its mask.",
FPNet,binary cross-entropy (BCE) loss,A loss function measuring the pixel-wise difference between predicted probabilities and binary ground-truth labels. It penalizes incorrect predictions and encourages the network to assign high confidence to true object pixels.,Calculating BCE between a predicted camo-snake mask and its annotated ground truth.,
SINet,bottleneck,"A network design pattern where the number of channels is reduced before expensive operations and then expanded back, reducing computational cost while retaining expressive power.","The MLP in SA uses a bottleneck: it first reduces the channel descriptor from 256 to 16 channels, applies a nonlinearity, then expands back—cutting down parameter count significantly.",
SAM,bottleneck attention module (BAM),"A concurrent work decomposing 3D attention into channel and spatial sub-modules, placed at ResNet bottlenecks.",BAM-ResNet-50 inserts BAM units inside each bottleneck block.,
Octave Convolution,bottleneck block,"A residual block variant that compresses channels via a 1×1 conv, processes them with a 3×3 conv, then expands back via another 1×1 conv.","In ResNet-50, each bottleneck block is [1×1 reduce] → [3×3 conv] → [1×1 expand] with a skip-connection around it.",
PVT,bottleneck dimension,"The intermediate size in an FFN’s hidden layer, typically smaller than or equal to the input/output embedding, controlling computation and introducing non-linearity.","With an embedding of 384 dims and MLP ratio 4, PVT uses a 1,536-dim bottleneck hidden layer.",
PVT,bottleneck hidden dimension,"The expanded size within an FFN sublayer, equal to (MLP ratio×embedding dim), controlling capacity.","With embedding = 320 and MLP ratio = 4 in Stage 3, the FFN hidden dimension is 1,280.",
FPNet,bottleneck layer,"A network block that reduces channel dimensions to a lower number before expanding again, improving efficiency.",Compressing a 512-channel feature into 128 channels in ResNet’s bottleneck block.,
RFB,bottom-up pathway,The standard forward pass that builds features from low-level to high-level through successive conv and pool layers.,VGG16’s conv→ReLU→pool blocks form a bottom-up pathway.,
FPNet,boundary F-score,A metric measuring alignment between predicted and ground-truth edges. It rewards precise boundary placement and penalizes deviations.,Computing the boundary F-score of 0.76 on CAMO to assess edge accuracy.,
FPNet,boundary loss,"A loss component that specifically penalizes discrepancies along object edges, sharpening boundary predictions by amplifying gradients near mask contours.",Adding a boundary loss term to tighten jagged edges around a camouflaged snail.,
FPNet,boundary refinement,"The process of tightening and correcting the edges of the detected mask. In FPNet, it is achieved by merging shallow high-resolution features with corrected deeper maps.",Applying a learnable boundary loss to refine the jagged outline of a segmented crab claw.,
SINet,bounding-box annotation,"Rectangular boxes drawn around each camouflaged object, providing coarse localization cues and facilitating comparisons with generic object detectors.",Each concealed crab on a rock receives a bounding box in COD10K for coarse localization benchmarking.,
RFB,box predictor,"The subnetwork that takes feature maps and outputs four localization offsets (Δx, Δy, Δw, Δh) for each default box, transforming anchors into predicted bounding boxes.","On conv9_2, a 3×3 convolution outputs 4 values per anchor for box regression.",
RFB,branch,"One distinct pathway in a multi-branch module, carrying out a specific kernel size and dilation before merging.",The RFB’s smallest-RF branch uses 1×1 conv followed by rate=1 atrous conv.,
FPNet,breakthrough points,"Subtle local deviations in texture or color where camouflaged object pixels “pop out,” guiding the coarse localization stage.",Spotting a single reflective scale that breaks the fish’s camouflage pattern.,
FPNet,breakthrough points,Local pixels where subtle spectral or textural differences “break through” camouflage. These serve as seeds for the coarse mask in the first FPNet stage.,Spotting a lone leaf vein that betrays a camouflaged insect on a bark surface.,
FPNet,breakthrough region,"Local image areas where subtle spectral or texture inconsistencies “break through” the camouflage, detected by the frequency-perception stage to seed the coarse mask.",Spotting a tiny texture deviation on bark that pinpoints a camouflaged insect.,
SAM,broadcast operation,Copying a lower-dimensional tensor along certain axes to match the shape of a higher-dimensional tensor for element-wise operations. CBAM broadcasts channel attention maps across spatial dimensions.,Broadcasting a 512×1×1 channel attention map to 512×7×7 by repeating values.,
SAM,broadcasting (multiplication broadcast),Expanding a smaller tensor along particular axes to align with a larger tensor for element-wise ops.,A 1×14×14 map is broadcast to 256×14×14 before multiplication.,
SAM,C (channels),The number of feature channels in a tensor. CBAM’s channel attention map length equals C.,A 64×56×56 map has C=64 channels.,
Octave Convolution,C2D,"A 2D CNN backbone applied frame-by-frame on video, often followed by temporal pooling or late fusion.",Applying ResNet-50 (C2D) independently to each frame of a video and averaging predictions over time.,
SINet,Calibrated MAE (CalMAE),"A post-processing calibration of MAE that compensates for global bias in predicted probabilities, e.g. by histogram equalization or optimal thresholding.",Applying the CalMAE routine reduces SINet’s raw MAE from 0.042 to 0.036 on a validation set of underexposed images.,
SINet,CalMAE,"A MATLAB tool that calibrates and computes MAE, often by finding an optimal shift or scale for pi before error calculation.","The call CalMAE(pred,gt) returns CalMAE=0.034, showing improved calibration on semi‐transparent jellyfish annotations.",
FPNet,CAMO,"The original camouflaged object dataset, containing images of camo patterns and hiding animals. Establishes baseline COD performance.",Comparing FPNet’s performance on ground-truth masks of camouflaged birds on tree bark.,
SINet,CAMO,"A 2,500-image COD dataset split into two 1,250-image subsets (CAMO and MS-COCO) covering eight categories. It provides object-level masks and bounding boxes, intended as a mid-sized benchmark for camouflaged segmentation.",Fine-tuning SINet on CAMO helps the network adapt to man-made camouflage patterns such as military gear against urban backdrops.,
FPNet,camouflage breakthrough point,"Subtle local cues (e.g., tiny texture or color deviations) that help a network break through the overall concealment. FPNet’s coarse stage seeks these “breakthrough” regions in the frequency domain.",Spotting a slight speckle on bark that pinpoints an insect’s location.,
FPNet,camouflaged object detection (COD),A computer-vision task aimed at locating and segmenting objects whose appearance closely matches their background. COD models often require specialized modules because standard saliency or segmentation methods fail when contrast between object and scene is low.,Detecting a snow hare blending into snowy terrain in wildlife photography.,
SINet,camouflaged object detection (COD),"A vision task focused on locating and segmenting objects that are deliberately blended into their backgrounds with minimal visual cues. Unlike generic detection, COD demands fine‐grained per-pixel classification to distinguish subtle object–background boundaries.",Detecting a stick insect whose body color and texture nearly match surrounding twigs and leaves.,
SAM,cardinality,"In ResNeXt, the number of parallel convolutional paths (“groups”). Higher cardinality can improve performance with controlled overhead.",ResNeXt-50 (32×4d) has 32 groups each with 4 filters.,
FPNet,CHAMELEON,"A smaller COD benchmark focusing on animals in natural settings, often used to test generalization of detection models.",Testing on chameleon images blending into green foliage.,
SINet,CHAMELEON,A small dataset of 76 images collected via keyword search (“camouflaged animal”) and manually annotated at the object level. It serves as an early benchmark for COD but lacks instance-level and matting annotations.,"Evaluating SINet’s baseline performance on CHAMELEON reveals its ability to segment large, well-defined camouflaged animals.",
FPNet,channel attention,A mechanism that computes per-channel weights—often via global pooling followed by an MLP—to re-weight feature map channels based on their global importance.,Using average pooling + FC layers to produce a 256-channel weight vector for frequency features.,
SAM,channel attention map,A 1×1×C tensor of values between 0 and 1 produced by the channel attention module to scale the original feature map’s channels. High values indicate more informative channels.,"A channel attention map [1.2, 0.8, ..., 0.05] boosts the first channel and suppresses the last.",
SAM,channel attention module,"The first sub-module of CBAM that generates a one-dimensional attention map across channels. It squeezes spatial dimensions via global average and max pooling, passes descriptors through a shared MLP with a reduction ratio, and applies a sigmoid gate to recalibrate channel importance.","For an input feature map of shape 512×7×7, the channel attention map is of shape 512×1×1 and highlights channels detecting edges.",
SAM,channel axis,The axis indexing feature channels in a tensor. CBAM’s channel attention operates along this axis.,"In F ∈ R^{512×7×7}, axis 0 (size 512) is the channel axis.",
SAM,channel descriptor,"The condensed vector of size C obtained from pooling spatial dimensions, used for channel-attention computation.",A 256-vector summarizing activations after conv3 in ResNet.,
SINet,channel descriptor,"A vector summarizing each channel’s global response across spatial dimensions, computed via pooling operations like GAP. Channel descriptors feed into attention modules to learn reweighting.","SA computes a 1×1×256 descriptor by applying GAP to the RF‐processed C5 tensor, capturing which channels matter most for camouflaged object detection.",
RFB,channel dimension,"The depth (number of filters) in a feature map tensor, each channel corresponding to one filter’s response.","After RFB concat, channel dimension grows from 512 to 1536.",
Octave Convolution,channel dimension,The first dimension of a 3-D feature map tensor that indexes distinct convolutional filters or feature types.,"In a 64-channel feature map of size 224×224, the channel dimension is 64.",
SINet,channel dimension,"The number of feature channels in a tensor, reflecting the network’s capacity to encode diverse patterns. More channels increase representational power but add parameters.","In SINet, C5 may have 2048 channels, carrying high‐level semantics into the RF module and SA for coarse object localization.",
SAM,channel dimension (C),"The number of feature channels in a feature map, each channel representing one filter’s responses. CBAM’s channel attention operates across this axis.",A feature map with C=512 channels indicates 512 filter responses.,
PVT,channel dimension growth,The progressive increase of embedding dimension (channel count) across pyramid stages to boost representational capacity at coarser resolutions. Increasing channels at higher stages enhances feature richness for complex tasks.,PVT grows its channel dims from 64 at Stage 1 up to 512 at Stage 4 to capture higher-level semantics.,
FPNet,channel grouping,"Dividing feature channels into disjoint groups that are processed separately, reducing computational cost and encouraging diverse feature representations within each group.",Splitting 128 channels into 4 groups of 32 for grouped convolutions in the frequency module.,
PVT,channel projection,A linear layer that changes the channel (embedding) dimension of token features. It’s used in FFNs and patch merging to adjust feature sizes for subsequent processing.,"When merging four 64-dim tokens into one in PVT’s patch merging, a 256×128 projection matrix reduces concatenated features to a 128-dim embedding.",
RFB,channel reduction,Using 1×1 conv to decrease channel count of a concatenated tensor to control model size.,A 1×1 conv maps 1536 channels down to 512 after RFB concat.,
FPNet,channel shuffle,A post-group-convolution operation that permutes channel order to allow cross-group information exchange. It mitigates the isolation caused by channel grouping.,Shuffling grouped outputs so high- and low-frequency information intermix before the next block.,
RFB,channel-wise concatenation,Stacking feature maps along the channel axis to combine multiple branch outputs.,Three 512-channel branch outputs yield a 1536-channel tensor.,
FPNet,channel-wise multiplication,"Element-wise product between a channel attention map (weights per channel) and feature map, emphasizing more informative channels while attenuating others.",Multiplying a 1×1×256 channel weight vector with a feature map to boost object channels.,
SAM,channel-wise multiplication,Broadcasting and multiplying a C×1×1 attention map across H×W to reweight channels.,Applying a 256×1×1 map to a 256×14×14 feature by repeating it spatially.,
SAM,channel-wise pooling,Reducing spatial dimensions by summarizing each channel via pooling (average or max). Provides channel descriptors.,Global average pooling of channel 10 yields the mean activation over all pixels in that channel.,
FPNet,checkpointing,Saving model parameter snapshots at intervals during training to enable resumption or rollback.,Storing FPNet’s weights every 10 epochs and retaining the best validation MAE checkpoint.,
SAM,CIFAR benchmarks,"Standard datasets (CIFAR-10/100) for evaluating small-scale image classification, used in network comparisons.",WideResNet-28-10 achieves 96.0% accuracy on CIFAR-10.,
FPNet,class imbalance,"Unequal ratio of object to background pixels, common when camouflaged objects occupy small image areas.",Adjusting loss weights in FPNet because camouflaged snakes cover <5% of most forest images.,
SINet,class-independent task,"A formulation where the model learns to detect objects without relying on specific category labels. COD is class-independent because camouflaged objects of any class must be found, regardless of category.","SINet assigns every pixel a confidence score pi, rather than one of K class scores, enabling it to detect unknown camouflage types.",
PVT,classification head,"A small prediction network (often an FFN) that takes per-token features and outputs class scores (e.g., object categories or pixel labels) for classification or segmentation.","For semantic segmentation, the head takes each 56×56 token from Stage 1 and projects it to a softmax over 150 ADE20K classes.",
RFB,classification head,"The branch of the detection head that predicts class confidence scores for each default box, usually via Softmax cross-entropy loss.","On conv11_2, the classification head is a 3×3 conv producing 21 scores (20 object classes + background) per anchor.",
RFB,classification loss,The softmax cross-entropy loss measuring error in class prediction for each default box.,A default box with true class dog gets −log(p_dog) added to classification loss.,
SAM,classification task,Predicting discrete labels (classes) for input images. CBAM improves classification accuracy by refining features.,ImageNet classification with 1 000 classes is a large-scale classification task.,
FPNet,closing,"A dilation followed by erosion, used to close small holes or gaps within segmentation masks.",Applying closing to fill small gaps inside the segmented rabbit mask.,
FPNet,co-occurrence feature,"A joint descriptor capturing how frequency and RGB cues co-vary at the same spatial locations, reinforcing regions where both domains signal potential camouflage.",Concatenating DCT-based texture maps with CNN color features to highlight a hidden chameleon spot.,
SINet,coarse,Referring to low-resolution or approximate representations that capture broad object locations but lack detailed boundaries. Coarse maps guide subsequent refinement stages.,"The Search stage outputs a coarse 16×16 attention map that roughly indicates where a hidden frog may appear, before upsampling and refinement.",
FPNet,coarse localization stage,"The first stage in FPNet where frequency cues guide the network to approximate object regions. It uses multi-level transformer features, the frequency-perception module, and neighbor interaction to produce a rough binary mask of candidate camouflaged areas.",Generating a low-res heatmap highlighting probable frog positions in marsh imagery.,
FPNet,coarse mask,A binary or probabilistic map highlighting rough object regions. Generated in FPNet’s first stage and used as a prior for subsequent fine localization.,A 64×64 downsampled mask roughly marking a hidden snake’s body in undergrowth.,
FPNet,coarse mask prediction,"Generation of an initial segmentation mask (low resolution, high recall) in Stage 1, capturing most object regions but with fuzzy boundaries.",Producing a 32×32 mask that highlights all potential bird regions in a forest image.,
PVT,coarse patch,"A relatively large patch (e.g., 16×16 pixels) used in classification-only Transformers like ViT, resulting in low-resolution outputs.","ViT uses 16×16 coarse patches at 224×224 resolution, yielding only 14×14 tokens—whereas PVT uses fine 4×4 patches for higher detail.",
PVT,coarse patch embedding,Flattening and projecting a coarse patch before feeding into a Transformer; analogous to PVT’s patch embedding but at larger patch size.,"A 16×16×3 patch (768 values) in ViT is projected to a 768-dim embedding, while PVT uses 4×4×3→64 embeddings at Stage 1.",
FPNet,COD10K,"A large camouflaged-object detection dataset containing 10,000 annotated images. Provides diverse scenes for robust benchmarking of COD algorithms.","Evaluating FPNet on images of camouflaged birds, insects, and sea creatures in COD10K.",
SINet,COD10K,"A large‐scale, densely annotated dataset of 10,000 images designed for camouflaged object detection research. It covers 78 object categories with hierarchical labels (category, bounding-box, object-level, instance-level, matting-level) and multiple challenge attributes.","Training SINet on COD10K enables it to generalize across aquatic, flying, amphibian, and terrestrial camouflaged species.",
FPNet,code release,"Public availability of implementation (e.g., GitHub link), facilitating replication and further development by the community.",Publishing FPNet’s PyTorch code and sample weights on GitHub for researchers.,
FPNet,color jitter,"A photometric augmentation altering brightness, contrast, saturation, or hue. It encourages the network to focus on structural cues rather than color biases, which is critical when camouflage relies on color similarity.",Randomly adjusting brightness and saturation of underwater images to prevent color overfitting.,
PVT,columnar structure,"A single-scale, uniform-resolution series of Transformer encoders (as in ViT), contrasted with PVT’s multi-scale pyramid.","ViT uses a columnar structure: 16×16 patches all the way through 12 identical encoder blocks, resulting in a single 14×14 feature map.",
PVT,columnar structure,"A network design where all Transformer encoders operate at a single, constant token resolution throughout the backbone. This contrasts with hierarchical pyramids that change resolution across stages.","ViT employs a columnar structure: it processes 14×14 tokens through 12 identical Transformer blocks, producing a single-scale feature map for classification.",
FPNet,community benchmark,"Widely adopted datasets and evaluation protocols (COD10K, CHAMELEON, CAMO) that ensure reproducible comparisons.",Following the official CAMO train/test split when evaluating FPNet’s performance.,
PVT,complexity reduction,"Techniques (e.g., pyramid shrinking, SRA) that lower the number of token interactions, reducing FLOPs and memory usage in deeper layers.","By halving token count each stage, PVT cuts computational complexity roughly by a factor of 4 per subsequent encoder stage.",
RFB,computational complexity,"The total floating-point operations (FLOPs) needed for forward inference, impacting latency.",RFB300 uses ~25 GFLOPs per image.,
PVT,computational complexity,"The relationship between input size (token count, embedding dim) and required operations (FLOPs), often expressed as O(N²) for self-attention. High complexity can bottleneck real-time applications.","PVT’s full self-attention without SRA would incur O(3,136²) operations at Stage 1, motivating its progressive shrinkage.",
PVT,computational cost,The amount of floating-point operations (FLOPs) required by the model; a key metric when comparing architectures.,"PVT-Small incurs ~3.8 GFLOPs on a 224×224 input, compared to ~4.6 GFLOPs for a comparable ResNet-50.",
RFB,computational cost,"The amount of compute (FLOPs, latency) required for forward (and sometimes backward) passes.",RFB adds ~5% extra FLOPs compared to baseline SSD head.,
FPNet,computational overhead,"Extra FLOPs or latency introduced by added modules (e.g., octave convolution) relative to the base backbone, measured during inference.",Measuring a 12 GFLOPs increase when integrating the frequency module into ResNet-50.,
RFB,concatenation,Stacking feature maps from different branches along the channel dimension to fuse multi-scale outputs.,The three branch outputs of sizes 512×38×38 each become a single 1536×38×38 tensor.,
SINet,concatenation,"A fusion operation that stacks tensors along the channel dimension, preserving all information while increasing channel capacity.","Although SINet uses summation for efficiency, other segmentation networks often concatenate encoder and decoder features to allow the decoder to access diverse cues directly.",
SAM,concatenation (along channel axis),The operation of joining two or more tensors along their channel dimension. Used in CBAM’s spatial attention to combine pooled maps.,Concatenating two 1×14×14 maps yields a 2×14×14 tensor.,
SINet,Conda environment,"An isolated Python environment managed by Anaconda/Miniconda, allowing reproducible package installation without affecting the system Python.",SINet’s README instructs: conda create -n SINet python=3.6 && conda activate SINet.,
FPNet,conditional random field (CRF),A probabilistic graphical model used in post-processing to refine segmentation masks by enforcing spatial and appearance consistency. CRFs can align predicted masks more closely with image edges and textures.,Running a dense CRF to refine mask boundaries around a hidden hedgehog.,
FPNet,confidence score,"Probability assigned to each final detection or pixel, reflecting model certainty.",Thresholding a pixel’s 0.6 confidence to decide if it belongs to the camo-turtle mask.,
RFB,confidence score,The predicted probability that a default box contains an object of a given class.,A detection is kept if its max confidence across classes exceeds 0.6.,
FPNet,context aggregation,"The process of gathering feature responses from multiple scales or regions (e.g., via pooling or dilated convs) to enrich local representations with broader scene information.","Using dilated convolutions of rates {1,2,4} to gather local and global cues for camo detection.",
SINet,contextual aggregation,The incorporation of information from large receptive fields or dilated convolutions to understand object‐level context amid cluttered backgrounds.,"The RF module’s large‐dilation branch aggregates features across a 15×15 region, helping SINet discern a camouflaged snake whose scales blend into the surroundings.",
FPNet,contour extraction,"The process of detecting and tracing the outlines of binary regions, often via algorithms like Canny edge detection or explicit contour finding routines.",Extracting the contour of a camouflaged beetle mask using OpenCV’s findContours.,
RFB,conv10_2,"The second convolutional layer in SSD’s conv10 block, creating a 3×3×256 map for even larger object detection.",conv10_2 anchors detect objects around 150×150–260×260 px.,
RFB,conv11_2,"The second convolutional layer in SSD’s conv11 block, a 1×1×256 feature map that captures the largest-scale objects or global context.","conv11_2’s single cell can cover the full 300×300 input, detecting very large objects.",
RFB,conv4_3,"The output feature map of VGG16’s fourth convolutional block, with shape 38×38×512 for a 300×300 input. It serves as the highest-resolution map for detecting small objects.",RFBNet inserts an RFB module on conv4_3 to enrich small-object features at 38×38 resolution.,
RFB,conv8_2,"The second convolutional layer in SSD’s conv8 block, yielding a 10×10×512 feature map for detecting slightly larger objects.",Anchors on conv8_2 cover object scales around 60×60–105×105 px in a 300×300 image.,
RFB,conv9_2,"The second convolutional layer in SSD’s conv9 block, producing a 5×5×256 feature map targeting larger objects.",Anchors on conv9_2 cover object sizes approximately 100×100–200×200 px.,
FPNet,convex intensity,A measure of pixel brightness arranged in a convex shape representing object regions. Early saliency and segmentation methods used convex intensity to identify blobs by assuming salient regions form convex clusters in color or intensity space.,Detecting circular fruits in an orchard image by grouping pixels into convex intensity clusters.,
RFB,convolution kernel,"A small, learnable weight matrix (e.g. 3×3) that slides over the input feature map performing dot-products to extract local patterns.",The central branch of RFB uses a 3×3 kernel to capture intermediate-scale texture.,
SAM,convolution operation,A sliding dot-product between a kernel and local regions of an input tensor. CBAM’s spatial attention uses a 7×7 convolution.,Applying a 7×7 filter on a 512×14×14 feature map to produce a 1×14×14 map.,
PVT,convolution-free pipeline,An end-to-end architecture built entirely from Transformer blocks—no convolutional layers or manually designed modules like anchors or NMS—so that all feature extraction is via self-attention.,"By combining PVT with a DETR decoder, one obtains a fully convolution-free detection system without any handcrafted anchor or non-maximum suppression steps.",
SAM,convolutional block,"A module in a CNN comprising one or more convolutional layers (often with batch normalization and activation), possibly with skip connections. CBAM attaches its attention modules immediately after each convolutional block.","A ResNet “bottleneck” block contains 1×1, 3×3, and 1×1 convolutions plus a skip connection.",
SAM,convolutional block attention module (CBAM),A lightweight plug-and-play attention module for convolutional neural networks that sequentially infers channel and spatial attention maps to adaptively refine feature maps. It enhances representation power by emphasizing informative features and suppressing irrelevant ones with negligible parameter overhead. CBAM is end-to-end trainable and can be inserted into any feed-forward CNN.,Integrating CBAM into each residual block of ResNet-50 yields a 0.8% top-1 accuracy boost on ImageNet-1K.,
SAM,convolutional feature,The activation responses of convolutional filters indicating the presence of learned patterns.,Edge detectors in early layers produce convolutional features highlighting edges.,
SAM,convolutional kernel,"The small filter tensor (e.g., 7×7×C_in×C_out) used in convolution to extract features. CBAM uses a single-channel 7×7 kernel in spatial attention.",A 7×7×2→1 kernel processes two concatenated pooling maps.,
PVT,convolutional kernel size,The spatial dimensions (height×width) of filters in a convolutional layer. Larger kernels capture broader context but cost more compute.,PVT’s convolutional stem uses 3×3 kernels in each layer to balance receptive field and efficiency.,
SAM,convolutional layer,A neural network layer that applies several convolution kernels to produce feature maps. CBAM’s spatial sub-module includes one convolutional layer.,A conv layer with 64 filters of size 3×3 converts a 3×224×224 image to 64×224×224.,
FPNet,convolutional neural network (CNN),A deep learning architecture built from convolutional layers that automatically learn spatial filters to detect patterns. CNNs excel at image tasks by hierarchically extracting low-level edges up to high-level object features through stacked convolutions and non-linearities.,Classifying road signs by learning edge and color patterns in traffic images.,
Octave Convolution,convolutional neural network (CNN),A deep learning model composed of layers that apply convolutional filters to extract spatial features from input data. CNNs are widely used for image and video tasks.,A ResNet-50 is a CNN that uses shortcut connections to ease training of very deep networks on ImageNet.,
SINet,convolutional neural network (CNN),"A deep architecture consisting of stacked convolutional layers that learn spatial hierarchies of features by applying trainable kernels. CNNs exploit weight sharing and local connectivity, making them highly effective for image‐based tasks.","SINet uses a CNN backbone (ResNet-50) pretrained on ImageNet to extract feature maps C3, C4, and C5 at different depths, supplying multi‐scale cues for camouflaged object localization and segmentation.",
PVT,convolutional stem,An optional initial set of convolution layers replacing the first patch embedding to extract lower-level features before tokenization.,"Some PVT variants add a 3-layer 3×3 convolutional stem to produce 64-dim features, which are then treated as patch embeddings at stride 4.",
PVT,convolutional stem,An initial stack of convolutional layers used in place of pure patch embedding to extract low-level features before the Transformer blocks.,"Some PVT variants include a three-layer 3×3 convolutional stem producing 64-dim features, which are then tokenized for the first stage.",
PVT,convolutional stride,"The step size at which a convolutional kernel moves over the input, controlling downsampling. A stride >1 reduces spatial dimensions.","In PVT’s stem, a stride of 2 halves feature map size from 224×224 to 112×112 in the first convolutional layer.",
FPNet,correction fusion module (CFM),A component in the fine localization stage that integrates the coarse prior with high-level semantics and cross-layer channel associations. It progressively corrects misaligned regions and strengthens feature consistency before boundary refinement.,Combining an initial mask with ResNet-deep features to remove false positives around foliage.,
FPNet,correction fusion module (CFM),"The network block in Stage 2 that incrementally integrates coarse priors with high-level semantics and shallow details, correcting false positives/negatives through cross-layer channel association.","Sequentially fusing a blurred prior mask, semantic maps, and high-res edges to refine a fish’s mask.",
FPNet,correction fusion module (CFM),The FPNet block in the fine stage that integrates the coarse prior with high-level semantics and shallow details. It performs stepwise corrections to refine mask accuracy and edge sharpness.,"Sequentially blending the 1/16 prior mask, C5 features, and early edges to correct false-positive specks.",
FPNet,correction fusion module (CFM),"The refinement block in FPNet’s second (fine) stage that takes the coarse prior mask plus high-level features and shallow edges, applies prior-guided corrections and cross-layer channel association, and produces the final mask.",Using the CFM to merge a 1/16-scale blurred prior with ResNet C5 semantics and early edges to sharpen a turtle mask.,
SINet,CPD1K,"The first camouflaged-person detection dataset, comprising 1,000 images in two scene types (woodland, snowfield) with pixel-wise person masks. It predates COD10K and targets human camouflage in natural environments.",Testing SINet on CPD1K demonstrates its transferability to person-detection scenarios where subjects wear ghillie suits.,
FPNet,CPU threads,Number of parallel CPU workers handling data loading and preprocessing to keep the GPU fed.,Launching 8 CPU threads in the DataLoader to augment and batch images concurrently.,
PVT,cross-attention,"In a Transformer decoder, attention where query tokens (e.g., object queries) attend to encoder-produced keys/values, enabling task-specific information flow.","In DETR, cross-attention lets each of the 100 object queries attend over the PVT backbone tokens to predict bounding boxes.",
SAM,cross-channel Information,Statistical dependencies and relationships among different channels in a feature map. CBAM’s channel attention captures these via pooled descriptors.,Channels detecting edges and textures may correlate and be jointly reweighted.,
FPNet,cross-domain evaluation,Testing a model trained on one dataset directly on another to gauge transferability.,Applying FPNet trained on CHAMELEON to the CAMO dataset and measuring performance drop.,
FPNet,cross-layer feature channel association (CLFCA),A technique where feature channels from different depths (layers) are grouped and re-weighed to emphasize consistent object cues. It strengthens the signal of channels that jointly predict camouflaged areas and suppresses uninformative ones.,Correlating channel 32 in layer 3 with channel 64 in layer 5 to boost true-positive activations.,
FPNet,cross-layer feature channel association (CLFCA),"A strategy to group and correlate specific channels from multiple depths, reinforcing those consistently signaling an object across layers while suppressing inconsistent channels.","Grouping channels 12, 48, and 96 from layers 3,4,5 that all fire on hidden gecko textures.",
FPNet,cross-layer feature channel association (CLFCA),"The process of grouping and re-weighting specific channels from multiple depths such that those consistently signaling the object are amplified, while inconsistent channels are suppressed before final fusion.",Correlating channel 12 from layer C3 with channel 48 from C5 and boosting them jointly when both detect a hidden bird.,
FPNet,cross-layer fusion,The process of merging feature maps across different depths. Critical in FPNet’s second stage to integrate semantic and boundary cues.,Concatenating ResNet C3 and C5 maps before a 1×1 conv to fuse textures with semantics,
FPNet,cross-validation,A statistical method where models are trained and validated on multiple fold splits. It assesses stability and variance when dataset size permits.,Running 5-fold cross-validation on a subset of COD10K to gauge FPNet’s performance consistency.,
RFB,daisy-shaped receptive field,"A spatial pattern of sampled RF points resembling petals around a center, emerging when combining multi-dilated branches.","Fig. 2 illustrates petals at offsets {±1,±3,±5} pixels forming a daisy shape.",
FPNet,data augmentation,"The process of generating varied training samples by applying transformations (e.g., flips, rotations). Augmentation improves model robustness to real-world variations and reduces overfitting when original datasets are limited.",Randomly rotating and flipping wildlife images during FPNet training.,
RFB,data augmentation,"Applying random transformations (flip, crop, scale, color jitter) to increase training data diversity.",SSD pipeline randomly crops patches ranging 0.3–1.0 of original image.,
SINet,data augmentation,"On-the-fly image transformations (e.g., flips, rotations, scaling) applied during training to increase data diversity and robustness.","SINet randomly flips each training image horizontally and scales it by [0.8,1.2], exposing the network to varied camouflaged object orientations.",
FPNet,data loader,"A software component that handles efficient data reading, augmentation, and batching for training.",Implementing PyTorch’s DataLoader with 4 worker threads to feed FPNet without I/O stalls.,
SINet,data split (train/test),The organization of datasets into disjoint subsets for model fitting and evaluation; SINet uses 6K images for training and 4K for testing in COD10K.,The README instructs copying COD10K‐train into ./Dataset/TrainDataset/ and COD10K‐test+others into ./Dataset/TestDataset/ for proper experiment setup.,
FPNet,data split protocol,"The specific allocation of images into training, validation, and test sets. Consistency in splits avoids overfitting and enables reproducible results.",Using CHAMELEON’s 70/30 train/test division when evaluating FPNet’s generalization.,
PVT,dataset-level pre-training,"Initial training on a large dataset (e.g., ImageNet) before fine-tuning on downstream tasks, allowing the backbone to learn generic visual features.",PVT backbones are pre-trained on ImageNet-1K classification then fine-tuned for COCO detection and ADE20K segmentation.,
SINet,decoder,"A subnetwork that transforms compact, low‐resolution feature representations back into high‐resolution predictions, often incorporating upsampling and skip connections.","SINet’s Partial Decoder Component acts as a lightweight decoder, reconstructing pixel‐level segmentation masks from fused multi‐scale features.",
FPNet,decoder bottleneck,A narrowed layer in the decoder path that processes compressed features before upsampling.,Applying a 1×1 conv to 256 channels in the final decoder stage to refine mask details.,
PVT,decoder query,"Same as query embedding; the input to each decoder layer’s cross-attention, dictating how the decoder attends to encoder features.","At decoder layer 2, query number 17 attends to Stage 4’s 7×7 tokens to refine the bounding box for object 17.",
RFB,deconvolution,Transposed convolution used to upsample feature maps by learned filters.,DSSD upsamples a 7×7 map to 14×14 via a 4×4 deconvolution with stride=2.,
RFB,default box,A pre-defined anchor box with specific size and aspect ratio placed at each spatial location of a feature map.,"On the 38×38 map of SSD300, a 30×30 px (1:1) default box is centered at cell (10,10).",
RFB,default-box matching,The process of pairing each default (anchor) box on feature maps with ground-truth boxes based on IoU criteria to generate positive and negative training samples.,"On conv4_3’s 38×38 map, SSD matches each of the 4 anchors per cell to VOC objects using IoU ≥ 0.5.",
RFB,deformable convolutional network (DCN),"A CNN that learns spatial offsets for each kernel tap, allowing adaptive RF shapes per object.","DCN might shift a 3×3 grid by offsets {±0.5,±1.2} pixels per position.",
PVT,dense anchor,"A grid of pre-defined bounding boxes at multiple scales and aspect ratios, used by many CNN-based detectors to propose candidate objects.",RetinaNet relies on dense anchors at each spatial location; PVT+RetinaNet inherits the same anchor scheme when used as the backbone.,
PVT,dense partitions,"Dividing an image into very small, non-overlapping patches (e.g., 4×4 pixels) so that high-resolution features are preserved for dense prediction.","PVT uses dense partitions of 4×4 pixels at Stage 1, yielding 56×56 tokens that retain fine spatial details for segmentation.",
PVT,dense prediction,"Tasks that require per-pixel or per-region outputs (e.g., segmentation, detection). These tasks need high-resolution, multi-scale representations rather than single‐label outputs.","Using PVT for semantic segmentation on ADE20K, the model predicts a class label for each pixel, leveraging its high-res feature maps.",
SAM,DenseNet,"A network where each layer concatenates all preceding feature maps, improving gradient flow and parameter efficiency.",DenseNet-121 has four dense blocks and three transition layers.,
SAM,depth,The number of layers or blocks in a network. Increasing depth can improve representation at the risk of vanishing gradients.,ResNet-152 has greater depth (152 layers) than ResNet-50.,
FPNet,depth map,"A per-pixel representation of distance from camera to scene. Though not used in this COD work, depth maps are common in RGB-D saliency and segmentation tasks to aid localization.",Leveraging Kinect’s depth output to segment a camo diver in underwater footage.,
Octave Convolution,depth-wise convolution,"A special group convolution where the number of groups equals the number of channels, i.e., each channel is convolved independently.","In MobileNetV1, a depth-wise 3×3 conv processes each channel separately, followed by a 1×1 “pointwise” conv to recombine channels.",
FPNet,depthwise separable convolution,"A factorized convolution that splits standard convolution into depthwise and pointwise steps. This reduces parameter count and computation while retaining representational power, enabling lightweight and efficient network designs.",Replacing a 3×3 conv with depthwise + 1×1 conv in a mobile-optimized encoder.,
FPNet,detail-preserving fine localization,"The second stage in FPNet that refines the coarse mask boundaries. It employs a Correction Fusion Module to merge high-level prior corrections with shallow high-resolution features, yielding the final, sharply delineated object mask.",Sharpening the edges of a rough turtle silhouette mask by fusing early CNN features.,
RFB,detection head,The subnet attached to each feature map that outputs class confidences and box offsets for its default boxes.,Each head on conv4_3 predicts 4 localization values and 21 class scores per default box.,
PVT,detection head,"A prediction module that outputs object bounding boxes and class probabilities from backbone features—can be anchor-based (e.g., RetinaNet) or anchor-free (e.g., FCOS).",PVT+RetinaNet attaches a detection head that consumes the 28×28 tokens from Stage 2 and predicts 9 anchors per location.,
SAM,detection task,Identifying and localizing multiple objects in an image with bounding boxes and class labels. CBAM-enhanced backbones boost detection performance.,Faster R-CNN with a CBAM-ResNet-50 backbone yields 40.2 mAP on COCO.,
RFB,detector architecture,"The overall design specifying backbone, neck (e.g. RFB), heads, and post-processing steps.","Fig. 3 shows RFB Net with VGG16 backbone, three RFB modules, and SSD heads.",
FPNet,Dice loss,"A region-based loss derived from the Dice coefficient, emphasizing overlap between prediction and ground truth. It is robust to class imbalance by directly optimizing the similarity of binary masks rather than individual pixel errors.",Optimizing the overlap score when segmenting camouflaged liver tumors in CT scans.,
FPNet,dilated convolution,"A convolution operation that inserts “holes” between kernel elements to enlarge the receptive field without increasing parameters. It captures broader context while preserving resolution, which can aid detection in low-contrast scenarios.",Using a 3×3 dilated conv with rate=2 to capture global tree canopy context around a camo bird.,
SINet,dilated convolution / dilation rate,A convolution operation where kernel elements are spaced apart (dilated) by a specified rate. This increases the effective receptive field—allowing the network to aggregate information over a larger region—without adding parameters or down‐sampling spatial resolution.,"A 3×3 filter with dilation 2 “sees” a 5×5 region; SINet uses rates {1, 4, 7} in different RF branches to jointly gather details and broad context around camouflaged targets.",
FPNet,dilation,"The inverse morphological operation to erosion, expanding foreground regions by adding pixels to object boundaries. It fills small holes in masks.",Dilating the eroded frog mask to restore its approximate shape after noise removal.,
RFB,dilation rate,"A parameter in dilated convolutions that spaces out kernel elements by inserting zeros, effectively enlarging the RF without adding weight.",A dilation rate of 2 on a 3×3 kernel yields an RF spanning a 5×5 neighborhood.,
FPNet,discrete cosine transform (DCT),"A variant of Fourier for real-valued signals widely used in image/video compression. Often used in frequency-guided vision, though FPNet uses octave convolution instead.",Using DCT coefficients in JPEG to highlight block artefacts around a hidden object.,
FPNet,domain shift,"Mismatch between training and testing data distributions (e.g., daylight vs. night scenes).",Training FPNet on daytime camouflage images and testing on dusk scenes to assess robustness.,
PVT,dot-product attention,"The core operation in self-attention: each query vector is compared against all key vectors via a dot product, producing similarity scores used to weight value vectors.","In Stage 3 of PVT, for 196 tokens, PVT computes a 196×196 matrix of query–key dot products in each head.",
FPNet,down-sampling,"The process of reducing spatial resolution, usually via pooling or strided convolution. It increases feature abstraction and receptive field but risks losing detail that is critical for tasks requiring precise boundary delineation.",Striding a 3×3 convolution by 2 to halve the height and width of feature maps.,
Octave Convolution,down-sampling,"The process of reducing a tensor’s spatial resolution, typically by pooling or strided convolution.",Applying a 2×2 max-pool with stride 2 to a 224×224 feature map yields a 112×112 map.,
PVT,downsampling,"Reducing the spatial resolution of features—via strided pooling, strided convolution, or patch merging—to enlarge receptive field and lower token count.",PVT uses strided patch merging that concatenates each non-overlapping 2×2 neighborhood of tokens and projects them to a lower token count for the next stage.,
RFB,downsampling,Reducing spatial resolution via pooling or strided convolution to enlarge RF.,A 3×3 conv with stride=2 halves both height and width of the feature map.,
SINet,downsampling,"The process of reducing spatial resolution, often via pooling or strided convolution. In SINet, GT masks are downsampled to match the low-res search map for supervision.","The 256×256 binary GT mask of a camouflaged crab is max-pooled to 16×16, producing a coarse search target that guides the Search stage’s 16×16 output.",
PVT,downsampling block,A stage transition module combining patch merging and projection to reduce spatial resolution and prepare the sequence for deeper layers.,The block from Stage 2 to Stage 3 in PVT halves token count from 28×28 to 14×14 and increases embedding dims from 128 to 320.,
PVT,DropPath,"Also known as stochastic depth, this regularization technique randomly drops entire residual branches during training. It encourages the model to learn more robust representations and reduces overfitting.","PVT uses DropPath with a rate of 0.1, randomly skipping some Transformer blocks on each training step.",
RFB,DSSD (deconvolutional SSD),An SSD variant that adds deconvolutional upsampling and skip connections to improve small-object detection.,"DSSD513 uses ResNet-101 and deconv layers, achieving 33.2 mAP at 13 FPS.",
FPNet,E-measure,A metric combining local pixel and global region metrics to evaluate structural similarity between prediction and ground truth.,Scoring an Eξ of 0.90 that reflects both accurate edges and region shapes.,
FPNet,early stopping,"Halting training when validation performance plateaus for a set number of epochs, preventing overfitting and saving compute resources.",Stopping training if validation MAE does not improve for 10 consecutive epochs.,
FPNet,early stopping,Automatically halting training when validation performance stops improving to prevent overfitting.,Stopping FPNet training after 10 epochs without validation loss decrease.,
RFB,eccentricity,"The distance from the center of gaze (fovea) to a sampling point in visual field coordinates. In RFB, dilation rates simulate increased eccentricity.",A dilation rate of 3 in a 3×3 convolution samples neighbors at 3 pixels away from the center.,
RFB,eccentricity ratio,"The ratio of RF size to its distance from RF center, controlling emphasis on center vs. periphery.","In RFB, size=5, dilation=3 gives eccentricity ratio=5/3≈1.67.",
SINet,edge texture,Local image patterns at object boundaries combining gradient orientation and intensity changes. Edge textures signal where foreground and background regions meet.,"In the Search Attention module, channels that respond to edge textures (e.g., wing outlines against leaf veins) receive higher weights, sharpening the coarse attention map.",
FPNet,element-wise multiplication (⊗),"A binary tensor operation that multiplies corresponding elements from two feature maps. In COD, it’s often used to fuse spatial priors (coarse masks) with feature activations.",Multiplying the coarse mask by deep features to focus refinement on likely object regions.,
SAM,element-wise multiplication (⊗),The operation that multiplies two tensors of the same shape by multiplying corresponding elements. CBAM uses it to apply attention maps to feature maps.,Multiplying a 512×7×7 attention map element-wise with a 512×7×7 feature map.,
SINet,element‐wise multiplication,"A fusion operation that multiplies corresponding elements of two equally sized tensors, enabling gating and feature modulation.","In the identification stage, SINet multiplies the upsampled search map with the RF‐processed C3 features, allowing the coarse attention to selectively gate relevant fine‐grained information.",
SINet,element‐wise summation,"Adding corresponding elements of two equally sized tensors, a lightweight fusion that combines information without increasing channel count.","The Partial Decoder Component sums upsampled features from C5 and C4, merging coarse semantic cues with mid‐level detail maps to reconstruct high‐fidelity segmentation masks efficiently.",
PVT,embedding dimension,"The size of each token vector after patch embedding, which remains constant across MHSA and FFN sublayers.","PVT-Base uses an embedding dimension of 768, so each patch embedding is a 768-element vector.",
PVT,embedding normalization,"The application of layer normalization to token embeddings, stabilizing training by normalizing feature distributions across channels. Normalization improves numerical stability and speeds up convergence.",PVT applies layer normalization to each token embedding before feeding it into both the MHSA and FFN sublayers.,
PVT,embedding projection,A linear transformation that maps each flattened patch vector into a fixed-length token embedding. It learns a matrix of weights to convert raw pixel data into the Transformer’s feature space.,PVT uses a 48×64 projection matrix to turn each 48-dim flattened patch into a 64-dim token at the first stage.,
FPNet,encoder bottleneck,"The deepest, most compressed layer in the encoder path, capturing global context.",Taking the 1/32-scale ResNet output as the encoder bottleneck before coarse FPNet localization.,
FPNet,encoder-decoder architecture,"A two-part design where an encoder progressively reduces spatial dimensions to extract semantics, and a decoder upsamples to recover spatial resolution for tasks like segmentation. Skip connections often link encoder and decoder layers to preserve fine details.",U-Net’s down-up structure used to segment medical scans by reusing encoder details in the decoder.,
FPNet,encoder-decoder architecture,"A two-part design where the encoder compresses spatial information into high-level features and the decoder reconstructs spatial resolution, often with skip connections to preserve detail.",Using a U-Net backbone where downsampling and upsampling paths connect via skip links.,
SAM,encoder-decoder style attention module,An attention design that first encodes features to a lower resolution and then decodes to generate attention masks.,The Residual Attention Network uses encoder-decoder masks for spatial attention.,
PVT,encoder–decoder architecture,"A network pattern with an “encoder” (backbone) that extracts features and a “decoder” (task head) that generates final predictions, often via cross-attention in Transformers.","PVT serves as the encoder; when paired with DETR, the Transformer decoder acts as the detection head via cross-attention.",
FPNet,end-to-end learning,"A strategy where all parts of a model are trained jointly from raw input to final output. It obviates the need for manual feature design or separate module optimization, allowing gradients to propagate through every component for unified performance gains.",Training FPNet from input images directly through both its coarse and fine stages in one optimization run.,
SAM,end-to-end trainable,The ability to learn all module parameters jointly with the base network via backpropagation. CBAM is fully differentiable.,"On ImageNet, CBAM-augmented networks learn both convolution and attention weights simultaneously.",
PVT,end-to-end training,"Optimizing all components of a model jointly from input to final loss, without separate stage-wise training or manual heuristics.","The PVT+DETR pipeline is trained end-to-end, from raw images and query embeddings all the way to bounding-box/loss predictions, without anchor/RPN pre-training.",
SINet,end‐to‐end learning,"Training all network components jointly in a unified pipeline, enabling feature extractors and task‐specific modules to co‐adapt for optimal performance.","SINet’s backbone, RF module, SA, and PDC are optimized together via backpropagation on the COD10K dataset, learning to detect and segment camouflaged objects in one cohesive process",
SINet,enhanced-alignment measure (Eξ),A perceptual measure assessing global and local similarity by aligning binary maps with GT before comparison. Eξ emphasizes meaningful structural matches and penalizes misaligned predictions.,"After thresholding, SINet’s camo mask vs. GT yields Eξ=0.765, showing strong alignment of concealed snake contours.",
SINet,Enhancedmeasure,A MATLAB routine that implements the E-measure (Eξ) by aligning and summing over all pixels.,"Using Enhancedmeasure(pred,gt) outputs Eξ=0.772, highlighting SINet’s alignment quality on a moss‐covered salamander image.",
FPNet,epoch,One full pass through the entire training dataset.,Training FPNet for 60 epochs to ensure convergence on COD10K.,
RFB,epoch,"One full pass through the entire training dataset. During an epoch, the model sees every training sample exactly once and updates its weights multiple times.",Training RFBNet on Pascal VOC for 240 epochs means processing all ~21 000 images 240 times.,
SINet,epoch,One full pass through the entire training dataset. Multiple epochs allow the model to iteratively refine its parameters.,"Training SINet for 40 epochs over COD10K yields diminishing returns after epoch 30, indicating convergence.",
FPNet,erosion,A morphological operation that shrinks foreground regions by eroding boundary pixels. It is useful for detaching weakly connected mask components.,Eroding small false-positive specks in a bird’s camo mask to isolate the main silhouette.,
SINet,evaluation toolbox,"A collection of MATLAB scripts (Fmeasure_calu, original_WFb, Enhancedmeasure, S_object, S_region, StructureMeasure) to benchmark COD models under uniform metrics.","After training SINet, users run main.m in the EvaluationTool folder to generate all metric scores on the COD10K‐test set.",
SAM,excitation operation,The transformation of a squeezed vector through an MLP to generate gating weights (channel attention).,Passing the 512-vector through W₀→ReLU→W₁→σ to excite channels.,
FPNet,expectation-maximization (EM),"An iterative statistical algorithm for estimating latent parameters in models with incomplete data. In vision, EM can segment regions by alternating between estimating class labels (E-step) and updating Gaussian or mixture parameters (M-step).",Segmenting foreground animals by fitting a two-component Gaussian model on pixel intensities.,
SAM,F ∈ R^{C×H×W},"Mathematical notation denoting an input feature map tensor with C channels, height H, and width W.",F ∈ R^{256×14×14} describes a mid-level ResNet activation.,
SINet,F-measure (Fβ),A harmonic‐mean metric combining precision (P) and recall (R) as (1+β²)·P·R / (β²·P+R). β>1 weights recall more heavily; β<1 weights precision. It evaluates segmentation quality in a single score.,"With P=0.92, R=0.85, and β=1, the F₁ score =2·0.92·0.85/(0.92+0.85) ≈ 0.88.",
SAM,f^{7×7} convolution,Notation for a convolutional layer with a 7×7 kernel used in CBAM’s spatial attention.,f^{7×7}([F_avg; F_max]) denotes spatial attention’s conv operation.,
RFB,fc7 (converted),"The original VGG16 fully connected layer “fc7” converted into a 1×1 convolution (conv7), producing a 19×19×1024 feature map used for medium-sized object detection in SSD and RFB Net.",SSD’s conv7 (from fc7) detects objects in the ~60×60–105×105 px range on a 300×300 image.,
SINet,feature aggregation,"The process of combining feature maps from different network depths or branches to enrich representations with multi‐scale and multi‐context information. Aggregation can occur via summation, concatenation, or more complex fusion strategies.","In the Partial Decoder Component, SINet aggregates high‐level attention maps from C5 and detailed mid‐level features from C4 by upsampling both to the same size and summing them to form a refined prediction map.",
FPNet,feature alignment,"Adjusting feature maps from different network stages or modalities to a common spatial resolution and distribution, ensuring consistent fusion.",Upsampling a 1/16-scale feature map to match a 1/8-scale map before addition.,
RFB,feature channel,An individual slice of the channel dimension representing one filter’s activation map.,Channel 23 of conv1_1 often detects horizontal edges.,
SINet,feature channel,"One of the parallel “slices” along the channel dimension of a tensor, each representing a distinct learned filter response. The number of channels reflects a layer’s capacity to encode diverse patterns.","C5 typically has 2,048 feature channels; SA computes a 1×1×2048 descriptor via GAP to assess each channel’s relevance for camouflage detection.",
FPNet,feature decomposition,Splitting a feature map into constituent parts—such as semantic vs. spectral or high-freq vs. low-freq—so each can be processed and fused selectively in network modules.,Separating channel activations into texture maps and contour maps before fusion.,
SAM,feature detector,A convolutional filter whose activation indicates detection of a specific pattern or attribute in the input.,A Gabor-like filter in conv1 responds to horizontal edges.,
RFB,feature discriminability,The ability of extracted features to distinguish between different object categories or patterns.,"Adding RFB increased mAP on small objects by 4 points, showing better discriminability.",
RFB,feature extraction,The process of computing hierarchical representations from input images via CNN layers.,VGG16’s first five conv blocks extract edges→textures→parts→objects.,
SINet,feature extraction,"The process of transforming raw input (e.g., an image) into a set of descriptive features (e.g., edges, textures, semantics) via learned filters in convolutional layers.","ResNet-50’s early layers extract low-level edge and texture features, which SINet’s RF module then refines for camouflaged object localization.",
FPNet,feature fusion,"Combining multiple feature sources—such as RGB, frequency, coarse mask, and shallow details—via concatenation or weighted sum to produce richer representations.",Concatenating coarse mask and deep semantic map before a 1×1 conv fusion layer.,
SINet,feature fusion,"Combining multiple feature maps—across scales, depths, or branches—into a unified representation, enhancing the network’s descriptive capacity. Fusion strategies include concatenation, summation, and attention‐based weighting.",SINet fuses C5 attention outputs and C4 detail features in PDC to leverage both global localization and boundary precision for segmenting camouflaged objects.,
FPNet,feature hierarchy,Layered organization of representations from low-level details to high-level semantics. Effective COD leverages cues across this pyramid in both frequency and RGB domains.,Fusing ResNet C2 edges with C5 semantics to locate a camouflaged bird.,
PVT,feature map,A 2D grid of activation vectors (tokens) representing learned features at a particular spatial resolution.,"After Stage 2, PVT outputs a feature map of size 28×28 tokens, each with a 128-dim embedding.",
RFB,feature map,A two-dimensional array of activations produced by convolving an image (or previous map) with a set of filters. Each channel encodes responses to a particular learned pattern.,conv4_3 output in SSD300 is a 38×38×512 tensor used for detecting small objects.,
SAM,feature map,A three-dimensional array (channels×height×width) of activations produced by convolutional filters applied over an input. CBAM refines these maps to improve downstream accuracy.,"In VGG16, the output of conv3_3 is a feature map of shape 256×56×56.",
Octave Convolution,feature map,"A 3-D tensor output by a convolutional layer, with dimensions corresponding to channels, height, and width. Each element encodes local responses over a spatial window.","After a 3×3 convolution on a 224×224 RGB image, you might get 64 feature maps of size 224×224.",
SINet,"feature map (C3, C4, C5)","Intermediate tensor outputs of the backbone, where C3, C4, C5 denote the feature maps from the 3rd, 4th, and 5th stages respectively. They differ in spatial resolution and feature abstraction level, enabling both fine‐detail and context modeling.",C5 (the deepest map) captures broad object‐level semantics—useful for rough localization—while C3 preserves fine textures needed for boundary refinement in the Identification stage.,
FPNet,feature map resolution,"Spatial dimensions (height×width) of intermediate feature maps, affecting granularity of predictions.",Using a 64×64 feature map for the coarse mask before upsampling to the original 512×512 grid.,
FPNet,feature map scale,"Downsampling factor relative to the input size (e.g., 1/8, 1/16), indicating level in the semantic hierarchy.","Processing 1/16-scale tokens in the transformer for global context, then fusing with 1/4-scale edges.",
FPNet,feature projection,"A 1×1 convolution or linear layer aligning channel dimensions when fusing streams. It enables seamless merging of RGB, frequency, and prior mask features.",Projecting a concatenated 512-ch frequency+RGB map down to 256 channels before fusion.,
FPNet,feature pyramid network (FPN),"A multi-scale feature extraction backbone that builds a top-down pathway with lateral connections. FPN combines high-level semantics with fine spatial resolution, improving detection across object sizes.","Attaching an FPN atop ResNet to provide 1/8, 1/16, and 1/32 resolution features for FPNet’s stage 1.",
RFB,feature pyramid network (FPN),"A top-down pathway that merges high-level semantic feature maps with lower-level, higher-resolution maps to build a feature pyramid.","FPN fuses conv5_3 features (7×7) with conv4_3 (14×14), conv3_3 (28×28), etc.",
PVT,feature pyramid network (FPN),"A multi-scale feature aggregation module that fuses backbone maps from different resolutions via lateral and top-down connections, yielding rich multi-scale outputs for detection/segmentation.",PVT+FPN takes PVT’s four outputs (56×56→7×7) and fuses them into a unified feature pyramid for RetinaNet.,
FPNet,feature redundancy reduction,"Process by which octave convolution trims duplicate information in adjacent high- and low-frequency channels, lowering computation and focusing on salient differences.",Halving channel count in low-freq stream to remove overlapping smooth gradient data.,
SAM,feature refinement,The process of improving raw feature representations by emphasizing relevant signals and attenuating noise via attention.,CBAM refines conv block outputs to better localize objects.,
RFB,feature robustness,The resilience of feature activations to image perturbations like noise or slight translations.,RFB’s high-eccentricity branch reduced localization jitter under random translations.,
PVT,feature stride,"The effective downsampling ratio between input pixels and output tokens (e.g., stride = 4 means each token covers a 4×4 pixel region).","PVT’s Stage 2 feature stride is 8, so each of the 28×28 tokens aggregates an 8×8 pixel patch from the original image.",
SAM,feed-forward convolutional neural network,A class of CNNs where data flows in one direction—from input through successive convolutional (and other) layers to the output—without recurrent loops. CBAM is designed to integrate seamlessly into such architectures.,"Standard ResNet, VGGNet, and DenseNet are feed-forward CNNs processing images in a forward pass only.",
FPNet,feed-forward network (FFN),A two-layer perceptron applied independently to each token in a transformer block; introduces non-linearity and increases model capacity between attention layers.,Applying a 2048→512 MLP to each token to mix channel information post-attention.,
FPNet,feed-forward network (FFN),A two-layer MLP applied to each token in a transformer block. It introduces non-linearity and channel mixing after attention.,Applying a 3072→768 MLP to each token post-attention in ViT-Base.,
PVT,feed-forward network (FFN),"A two-layer fully connected network with a non-linear activation (e.g., GELU), applied independently to each token after MHSA, for per-token transformation.","Each token in Stage 3 is passed through an FFN that first expands its 384-dim embedding to 1,536 dims and then projects back to 384 dims.",
SINet,fetch code,"A short, user-supplied string to retrieve large files (datasets or pretrained models) from shared cloud storage (e.g., Google Drive/Baidu Pan).","To download COD10K‐train, users enter the fetch code djq2 on the Baidu Pan link provided in the README.",
FPNet,few-shot learning,Adapting a pretrained model to new classes or domains using only a handful of labeled examples.,Fine-tuning FPNet on five annotated images of a newly discovered camouflaged frog.,
RFB,final representation,The merged feature map output (after concat and 1×1 conv) of a module ready for detection head input.,RFB’s 512-channel final map is fed into classification and regression layers.,
SINet,fine-grained,"Referring to small-scale, detailed information in an image, such as the thin legs of an insect or minute color variations. Fine-grained modeling is crucial when object and background appearances closely match.",The Identification stage uses fine-grained cues from C3 to recover delicate details like hair-like protrusions on a camouflaged spider.,
PVT,fine-grained image patches,"Small, fixed-size pixel blocks (e.g., 4×4) that are flattened into tokens before feeding into the Transformer encoder.","PVT uses 4×4 patches on a 224×224 image to create (224/4)² = 56² = 3,136 tokens at Stage 1, enabling high-resolution feature extraction.",
FPNet,fine-tuning,The process of adapting a pretrained model to a new task by continuing training on task-specific data. It leverages learned representations while allowing parameters to specialize for camouflaged object patterns.,"Fine-tuning the last 3 ResNet stages on 5,000 labeled COD10K images.",
PVT,fine-tuning,"Continuing training of a pre-trained model on a new task, typically with a smaller learning rate, to adapt features to the target domain.",The PVT backbone is fine-tuned on COCO detection for 12 epochs with a 1× learning schedule.,
RFB,fine-tuning,"Further training of a network initialized from pretrained weights (e.g., ImageNet) on a target task with a lower learning rate to adapt existing features.","After loading ImageNet-trained VGG16, RFBNet is fine-tuned on VOC2007 for 50 epochs with lr=1e-4.",
PVT,flatten operation,Converting each 2D patch (height×width×channels) into a 1D vector by concatenating its values. This prepares the patch for linear projection into the embedding space of the model.,A 4×4 patch with 3 channels is flattened into a 48-element vector before being mapped to a 64-dimensional token embedding.,
FPNet,FLOPs,Floating-point operations required by a network during inference. It quantifies computational overhead introduced by added modules.,Measuring that octave convolution adds 5 GFLOPs on top of the backbone.,
PVT,FLOPs,"Floating-point operations; a measure of computational complexity (e.g., billions of FLOPs) used to compare model efficiency.","PVT-Tiny incurs 1.9 GFLOPs on a 224×224 input, roughly half that of a ResNet-50 (4.1 GFLOPs).",
SINet,Fmeasure_calu,A MATLAB script that computes standard F-measure with user-defined β and adaptive thresholding.,"Running Fmeasure_calu(pred,gt,1) yields an F₁ score of 0.883 for the predicted butterfly mask at the optimal threshold.",
FPNet,focal loss,"A modified version of cross-entropy that down-weights easy examples and focuses training on hard, misclassified pixels. In COD, it helps the model learn subtle object signals buried in background noise.",Using focal loss to emphasize boundary pixels of a camouflaged turtle that are often misclassified.,
SINet,foreground,"The set of pixels in an image belonging to the object(s) of interest, as opposed to the background. In COD, foreground pixels form the camouflaged object mask.","In a leaf-insect image, all pixels covering the insect (legs, body, wings) are considered foreground in SINet’s ground-truth mask.",
FPNet,Fourier transform,"A mathematical transform converting spatial data into frequency components. While FPNet does not explicitly perform a Fourier transform, its frequency-perception module embodies the same principle of decomposing high/low frequencies.",Computing FFT on a 256×256 crop to reveal hidden periodic textures in fabric.,
FPNet,frequency cues,Information obtained by analyzing or filtering features in the frequency domain. They can reveal hidden texture and shape contrasts that are invisible in RGB.,Using high-pass filters to detect subtle vein patterns on a camouflaged toad.,
FPNet,frequency domain,"Representation of image content in terms of sinusoidal components (e.g., low/high frequency). High frequencies capture edges and textures; low frequencies capture overall shapes and illumination gradients.",Applying a Fourier transform to isolate edge information around a hidden insect.,
FPNet,frequency filter bank,"A collection of parallel convolutional kernels that imitate band-pass filters, each isolating a specific spectral band (e.g., low vs. high frequencies) for separate processing.","Using three parallel convolutions to capture low, mid, and high frequency cues around a hidden bird.",
FPNet,frequency perception network (FPNet),A two-stage deep network that fuses RGB features and frequency-domain cues to detect camouflaged objects. FPNet’s first stage generates a coarse mask via a learnable frequency-perception module; the second stage refines boundaries through cross-layer fusion and guided correction.,Using FPNet to spot hidden lizards on forest floor images by first isolating texture cues.,
FPNet,frequency perception network (FPNet),A plug-in block that splits feature maps into learnable high- and low-frequency components. It employs octave convolution to extract texture (high-freq) and contour (low-freq) cues and then fuses them for coarse object localization.,Integrating a drop-in module into ResNet that isolates texture bands for object spotting.,
FPNet,frequency perception network (FPNet),"The plug-and-play block in FPNet’s first (coarse) stage that decomposes backbone features into learnable high- and low-frequency streams via octave convolution, fuses them (with neighbor interaction), and outputs the coarse mask.",Inserting the FPM into a ResNet backbone to isolate texture edges of a camouflaged snake for coarse localization.,
SINet,fully connected (FC) layer,"A neural network layer in which each output neuron is connected to every input neuron, implementing an affine transform followed by a nonlinearity. FC layers are typically used to learn global, high-level relationships.","In the Search Attention (SA) module, SINet’s MLP comprises two FC layers that shrink the 256-channel descriptor to 16 channels and then expand back to 256.",
FPNet,fully connected (FC) layer,"A dense neural network layer where every input node connects to every output node. FC layers are used for tasks requiring global context integration, such as generating channel weights or final classification scores.",Using an FC layer to predict per-channel gating weights from pooled features.,
SAM,fully connected (FC) layer,A layer where every input neuron connects to every output neuron via learned weights. CBAM’s MLP uses two FC layers.,An FC layer maps a 512-vector to a 32-vector via a weight matrix of size 32×512.,
FPNet,gating mechanism,"A learnable module (often using sigmoid activations) that dynamically controls the flow of information—e.g., selecting which frequency band or channel to emphasize before fusion.",Generating a sigmoid gate to blend high- and low-freq streams based on current context.,
SAM,gating mechanism,A learned multiplicative gate (via sigmoid) that modulates information flow. CBAM uses gates for both channel and spatial attention.,A sigmoid gate of 0.2 suppresses a feature map region by 80%.,
SINet,gating mechanism,"Any operation that uses one signal to modulate another, typically via element‐wise multiplication, allowing context‐dependent feature selection.","The identification stage’s gating multiplies the coarse search map with fine RF features, enabling SINet to focus on relevant regions and suppress background noise.",
SINet,gating operation,"A fusion mechanism where one feature map modulates another via element-wise multiplication, enabling context-driven filtering.",The identification stage multiplies the upsampled search map (gate) with RF-processed C3 features to suppress irrelevant background.,
FPNet,Gaussian smoothing,A post-processing filter that applies a Gaussian kernel to soften mask predictions and reduce spurious noise. It spreads activation values smoothly but may blur fine boundaries if over-applied.,Applying a 3×3 Gaussian blur on the predicted mask to remove isolated spikes.,
PVT,GELU activation,"Gaussian Error Linear Unit; a smooth, non-linear activation function often used in Transformers instead of ReLU to improve gradient flow.","In every FFN of PVT’s Transformer encoders, GELU is applied between the two linear projections.",
FPNet,generalizability assessment,"Evaluation of a trained model’s performance on unseen datasets or under varied conditions (multiple objects, occlusion, indefinable boundaries) to gauge robustness beyond the benchmarks.",Testing FPNet trained on COD10K directly on wildlife art images and measuring drop in F-measure.,
SINet,generic object detection (GOD),A class-dependent task that detects and classifies objects into predefined categories using bounding boxes or segmentation masks. It includes subtasks like semantic and panoptic segmentation.,Panoptic segmentation models trained for GOD can misclassify camouflaged objects as background due to low contrast.,
PVT,global attention,"Self-attention computed over the entire token sequence without locality constraints, capturing long-range dependencies.","Each token in MHSA at Stage 4 attends to all 49 tokens, modeling object co-occurrence across the whole image.",
FPNet,global average pooling (GAP),A layer that computes the mean value of each channel over spatial dimensions. It reduces spatial maps to a vector for classification or channel weighting while preserving global context.,Summarizing a 64×64 feature map into a 1×1×128 descriptor before channel attention.,
FPNet,global average pooling (GAP),"A pooling layer that replaces each channel’s entire spatial map with its mean value. It summarizes global context into a single descriptor per channel, often used for channel weighting or classification heads.",Collapsing a 64×64 feature map to a 1×1×128 descriptor for channel‐attention generation.,
SAM,global average pooling (GAP),"A spatial pooling operation that computes the average of each channel across all H×W locations, yielding a channel-wise descriptor. Used in CBAM’s channel attention.",A 512×7×7 map becomes a vector of length 512 by global average pooling.,
SINet,global average pooling (GAP),"A spatial pooling operation that replaces each H×W feature map channel with its average value, yielding a 1×1 descriptor per channel. GAP provides a compact global representation used by attention modules to summarize “what features matter” in the entire map.","SA applies GAP to a 32×32×256 feature tensor, producing a 1×1×256 vector; this global summary then drives the MLP that computes attention weights for redistributing focus.",
SINet,global average pooling (GAP),"A spatial pooling operation that computes the average of each H×W feature map channel, producing a 1×1 descriptor per channel. GAP captures global context of feature maps and reduces spatial dimensions for channel‐wise operations.","Before computing attention weights, SA applies GAP to a 32×32×256 tensor, yielding a 1×1×256 vector that summarizes the importance of each channel across the entire feature map.",
PVT,global context modeling,"Capturing relationships between distant tokens in the entire feature map, enabling long-range dependencies to inform local predictions. It’s essential for tasks like scene parsing.","At Stage 4, PVT’s self-attention lets each of the 49 tokens attend to all others, modeling the global context of the image.",
PVT,global contextual modeling,"Capturing relationships between any two patches in the image via self-attention, enabling long-range dependencies to inform local predictions.","Even at Stage 1 (3,136 tokens), PVT’s SRA lets each token attend to every other pooled token, modeling global context.",
FPNet,global max pooling,"A layer that takes the maximum value over each channel’s spatial map. It captures the strongest activation per channel, complementing average pooling by highlighting the most salient feature responses.",Extracting the strongest edge response in each channel to build a channel attention vector.,
FPNet,global max pooling,"A pooling layer that replaces each channel’s spatial map with its maximum activation. It highlights the strongest response per channel, complementing average pooling by highlighting the most salient feature responses.",Extracting peak activations per channel to detect the sharpest texture cues around a hidden insect.,
SAM,global max pooling,"A spatial pooling operation that takes the maximum activation of each channel across H×W, producing a channel descriptor. CBAM uses this alongside average pooling.",A 256×28×28 map yields a 256-vector via global max pooling.,
PVT,global receptive field,"Every output token can attend to (receive information from) any input token, enabling long-range context capture regardless of stage depth.",Even the 7×7 feature map at Stage 4 has a global receptive field: each token’s self-attention can consider all 49 tokens.,
SAM,GoogLeNet,The 22-layer Inception-based CNN that popularized multi-branch modules and global average pooling.,"Inception-v1 uses 1×1, 3×3, 5×5 conv branches and pooling.",
FPNet,GPU memory footprint,"Peak amount of GPU memory used during model training or inference, critical for deployment on limited-resource devices.",Recording that FPNet uses 7.2 GB of GPU memory when processing a 512×512 image batch.,
SINet,GPU memory footprint,"The amount of graphics‐card RAM consumed per input, affected by batch size, model complexity, and precision modality.","Under Apex O1, SINet uses ~305 MB/GPU per 256×256 image; in pure FP32 it requires ~419 MB.",
SAM,Grad-CAM visualization,A technique that uses gradients flowing into convolutional layers to produce class-specific localization heatmaps. CBAM-trained models produce sharper Grad-CAM maps.,Grad-CAM shows that CBAM-ResNet focuses more tightly on a dog’s body than baseline ResNet.,
SINet,gradient,The vector of partial derivatives of a loss function with respect to model parameters. Gradients indicate how to adjust weights to reduce error during backpropagation.,"During training, SINet backpropagates the gradient of L_total through both the Search and Identification stages, updating convolutional filters in the RF module.",
FPNet,gradient clipping,Restricting gradient values to a maximum norm to prevent exploding gradients and stabilize training.,Clipping FPNet’s gradients to a max L2 norm of 5.0 during backpropagation.,
FPNet,ground truth mask,The manually annotated binary map indicating object versus background. It serves as the reference for training losses and evaluation metrics.,Using hand-drawn masks of camouflaged deer on forest images as ground truth for FPNet training.,
SINet,ground-truth (GT),"The expert-annotated reference mask or label against which model predictions are compared for training and evaluation. GT for COD includes object-level, instance-level, and matting-level masks.",SINet minimizes cross-entropy loss between its predicted probability map and the GT mask of a hidden frog on a lily pad.,
FPNet,ground-truth annotation,Manually drawn binary masks indicating true object vs. background pixels.,Using hand-segmented masks of hidden deer in forest images as the training standard.,
RFB,ground-truth assignment,"Ensuring each ground-truth box is assigned to at least one default box so that every object has a positive anchor, even if its IoU is below the positive threshold.",A person’s ground-truth box of IoU 0.4 may still be assigned to its highest-IoU anchor on conv8_2 to guarantee one positive match.,
RFB,ground-truth box,"The manually annotated bounding box around an object in an image, specified by coordinates (xmin, ymin, xmax, ymax). It provides supervision for localization during training.","A bicycle in VOC2007 might have a ground-truth box [xmin=34, ymin=50, xmax=200, ymax=180].",
Octave Convolution,group convolution,"A convolution where input channels are split into groups, and each group is convolved separately, reducing cross-group connectivity.",ResNeXt uses group convolution with 32 groups; each group handles 1/32 of the input channels for compute savings.,
SAM,grouped convolution,"Splitting input channels into groups and convolving each group separately, reducing computation.",A 32-group convolution on 256 channels processes 8 channels per group.,
FPNet,guided correction,"A refinement scheme where the coarse mask guides adjustments in deeper feature maps, ensuring that corrections are concentrated around preliminary object proposals.",Directing a CRF-based boundary smoothing around a pre-masked fish shape.,
SAM,H (height),The spatial height dimension of a feature map. CBAM’s spatial attention map has height H.,"For an input of 224×224, an intermediate feature map might have H=28.",
Octave Convolution,"H→H, H→L, L→H, L→L filters","The four weight tensors in OctConv: H→H learns high→high updates, H→L sends high-frequency features to low-frequency branch, etc.",W_L2H upsamples low-resolution maps and applies a convolution so coarse structures can influence high-frequency details.,
RFB,hand-crafted mechanism,A manually designed architectural component (not learned) guided by domain insights.,"The specific dilation rates {1,3,5} in RFB are hand-picked to mimic pRF ratios.",
PVT,handcrafted components,"Task-specific algorithms or modules (e.g., dense anchors, NMS) that are manually engineered rather than learned end-to-end. PVT seeks to eliminate these in favor of pure attention.","Traditional object detectors use anchor boxes and NMS; in PVT+DETR, these handcrafted components are replaced by learned query–key cross-attention and set-based loss.",
FPNet,hard negative mining,Focusing training on background pixels that are frequently misclassified as foreground.,Re-sampling background patches that FPNet often confuses with leaf textures to reduce false positives.,
RFB,hard negative mining,Selecting the negative default boxes with highest classification loss to maintain a ratio (e.g. 3:1) of negatives to positives.,"Out of 1200 negatives, the top 384 by loss are kept per batch.",
PVT,head-specific projection,"The independent weight matrices W_q, W_k, and W_v for each attention head, allowing specialized subspaces of representation in multi-head attention.","In an 8-head MHSA block of PVT, each head has its own set of projections, enabling diverse relational patterns to be learned in parallel.",
SINet,heatmap,"A colorized or intensity‐based visualization of activation values or confidence scores over the spatial domain, aiding qualitative assessment of network focus.",Visualizing SINet’s search map as a heatmap reveals bright “hotspots” where the network is confident about the presence of a concealed animal.,
PVT,hidden layer dimension,The size of the intermediate representation inside the FFN sublayer between two linear projections. Larger hidden dims increase capacity at the cost of compute.,"For a 512-dim embedding and an MLP ratio 4, PVT’s FFN hidden layer dimension is 2,048.",
FPNet,high-frequency component,"The part of an image or feature map containing rapid changes (edges, textures). These details help locate subtle object boundaries even when contrast is low.",Isolating the zebra stripe edges via a Sobel filter as a high-frequency cue.,
Octave Convolution,high-frequency feature map,"A group of feature maps that capture rapid spatial variations (edges, textures). They are stored at full input resolution in Octave Convolution.",The detailed outline of a cat’s whiskers in a feature map would be represented in a high-frequency branch.,
RFB,high-level feature,Deep CNN activations capturing semantic content but with coarse spatial detail.,conv11_2 (1×1) in SSD is a high-level map used for large-object detection.,
SINet,high-level feature,"Features encoding abstract, semantic information such as object parts or entire objects. High-level features are extracted in deeper CNN layers and offer strong contextual cues for coarse localization.","C5’s 16×16 high-level feature map captures semantics like “insect shape,” guiding the Search stage to highlight candidate regions for further refinement.",
FPNet,horizontal flip,A simple augmentation that mirrors an image left to right. It helps models learn symmetric object appearances under different viewpoints without altering semantics.,Flipping a chameleon image horizontally so its left side appears on the right.,
FPNet,hybrid loss function,"A combination of pixel-wise (e.g., BCE) and region-aware (e.g., IoU) losses, balancing boundary sharpness with overall mask accuracy.",Summing BCE loss + IoU loss with weights 0.6 and 0.4 during optimization.,
SAM,hyperparameter,"A model configuration parameter set before training (e.g., reduction ratio r, kernel size), not learned during training.",CBAM’s reduction ratio r=16 is chosen via cross-validation.,
SINet,hyperparameter,"A configuration value (e.g., learning rate, batch size, weight decay) set before training that affects model behavior.","SINet’s α (loss weight), r (reduction ratio), dilation rates, and learning rate are all hyperparameters tuned on a validation subset of COD10K.",
FPNet,hyperparameter tuning,"The process of selecting optimal training settings (learning rate, batch size, weight decay) to maximize performance.","Grid-searching FPNet’s learning rate in {1e-3, 1e-4, 1e-5} and choosing 1e-4 based on validation MAE.",
Octave Convolution,hyperparameter α (alpha),"A scalar in [0,1] controlling the fraction of channels assigned to the low-frequency group in the Octave Convolution.",Setting α = 0.25 splits 25% of the channels to the low-frequency tensor and 75% to the high-frequency tensor.,
Octave Convolution,I3D,"“Inflated 3D” ConvNet that converts 2D conv filters into 3D by adding a temporal dimension, enabling end-to-end spatio-temporal feature learning.",Inflating a pretrained 2D ImageNet ResNet-50 into I3D by repeating 3×3 weights along a third dimension of size 3 and fine-tuning on a video dataset like Kinetics.,
SINet,identification stage,"The second module in SINet, which takes the coarse map from the Search stage and the original feature maps to generate a detailed, high‐resolution segmentation of the camouflaged object.","After the Search stage marks a fuzzily located frog on a leaf, the Identification stage yields the exact pixel mask delineating its legs and body.",
SAM,identity shortcut,"A skip connection that performs no transformation, simply adding input activations unchanged.",Adding the input feature map F to the block output in ResNet.,
FPNet,image frequency domain,Representation of an image by its spectral components (sinusoids). Decomposing into high/low frequencies reveals hidden edges and smooth gradients.,Performing a DCT on a 256×256 patch to isolate tree bark edges obscured by moss patterns.,
SAM,ImageNet-1K dataset,A large-scale dataset with ~1.2 M training images across 1 000 classes for benchmarking classification.,ResNet-50 attains 76.2% top-1 accuracy on ImageNet-1K.,
SAM,inception block,"A multi-branch module with parallel convolutions of varying kernel sizes and pooling, concatenated along channels.","GoogLeNet’s Inception-3 block uses 1×1, 3×3, and 5×5 convs plus pooling.",
RFB,inception module,"A multi-branch block that runs convolutional filters of several kernel sizes (1×1, 3×3, 5×5) in parallel and concatenates outputs.","In an Inception-A block, outputs from 1×1, 3×3, and 5×5 branches (totaling 1024 channels) are concatenated.",
SAM,Inception-ResNet,A hybrid that integrates residual connections into Inception modules for faster convergence.,Inception-ResNet-v2 combines Inception branches with shortcut additions.,
FPNet,indefinable boundaries (IB),"Regions where object edges are so blurred or ambiguous that clear delineation is difficult, posing a specific COD challenge.",Approximating the fuzzy outline of a camouflaged iceberg floating in fog.,
FPNet,indefinable boundaries (IB),"Object edges so blurred or ambiguous that clear delineation is difficult, even for humans. Networks must rely on spectral or semantic cues to approximate these fuzzy borders.",Approximating the edge of a moss-covered rock that gradually fades into its background.,
SINet,indefinable boundaries (IB),"An attribute flagging objects whose edges merge seamlessly into backgrounds based on low color‐histogram distance, posing extreme segmentation difficulty.",A moth whose wing edges match the bark texture (distance <0.9) is tagged “IB.”,
SINet,inference,"The process of deploying the trained network to predict camouflaged object masks on unseen images, typically involving only a forward pass without gradient computation.","At test time, SINet runs in inference mode on a 512×512 jungle image, producing a binary mask of concealed snakes in under 50 ms.",
FPNet,inference efficiency,"Rate (FPS) and resource usage (memory, FLOPs) of the full two-stage network during deployment, factoring in both coarse and fine localization stages.",Achieving 15 FPS on 512×512 inputs with 8 GB GPU memory usage.,
PVT,inference latency,"The time it takes to process one image in a forward pass, typically measured in milliseconds, critical for real-time applications.","PVT-Tiny achieves ~22 ms latency on a V100 GPU at 224×224 resolution, suitable for near-real-time tasks.",
FPNet,inference memory footprint,The amount of GPU memory consumed during model inference. Lower footprint enables deployment on resource-constrained devices.,Recording a 4 GB peak memory usage when running FPNet on a 512×512 input.,
RFB,inference speed,The runtime (FPS or ms/image) for a single forward pass during testing.,Inference time = 15 ms for RFBNet300 at 300×300 input.,
FPNet,inference speed (FPS),"The number of images a model can process per second during deployment. High FPS is critical for real-time applications, and COD methods must balance accuracy with speed when integrated into live systems.",Measuring that FPNet processes 16 HD frames per second on an NVIDIA V100.,
SINet,initialization (Xavier/Kaiming),"Weight initialization schemes that set initial parameter values to avoid vanishing/exploding activations. Xavier suits tanh/sigmoid, Kaiming suits ReLU.",SINet’s RF convolutions are initialized with Kaiming normal to match the ReLU activations that follow.,
SAM,input feature map,The feature map F ∈ R^{C×H×W} supplied to CBAM for attention processing.,F may be the output of conv4 in ResNet-50: 1024×14×14.,
PVT,input resolution,"The pixel dimensions (height×width) of images fed into the model. Choosing input resolution affects initial token count, model accuracy, and compute cost.","PVT is typically trained on 224×224 images for ImageNet classification, creating 56×56 patches at Stage 1.",
FPNet,instance segmentation,Assigning a separate mask to each individual object instance in an image.,Extending FPNet to output distinct masks for two camouflaged lizards in the same frame.,
SINet,instance-level label,"Masks that distinguish multiple object instances of the same category by assigning unique IDs, facilitating instance-aware evaluation.",Two camouflaged frogs on a log are annotated as separate binary masks with distinct instance IDs.,
Octave Convolution,inter-frequency information exchange,Mechanisms within OctConv that send information between the high- and low-frequency branches via H→L and L→H convolutions.,Combining high-frequency details into the low-frequency branch via W_H2L ensures that coarse maps also receive edge information.,
SAM,intermediate feature map,The 3D tensor output produced by a convolutional block before the final classification or detection head. CBAM takes this map as input to compute attention.,"After the third stage in ResNet-50, the intermediate feature map has shape 1024×28×28.",
RFB,intersection over union (IoU),A metric measuring overlap between predicted and ground-truth boxes: area(intersection)/area(union).,A predicted box of area 100 px² overlapping 60 px² with ground-truth yields IoU=0.6,
SINet,intersection over union (IoU) loss,"A region-level loss defined as 1–(intersection/union) of predicted and GT masks, encouraging maximal overlap. It complements BCE by penalizing global misalignment rather than independent pixels.","When SINet missegments part of a hidden snake’s body, the IoU loss drops sharply, signaling the network to adjust segmentation holistically rather than only pixel-wise.",
FPNet,IoU loss,"A metric-derived loss based on the Intersection over Union between prediction and ground truth masks. Minimizing IoU loss directly optimizes mask overlap, especially useful for imbalanced object/background ratios.",Minimizing IoU loss on the predicted lizard mask against its ground-truth annotation.,
RFB,IoU threshold,The minimum IoU value used to decide whether an anchor matches a ground-truth box (positive) or not (negative). Anchors with IoU above the threshold are positives; those below are negatives.,"SSD uses an IoU threshold of 0.5: anchors with IoU ≥ 0.5 become positives, those with IoU < 0.5 negatives.",
FPNet,iteration,A single parameter update step using one batch.,Observing that one epoch of FPNet with 500 images and batch size 5 equals 100 iterations.,
RFB,iteration,A single forward and backward pass using one mini-batch of samples. Each iteration computes the loss on that batch and updates model parameters once.,"With batch size 32, one iteration processes 32 images and applies one weight update via SGD.",
RFB,kernel size,"The height and width of the convolution kernel, such as 1×1, 3×3, or 5×5. Larger sizes capture broader context at the cost of more parameters.","In RFB Net, the outer branch uses a 5×5 kernel to cover a 5×5 patch of feature map.",
SINet,kernel size,The spatial dimensions (height × width) of a convolutional filter kernel. Kernel size determines the local neighborhood each convolutional operation examines when computing features.,SINet’s RF module uses 3×3 kernels (with varying dilation) so each branch can aggregate context over different effective receptive fields without increasing parameter count.,
SAM,kernel size (7×7),The spatial dimensions of a convolutional kernel. Larger kernels capture wider context at higher cost. CBAM’s spatial attention employs a 7×7 kernel.,A 7×7 convolution sees a 7×7 patch of activations at once.,
Octave Convolution,kernel size (k×k),"The spatial dimensions of a convolutional filter, defining the local receptive field (e.g., 3×3 or 1×1).","A 3×3 kernel has k=3, so each output location aggregates information from a 3×3 neighborhood in the input map.",
SINet,L_identification,"The loss applied to the Identification stage, combining weighted binary cross-entropy and Intersection-over-Union (IoU) loss on the high-resolution prediction. It forces precise boundary recovery and penalizes both misclassified pixels and poor overlap.","For a 256×256 camouflaged butterfly mask, SINet’s L_identification balances BCE (for per-pixel accuracy) and IoU loss (to tighten overall shape), refining fuzzy edges from the Search stage.",
SINet,L_search,"The loss that supervises the Search stage output, typically defined as the binary cross-entropy between the low-resolution search map and a downsampled ground-truth mask. It encourages the network to highlight approximate object regions.","During training, SINet computes L_search on the 16×16 search map, penalizing pixels where the predicted attention for a hidden frog does not match the downsampled GT mask of its silhouette.",
SINet,L_total,"The overall training objective, formed by summing L_search and a scale-weighted L_identification (often with a factor α). This joint loss ensures both coarse localization and fine segmentation are learned end-to-end.","SINet sets α=1.0, yielding L_total = L_search + L_identification. In early epochs, L_search dominates so the Search stage quickly learns to spot potential regions before identification details sharpen.",
FPNet,latency,"Time taken to process one sample (or batch) during inference, impacting responsiveness in real-time systems.",Measuring FPNet’s average inference time of 70 ms per 512×512 frame on an RTX 2080 Ti.,
FPNet,latent representation,Compact feature vectors in deep layers encoding abstract semantic information.,"Using the 1,024-dim token at the transformer’s top layer as a latent summary of the scene.",
PVT,latent token,A vector representing information at a specific location (patch) after embedding; the primary data unit processed by Transformer layers.,"The 384-dim token at Stage 3 corresponding to the patch covering pixels (56–63, 112–119) in the input.",
FPNet,layer normalization,A per-token normalization that stabilizes and speeds up training by re-centering and scaling intermediate features before attention or feed-forward layers.,Normalizing token activations after multi-head attention in each transformer block.,
FPNet,layer normalization,A normalization technique applied per token in transformers. It stabilizes training by ensuring consistent activation distributions across layers.,Normalizing ViT tokens after each self-attention block to accelerate convergence.,
PVT,layer normalization,A normalization technique applied before or after sublayers (MHSA/FFN) that stabilizes training by normalizing features across embedding dimensions for each token.,"PVT normalizes each token’s 768-dim vector before feeding into MHSA, ensuring zero-mean and unit-variance across dimensions.",
PVT,layer scaling,A learnable scalar applied to the output of a residual branch before it’s added back to the main path. It stabilizes training by controlling residual magnitude and balancing contributions from different branches.,In PVT one could multiply the FFN output by a learned factor α before adding it to the token embedding to limit large updates.,
FPNet,learnable frequency separation,A mechanism by which the network automatically splits feature maps into high- and low-frequency bands via trainable filters. It adapts during training to emphasize the most discriminative spectral cues for coarse object localization.,Training a filter bank that learns to separate leaf vein textures (high-freq) from smooth bark gradients (low-freq).,
PVT,learnable linear projection,"A trainable weight matrix used to project input vectors (e.g., flattened patches, token features) into another feature space. It enables the network to learn optimal mappings for downstream processing.",The patch embedding layer in PVT is a learnable linear projection that converts each 4×4 patch into a 64-dim feature vector.,
SINet,learning rate,"The hyperparameter that scales gradient updates, controlling how quickly the model learns. A high rate can speed training but risks divergence; a low rate ensures stability but slows convergence.",SINet’s initial learning rate is set to 1e–4; after 20 epochs it’s decayed by a factor of 0.1 to refine segmentation quality.,
FPNet,learning rate scheduler,"A strategy for adjusting the learning rate during training, such as step decay or cosine annealing. Scheduling helps balance exploration (high rate) and fine-tuning (low rate), often improving final accuracy and convergence stability.",Reducing the learning rate by half every 20 epochs in a step scheduler.,
RFB,learning rate scheduling,Lowering the learning rate at specified milestones or when performance plateaus to refine convergence.,LR multiplied by 0.1 at 80k and 120k iterations during training.,
RFB,learning rate warm-up,Gradually increasing the learning rate from a small value to target over initial iterations to stabilize training.,LR linearly rises from 1e-6 to 1e-3 over first 5000 iterations.,
SAM,LeNet,A pioneering CNN architecture for digit recognition consisting of alternating conv and pooling layers followed by FC layers.,LeNet-5 classifies MNIST digits with conv(6×5×5)→pool→conv(16×5×5)→pool→FC.,
SAM,light-weight module,A design that incurs minimal computational and memory cost relative to its performance benefit. CBAM’s two small MLP layers and one 7×7 conv are efficient.,CBAM adds ≈0.002 GFLOPs per block compared to ≈4 GFLOPs of ResNet-50.,
PVT,local context,"Neighborhood information around each patch; contrasted with global context, local context is typically captured by convolutions or windowed attention.","While CNNs capture only local 3×3 neighborhoods, PVT’s self-attention always incorporates information from the entire token grid.",
PVT,local receptive field,"The neighborhood of input locations a convolutional filter sees; in contrast, Transformers always have global receptive fields via attention.","A 3×3 convolution in CNN sees only 9 pixels at a time (local), while PVT’s attention lets any patch token interact with any other directly.",
RFB,localization loss,The regression loss (typically Smooth L1) measuring difference between predicted box offsets and ground-truth.,"For an offset of 0.2 vs. ground truth 0.0, Smooth L1 loss = 0.5×0.2² = 0.02.",
FPNet,logging,"Recording training and validation metrics (loss, accuracy) over time, often visualized with tools like TensorBoard.",Plotting FPNet’s training and validation loss curves to diagnose overfitting after epoch 50.,
SINet,logits,"The raw, unnormalized scores produced by a neural network before applying an activation function such as sigmoid or softmax. They represent evidence for each class or decision per element.","SINet’s Identification stage outputs a 256×256 tensor of logits, which are then passed through a sigmoid to convert into pixel-wise probabilities pi∈[0,1].",
FPNet,loss curve,A plot of training and validation loss values over epochs; analyzing loss curves reveals convergence behavior and potential overfitting.,Observing validation loss leveling off after 40 epochs during FPNet training.,
FPNet,low-frequency component,"The part of an image or feature map that varies slowly (smooth gradients, global shape). Capturing this helps understand overall object silhouette against the background.",Extracting the sky’s smooth gradient to outline a cloud frog’s profile.,
Octave Convolution,low-frequency feature map,"A group of feature maps that capture slowly varying, coarse patterns. They are stored at a reduced spatial resolution (e.g., half height and width).","The overall shape of a cat’s body across the scene might be represented in a low-frequency, half-resolution map.",
RFB,low-level feature,Early CNN activations retaining fine spatial resolution but less semantic richness.,conv4_3 (38×38) in SSD is low-level and used for small-object detection.,
SINet,low-level feature,"Features capturing basic image cues such as edges, color gradients, and textures. These are typically extracted in early layers of a CNN and preserve fine spatial details.","C3’s 64×64 low-level feature map carries fine-grained texture cues (e.g., bark grain) that aid the Identification stage in delineating object boundaries.",
FPNet,lung infection segmentation,"Segmenting areas of infection in lung CT or X-ray scans, benefiting from COD techniques to detect regions with subtle intensity differences.",Detecting COVID-affected regions in chest CT scans using frequency-guided cues.,
FPNet,lung infection segmentation,The process of identifying infected areas in lung CT scans. Frequency cues help detect subtle texture differences between healthy and diseased tissue.,Highlighting COVID-19 lesion regions in a chest CT volume using high-frequency detail.,
SINet,main.m,A MATLAB driver script in the Evaluation Toolbox that computes all COD metrics for a set of predictions versus ground-truth masks.,"Executing main in MATLAB opens batch prompts, then outputs Fβ, MAE, Sα, Eξ, and WFβ for the provided prediction and GT directories.",
RFB,manual hyper-parameter,A parameter set by the designer (e.g. dilation rates) rather than learned during training.,"Dilation rates {1,3,5} in RFB are manual hyper-parameters.",
FPNet,mask binarization,Converting a continuous probability mask into a binary map using a threshold. Proper thresholding is crucial to trade precision for recall.,Thresholding a soft mask at 0.5 to produce the final camouflaged-object segmentation.,
PVT,mask R-CNN head,"An instance segmentation head combining a small FCN for mask prediction with classification and box regressor, operating on ROI features.","In Mask R-CNN+PVT, ROIAlign pools PVT’s 14×14 tokens into 7×7 per region, then the mask head upsamples to 28×28 binary masks.",
SINet,matting-level label,Per-pixel transparency values that define partial memberships (alpha channel) for camouflaged objects. They extend binary masks to handle translucent or fuzzy edges.,A semi-transparent jellyfish in water might be annotated with matting-level labels where its tentacles have intermediate alpha values.,
RFB,max pooling,"Selecting the maximum activation within each pooling window, highlighting strongest features.","A 2×2 max pool on [[1,3],[2,4]] outputs 4.",
SINet,max pooling,"A pooling operation that slides a window (e.g., 2×2) over the feature map and takes the maximum value, downsampling while preserving strong activations.","To create the downsampled search GT, SINet applies 16×16 max pooling to the 256×256 ground-truth mask.",
FPNet,max pooling,"A downsampling operation that takes the maximum value within each non-overlapping window (e.g., 2×2). It reduces spatial size, increases receptive field, and retains the strongest activation, though it can discard fine boundary information if overused.",Applying 2×2 max-pooling to reduce image resolution by half while keeping the brightest features.,
FPNet,mean absolute error (MAE),An evaluation metric measuring the average absolute pixel-wise difference between prediction and ground truth. Lower MAE indicates better mask accuracy.,Reporting an MAE of 0.028 on the CAMO test set.,
SINet,mean absolute error (MAE),The average per-pixel absolute difference between predicted probability pi and ground truth yi. It assesses overall calibration of the probability map.,"If for 10 pixels the absolute errors sum to 0.4, MAE=0.4/10=0.04, indicating high agreement between SINet outputs and GT.",
SAM,mean average precision (mAP),A standard metric for object detection measuring average precision across recall levels and classes.,"A detector with per-class APs [0.75,…,0.55] yields mAP=0.65.",
RFB,mean average precision (mAP),"The mean over classes of the area under the precision–recall curve, standard for detection benchmarks.",RFBNet300 achieves 80.7% mAP on VOC2007 test.,
PVT,mean average precision (mAP),"The primary detection metric that averages AP over multiple IoU thresholds (e.g., 0.5:0.95) or classes, measuring both localization and classification quality.","PVT-Small+RetinaNet achieves 40.4 mAP on COCO val2017, outperforming ResNet50’s 36.3.",
FPNet,mean E-measure,An evaluation metric averaging pixel- and image-level structural similarities. It assesses how well predicted masks preserve both local edges and global shapes relative to ground truth.,"Reporting an Eξ of 0.92 on the COD10K test set, reflecting accurate boundary and region structure.",
PVT,mean intersection over union,"The segmentation metric averaging the IoU between predicted and ground-truth masks across classes, commonly used in semantic segmentation.","PVT-Large+SemanticFPN records 42.1 mIoU on ADE20K validation, indicating improved pixel-level accuracy.",
SINet,mean-std normalization,"Preprocessing that subtracts per-channel mean and divides by per-channel standard deviation, standardizing input distributions for stable training.","Before backbone input, SINet normalizes each RGB channel by ImageNet’s mean [0.485, 0.456, 0.406] and std [0.229, 0.224, 0.225].",
FPNet,median filtering,A non-linear filter that replaces each pixel value with the median of its neighborhood. This removes “salt-and-pepper” artifacts without excessively blurring object edges.,Applying a 3×3 median filter to clean speckled noise around a camouflaged soldier silhouette.,
FPNet,memory bandwidth,"The rate at which data moves between memory and compute units during inference, affecting throughput.",Ensuring FPNet’s fused feature maps fit in L2 cache to minimize DRAM transfers on GPU,
PVT,memory complexity,"The scaling of required memory to store feature maps and intermediate attention matrices, also often O(N²). Reducing memory complexity is crucial for high-resolution attention.","Without SRA, Stage 1’s 3,136×3,136 attention map would exceed GPU memory, so PVT pools keys/values early to shrink the map.",
PVT,memory footprint,"The amount of GPU memory needed to store activations, weights, and intermediate tensors during training or inference.",Using SRA to pool keys/values can reduce peak memory usage by up to 40% versus full MHSA at high resolution.,
PVT,memory footprint,"The GPU memory required to store activations, weights, and optimizer states during training/inference.","Using SRA reduces memory footprint: Stage 2’s 28×28→14×14 pooling cuts the 784×784 attention map to 196×196, saving ~75% of memory in that layer.",
SINet,mixed precision training,"A technique that uses lower-precision (FP16) arithmetic for most operations and higher-precision (FP32) for stability, reducing memory usage and speeding up training.","By enabling apex-mode=O1, SINet halves its memory footprint per image (∼419 MB → 305 MB), allowing larger batch sizes without accuracy loss.",
SINet,mixed precision training,"A technique combining FP16 and FP32 arithmetic to accelerate deep‐learning workloads and reduce GPU memory usage, without significant accuracy loss.","SINet’s authors enable mixed precision in their PyTorch script, cutting training time by 35% on COD10K while retaining Sα within 0.01 of full-precision runs.",
FPNet,mixed precision training,Combining half-precision (FP16) and single-precision (FP32) arithmetic to accelerate training and reduce memory consumption.,Enabling NVIDIA’s Apex AMP to train FPNet at 1.8× the speed using 16-bit precision.,
PVT,MLP ratio,"The expansion factor in the FFN’s hidden layer relative to the embedding dimension, controlling model capacity and compute.","PVT sets an MLP ratio of 4, so a 384-dim embedding expands to 1,536 dims inside the FFN.",
RFB,MobileNet,A lightweight CNN using depthwise separable convolutions to reduce parameters and compute.,MobileNet v1 reduces FLOPs by ~9× compared to VGG16 for similar classification accuracy.,
FPNet,modality fusion,"The combination of heterogeneous feature sources (e.g., RGB and frequency) to exploit complementary cues for robust object detection.",Concatenating RGB features with DCT-based frequency features before a 1×1 conv.,
FPNet,model checkpoint,"A saved snapshot of model parameters at a specific training iteration, enabling resumption or intermediate evaluation without restarting from scratch.",Saving FPNet weights every 5 epochs to allow rollback to the best-performing state.,
PVT,model convergence,"The process by which training loss decreases and stabilizes as parameters approach an optimum, influenced by architecture and optimization choices.","PVT’s pre-norm design accelerates convergence, achieving minimal training loss in 90% of the epochs required by a post-norm ViT at the same resolution.",
FPNet,model parameter count,"The total number of trainable weights in a network. A lower parameter count often correlates with efficiency, but excessive reduction can harm capacity—COD models aim to minimize size without sacrificing detection quality.",Reporting 28 million parameters for FPNet’s full two-stage architecture.,
FPNet,model parameter count,The total number of trainable weights in a network. Smaller counts improve efficiency but risk capacity loss—COD architectures seek compactness without sacrificing detection quality.,Reporting 35 million parameters for FPNet’s two-stage architecture.,
PVT,model scalability,"The capacity to adjust model size—depth, width, token resolution—to meet different resource and accuracy needs. Scalability aids in adapting to diverse deployment constraints.","PVT offers Tiny, Small, Medium, and Large variants, each with different block counts and embedding dims to trade off FLOPs and accuracy.",
PVT,model variant,"A specific configuration of the PVT backbone (Tiny, Small, Medium, Large) that differs in depth, width, and embedding dims. Variants let practitioners choose trade-offs between speed and performance.","PVT-Medium uses [3, 4, 18, 3] blocks with embedding dims [64, 128, 320, 512], totaling ~44.2 M parameters for mid-range accuracy and efficiency.",
FPNet,model zoo,A collection of publicly released pretrained model checkpoints for easy reuse and benchmarking.,Providing FPNet’s PyTorch weights on GitHub so others can fine-tune or evaluate.,
FPNet,momentum,An optimization hyperparameter that accumulates a running average of gradients to smooth parameter updates. Momentum helps escape shallow local minima and speeds convergence by dampening oscillations along steep directions.,Using momentum of 0.9 in SGD to train FPNet for stable convergence.,
RFB,momentum,A parameter in SGD that accumulates past gradients to smooth and accelerate updates.,Momentum = 0.9 means 90% of last update carried into current.,
RFB,MS COCO,A large-scale detection dataset with 80 classes and challenging AP metrics evaluated at IoU thresholds 0.5:0.95.,RFBNet512 scores 34.4 AP on COCO test-dev at 30 FPS.,
SAM,MS COCO dataset,"A dataset for object detection, segmentation, and captioning with 80 object categories.",CBAM-augmented Faster R-CNN achieves 42.5 mAP on COCO val2017.,
SAM,multi-branch architecture,"A network design with parallel convolutional paths (branches) that are later merged, capturing multi-scale features.","Inception modules use 1×1, 3×3, and 5×5 conv branches in parallel.",
SINet,multi-branch convolutional block,"A module containing multiple parallel convolutional branches, each configured differently (e.g., distinct dilation rates), whose outputs are fused to capture diverse contextual scales in one layer.","SINet’s RF module is a multi-branch block with three parallel 3×3 convolutions (dilations = 1, 4, 7), enabling simultaneous local-detail and global-context aggregation.",
RFB,multi-branch pooling,"The technique of processing the same feature map through several parallel “branches,” each performing a different convolution or pooling.","RFB uses three branches: 1×1, 3×3 (rate=3), and 5×5 (rate=5) convolutions.",
Octave Convolution,multi-frequency feature representation,A factorization of standard feature maps into two tensors—one for high frequencies at full resolution and one for low frequencies at reduced resolution—to reduce redundancy.,Splitting a 128-channel tensor into 96 high-frequency channels at 224×224 and 32 low-frequency channels at 112×112 forms a multi-frequency representation.,
PVT,multi-head cross-attention,"The cross-attention mechanism split into multiple parallel “heads,” each with separate projection matrices, increasing modeling capacity for query–key/value interactions.",A 6-head cross-attention in the DETR decoder lets each head capture different relations between object queries and PVT features at Stage 3 (14×14 tokens).,
FPNet,multi-head self-attention (MHSA),"An attention mechanism running multiple parallel “heads,” each learning different projection subspaces of queries, keys, and values to capture diverse spatial dependencies across tokens.","Using eight heads in ViT to attend simultaneously to texture, color, and shape cues of a hidden lizard.",
FPNet,multi-head self-attention (MHSA),"An attention mechanism with parallel heads, each attending to different feature subspaces. It captures diverse contextual relationships among tokens.",Using 8 heads in ViT to link texture patterns and shape cues of a hidden lizard across distant patches.,
PVT,multi-head self-attention (MHSA),"An attention mechanism that splits queries, keys, and values into multiple subspaces (“heads”), performs scaled dot-product attention in each, and concatenates results.","A 4-head MHSA in PVT processes a 196-token sequence by computing four parallel attention maps, each focusing on a different relational pattern.",
SAM,multi-layer perceptron (MLP),"A fully-connected neural network with one or more hidden layers. In CBAM’s channel attention, an MLP learns to model inter-channel dependencies.",A two-layer MLP with hidden size C/r=128 processes a 512-vector.,
FPNet,multi-level features,Feature maps extracted from different depths of the backbone. Low-level maps preserve texture and spatial detail; high-level maps encode semantic abstraction. FPNet processes and fuses these levels in both stages.,"Aggregating ResNet C2, C3, C4, C5 maps to capture edges and semantics of a hidden deer.",
FPNet,multi-level transformer features,Feature maps extracted at different depths of a vision transformer backbone. Lower levels capture textures; higher levels encode global semantics.,"Collecting ViT C2, C4, C6 maps to jointly represent a hidden bird’s fine feathers and overall shape.",
RFB,multi-scale feature maps,A set of feature maps at different spatial resolutions used to detect objects of varying sizes.,"SSD300 uses feature maps of sizes 38×38, 19×19, 10×10, 5×5, 3×3, 1×1 for small-to-large objects.",
PVT,multi-scale feature representation,"The ability to produce and use features at multiple spatial resolutions for downstream heads (e.g., detection, segmentation), capturing both coarse and fine details.","PVT outputs four feature maps at 56×56, 28×28, 14×14, and 7×7, which are fed into FPN heads for object detection.",
PVT,multi-scale fusion,The integration of feature maps from multiple pyramid stages to leverage both high-resolution and low-resolution information for dense prediction heads.,"PVT feeds its four output maps (56×56, 28×28, 14×14, 7×7) into an FPN, which fuses them for object detection with RetinaNet.",
FPNet,multi-scale prediction,"A strategy where the network outputs segmentation masks at different resolutions. Combining multi-scale outputs helps capture both global context and fine details, improving robustness across object sizes.","Producing 64×64, 128×128, and 256×256 mask predictions and merging them for final camo detection.",
RFB,multi-scale representation,Encoding features at several spatial scales to detect objects of varying sizes.,"RFB’s three branches produce RFs covering local (1), mid (3), and global (5) contexts.",
PVT,multi-stage pyramid,"A hierarchical backbone architecture with several stages, each producing token grids at progressively coarser resolutions and higher channel dims.","PVT’s pyramid has four stages, downsampling resolutions by 2× per stage and increasing embeddings from 64 to 512 dims.",
FPNet,multi-task learning,"A training paradigm where a single model is optimized to perform multiple related tasks simultaneously. By sharing representations across tasks (e.g., segmentation and edge detection), the model can learn more robust features and avoid overfitting to one objective.",Training a network to both segment camouflaged lizards and predict their surface normals in one pass.,
RFB,multi-task Loss,"A combined objective summing localization and classification losses, often with weighting factor α.",Total loss = L_cls + α·L_loc with α = 1.0 in SSD training.,
SINet,multi‐layer perceptron (MLP),A small fully connected neural network comprising one or more hidden layers with non‐linear activations. MLPs are often used to learn channel‐wise weights in attention mechanisms by processing pooled descriptors.,SINet’s Search Attention module uses an MLP with two FC layers (with a bottleneck defined by reduction ratio r) to transform the GAP‐derived channel descriptor into scaling factors for reweighting C5 features.,
SINet,multi‐scale exploration,Processing image features at various spatial extents to detect objects of different sizes and under varying contextual conditions.,"SINet’s RF module uses dilation rates {1,4,7} to extract cues from local textures up to broad scene context, enabling robust detection of small and large camouflaged objects alike.",
FPNet,multiple objects (MO),"Test cases containing more than one camouflaged object, each requiring individual localization and segmentation.",Segmenting both a hidden deer and a camouflaged boar in the same woodland image.,
FPNet,multiple objects (MO),Scenes containing more than one camouflaged target. The model must individually localize and segment each object despite overlapping or adjacent frequency and color cues.,"Segmenting two chameleons perched on the same branch, each blending differently into the foliage.",
SINet,multiple objects (MO),"An attribute indicating images where two or more camouflaged objects appear, challenging detection due to mutual interference.",A scene with three camouflaged turtles on a log is labeled “MO” to assess SINet’s ability to segment multiple instances simultaneously.,
Octave Convolution,nearest-neighbor interpolation,A simple up-sampling method that replicates each pixel into a larger grid when increasing spatial resolution.,Upsampling a 112×112 low-frequency map to 224×224 by copying each pixel into a 2×2 block.,
RFB,negative sample,"An anchor whose IoU with every ground-truth box falls below the negative IoU threshold (e.g., < 0.5), treated as background during classification training.",An anchor with IoU = 0.2 against all objects is sampled as a negative example for the “background” class.,
SINet,negative sample,"In COD training, images containing clearly visible (salient) objects or pure background scenes used as “non-camouflage” examples. They help the model learn to suppress false positives in high-contrast regions.",Feeding SINet a batch of negative samples from VOC datasets teaches it not to segment conspicuous cars and pedestrians.,
FPNet,neighbor interaction,A mechanism that exchanges frequency cues between adjacent resolution maps. It enforces spatial coherence in the coarse mask across scales.,Passing information between 1/8 and 1/16 scale maps so the rough mask aligns consistently across resolutions.,
FPNet,neighbor interaction mechanism (NIM),"A design in the frequency-guided stage that enables adjacent feature levels to communicate. By passing information between “neighboring” resolution maps, the network maintains spatial coherence of the coarse mask across scales.",Exchanging cues between 1/8- and 1/16-scale feature maps to keep the segmentation consistent.,
FPNet,neighbor interaction mechanism (NIM),"The scheme in the coarse stage that passes frequency-separated cues between adjacent feature levels (e.g., 1/8 ↔ 1/16 scale) to maintain spatial coherence of the coarse mask across resolutions.",Exchanging cues between 32×32 and 64×64 feature maps so the rough frog mask aligns across scales.,
PVT,network depth,"The total number of Transformer encoder layers or blocks in a model, often broken down per stage, which controls model capacity. Greater depth typically increases accuracy at the cost of more compute.","PVT-Small’s stage depths are [3, 4, 6, 3], whereas PVT-Base uses [3, 4, 12, 3] blocks for higher capacity.",
FPNet,non-maximum suppression (NMS),Eliminating overlapping proposals by keeping the highest-scoring box and discarding the rest above an IoU threshold.,Suppressing duplicate bounding boxes around a camouflaged crab so only the best one remains.,
RFB,non-maximum suppression (NMS),A post-processing algorithm that removes overlapping detections by keeping highest-scoring boxes.,"Boxes with IoU>0.45 suppressed, retaining only one per object.",
PVT,non-maximum suppression (NMS),"A post-processing step that filters overlapping detections by score, keeping only the highest-scoring box in each region.","In a convolutional pipeline, NMS is applied after the classifier head; in the PVT+DETR convolution-free variant, NMS is entirely removed.",
PVT,normalization placement,"The choice of applying normalization (e.g., LayerNorm) before or after attention/FFN sublayers (pre-norm vs. post-norm). Pre-norm placement often yields more stable gradients in deep Transformers.",PVT uses pre-norm: it normalizes tokens before passing them into MHSA and FFN within each encoder block.,
SINet,NVIDIA Apex,"A PyTorch extension for mixed-precision and distributed training, offering automated loss-scaling and optimized FP16 kernels.","The authors use Apex by importing from apex import amp and wrapping the optimizer: model, optimizer = amp.initialize(model, optimizer, opt_level='O1').",
FPNet,object proposal,"Candidate regions (typically bounding boxes) likely to contain objects, used to narrow the search space.",Generating 200 rectangular proposals per image before mask refinement in a two-stage COD pipeline.,
SINet,object-level label,"Coarse binary masks that mark all pixels belonging to any camouflaged object in an image, without distinguishing instances.",A flock of camouflaged birds is represented by a single object-level mask covering all bird pixels.,
FPNet,objectness score,"Confidence that a proposal contains any object, used to rank candidate regions.",Assigning a 0.85 objectness score to a proposal covering most of a hidden bird silhouette.,
FPNet,occluded objects,"Scenarios where camouflaged objects are partially hidden by other scene elements, compounding detection difficulty.",Detecting a fish partly obscured by seaweed in underwater footage.,
FPNet,occluded objects,Camouflaged targets partially hidden by other scene elements. Detection requires robust fusion across layers to recover missing contours.,Identifying a frog partly covered by leaves in a rainforest floor image.,
SINet,occlusion (OC),"An attribute for objects partially hidden by other elements (leaves, rocks), testing the model’s inference of missing contours.",A frog partially covered by a leaf in the foreground is annotated “OC.”,
FPNet,octave channels,"In octave convolution, two streams—“high octave” for high-frequency details and “low octave” for coarse, smooth content—each processed at different spatial resolutions.",Processing 1/2 resolution for the low-freq branch and full resolution for the high-freq branch.,
FPNet,octave convolution,"A convolutional operator that factorizes features into “octave” channels (low- vs. high-frequency). By processing each octave separately, it reduces spatial redundancy while preserving both detailed texture and coarse shape information.",Replacing a 3×3 conv with an octave convolution layer that halves spatial resolution for low-freq.,
Octave Convolution,octave convolution (OctConv),"A generalized convolution that processes high- and low-frequency feature groups separately, using four kernel sets (H→H, H→L, L→H, L→L) with cross-frequency information exchange.",Replacing every 3×3 vanilla convolution in ResNet-50 with OctConv (α = 0.25) to simultaneously handle high- and low-frequency maps and reduce FLOPs.,
SAM,one-shot detector,A single-stage object detector that predicts bounding boxes and classes in one forward pass.,SSD (Single Shot MultiBox Detector) is a popular one-shot detector.,
RFB,one-stage detector,"An object detector that skips proposal generation, directly regressing boxes and classes in one network pass.",YOLOv3 is another popular one-stage detector achieving 57.9 mAP on COCO at 20 FPS.,
FPNet,opening,A morphological sequence—erosion followed by dilation—that removes small artifacts and noise while preserving overall shape.,Performing opening on a noisy camo-fish mask to eliminate tiny disconnected regions.,
FPNet,optical flow,"A dense vector field that estimates pixel-wise motion between consecutive frames. It captures temporal changes, enabling methods to identify moving objects or texture inconsistencies that standard appearance cues might miss.",Tracking a camouflaged fish swimming in a river by computing frame-to-frame motion vectors.,
SINet,optimizer,"The algorithm that updates network weights during training by minimizing the loss, based on computed gradients. Common choices include SGD and Adam.","SINet uses the Adam optimizer with default β₁=0.9, β₂=0.999 to train all modules end-to-end on COD10K.",
SINet,original_WFb,"A MATLAB function computing the Weighted Fβ measure using original code from [Wu et al.], applying boundary-aware weighting.","Calling original_WFb(pred,gt) on two binary masks returns WF₁=0.848 for a rock‐camouflaged crab.",
FPNet,out-of-distribution (OOD),"Data whose distribution differs from the training set, used to test generalization.",Evaluating a COD model trained on COD10K directly on a new wildlife art dataset without fine-tuning.,
SINet,out-of-view (OV),"An attribute marking objects partially or fully outside the image frame, creating incomplete shapes and ambiguous boundaries.",A chameleon whose tail extends beyond the image boundary is marked with “OV.”,
PVT,output projection,The second linear transformation in an FFN that maps the hidden layer back to the original embedding dimension. It restores the token size after expansion.,"In Stage 3 of PVT, the FFN projects its 1,536-dim activations back down to 384 dims for each token.",
PVT,output resolution,The spatial size (in pixels or patch tokens) of feature maps produced by each stage; critical for pixel-level tasks.,"Stage 1’s output resolution is 56×56 tokens, giving correspondingly fine spatial detail for downstream heads.",
RFB,padding,Zero-padding around feature map edges to control output size or preserve spatial dimensions.,“Same” padding on a 3×3 conv adds 1 pixel border so output stays 64×64.,
SINet,padding,The addition of extra “border” pixels (often zeros) around feature maps to control the spatial dimensions of the output after convolution. Padding prevents feature maps from shrinking when using kernels larger than 1×1 or non-unit strides.,"To keep C5 at 16×16 resolution after applying a 3×3 dilated convolution with dilation = 4, SINet adds appropriate padding around the input tensor.",
SINet,panoptic segmentation,A unified framework that simultaneously performs instance segmentation for “things” (countable objects) and semantic segmentation for “stuff” (amorphous regions). Panoptic outputs assign every pixel a class label and instance ID.,Contrasting panoptic maps with SINet’s COD masks highlights how instance ID is less relevant when objects blend into backgrounds.,
PVT,parameter count,"The total number of learnable parameters in the model, reflecting its size and potential capacity. Monitoring parameter count helps compare model scale and resource requirements.","PVT-Small has ~24.5 M parameters, while PVT-Large increases to ~61.4 M, allowing for higher accuracy at the cost of more compute.",
PVT,parameter count,"The total number of learnable weights in a model, indicating its size and memory footprint.","PVT-Medium has ~44.2 million parameters, compared to ~25 M in PVT-Small and ~23 M in ResNet-50.",
FPNet,parameter efficiency,The relationship between model performance and its parameter count; high efficiency means strong accuracy with fewer weights.,Comparing FPNet’s 35 million parameters to a baseline’s 50 million while achieving higher S-measure.,
RFB,parameter overhead,The extra number of weights introduced by a module relative to baseline.,RFB adds ~0.5 million parameters to VGG16+SSD.,
SAM,parameter overhead,The extra number of learnable parameters introduced by a module. CBAM’s overhead is negligible (≈1‰ of base ResNet-50 parameters).,Adding CBAM to ResNet-50 increases parameters by ≈0.03 M out of 25.6 M.,
RFB,parameter sharing,The reuse of the same convolution kernel across all spatial positions to enforce translation equivariance.,The 3×3 weights in a conv layer are identical for every pixel location.,
NCD,partial decoder component (PDC),"A lightweight decoding block that merges only a subset of deep feature maps (i.e. partial features) to reconstruct a high-resolution segmentation map. By focusing on key feature levels, PDC avoids redundant computation.",PDC fuses the high-level “search” features (strong object cues) with mid-level detail maps to recover sharp boundaries of the concealed object.,
SINet,partial decoder component (PDC),"A lightweight decoder that fuses only the most informative subset of RF-enhanced feature maps (e.g. from C4 and C5), avoiding the cost of merging all scales. It upsamples and combines these partial features to reconstruct a high-resolution prediction mask.","PDC takes the coarse attention map from C5 and mid-level details from C4, upsamples both to the original image size, then sums them to yield the final segmentation of a hidden lizard.",
RFB,Pascal VOC,"A widely used detection dataset with 20 object classes and train/val/test splits (e.g. VOC2007, VOC2012).",Training on VOC2007+2012 (21k images) yields baseline SSD300 mAP=77.2%.,
SAM,Pascal VOC 2007 dataset,A detection and segmentation dataset with 20 object classes used for evaluating one-shot detectors.,A YOLO-style detector attains 72.4 mAP on VOC2007 test.,
FPNet,patch embedding,"Initial mapping of image patches (e.g., 4×4 pixel blocks) into a fixed-length feature vector before transformer processing, preserving locality.",Applying a linear projection to map 4×4 pixel regions into 128-dim embeddings.,
FPNet,patch embedding,Mapping of non-overlapping image patches into fixed-length vectors before transformer processing. It preserves spatial locality in tokenized form.,Projecting each 16×16 pixel patch into a 768-dim token in ViT’s first layer.,
PVT,patch embedding,"A learnable linear projection that maps flattened image patches into a fixed-length embedding vector, analogous to word embeddings in NLP.",A 4×4×3 patch (48 values) is projected via a 48×64 weight matrix to create a 64-dim token for Stage 1.,
FPNet,patch merging,"Operation in pyramid transformers that downsamples spatial tokens by concatenating and linearly projecting groups of neighboring embeddings, producing coarser feature maps for deeper layers.",Merging four adjacent tokens into one at each depth reduction step.,
PVT,patch merging,"The process of combining groups of adjacent token embeddings into larger units to downsample spatial resolution. Typically implemented by concatenating or averaging neighboring tokens, followed by a projection.","Between Stage 1 and Stage 2, PVT merges each 2×2 block of tokens (4 tokens total) by concatenation and projection, reducing the grid from 56×56 to 28×28.",
PVT,patch partitioning,The operation of dividing the input image into a grid of non-overlapping rectangular regions (patches) before any further processing. It reorganizes pixel values into fixed-size blocks that become individual tokens for the Transformer.,"In PVT, a 224×224 image is partitioned into 4×4 patches, yielding a 56×56 grid of 3,136 tokens at Stage 1.",
PVT,patch resolution,The size (height×width) of each image patch fed into the patch embedding layer; smaller patches yield higher-res tokens but more computation.,"PVT uses a patch resolution of 4×4 at Stage 1, while ViT uses 16×16, explaining PVT’s ability to maintain finer spatial detail.",
RFB,pipeline,"The sequence of processing stages from input image, through backbone, any neck modules (e.g. RFB), detection heads, and post-processing (e.g. NMS).",SSD pipeline: resize→forward→decode→NMS→output boxes.,
SINet,pixel confidence pi,"A real-valued score in [0,1] assigned to each pixel indicating the likelihood of belonging to a camouflaged object. Predictions are thresholded to produce a binary mask.","A pixel under a leaf insect wing might have pi = 0.85, causing it to be included in the final camouflage mask after thresholding at 0.5.",
PVT,pixel-level representation,"A continuous mapping from each pixel (or small region) to a feature embedding, enabling dense predictions such as segmentation masks.","In semantic segmentation, each 4×4 patch token’s final embedding is projected back into a pixel-level mask, preserving fine edges.",
FPNet,pixel-wise supervision,"The application of loss functions directly at each pixel of the predicted mask, enforcing correspondence with ground truth on a per-pixel basis. Common in segmentation tasks to obtain detailed mask learning.",Applying binary cross-entropy loss at each pixel of the camouflaged-object mask.,
FPNet,pixel-wise supervision,"Application of loss functions at each pixel location, enforcing detailed correspondence between predicted mask values and ground-truth labels.",Applying BCE loss at every pixel when training camo-object segmentation.,
SINet,pixel‐wise classification,"Assigning a probability or discrete label to each pixel in an image, determining its membership in foreground or background. This forms the basis of semantic segmentation and object‐level detection tasks.","SINet’s output pi ∈ [0,1] for each pixel is produced via pixel‐wise classification, enabling generation of binary masks by thresholding the probability map.",
FPNet,plug-and-play module,A self-contained network component designed to be inserted into various architectures without reengineering the whole pipeline. FPNet’s frequency-perception module is an example—it can be “plugged” into any backbone to add frequency awareness.,Dropping the frequency-perception block into U-Net for biomedical segmentation to improve edge detection.,
SAM,plug-and-play module,A module designed for seamless insertion into various network architectures without redesign. CBAM can be easily added to any CNN block.,Slotting CBAM after every convolutional block in VGG16 requires only a few lines of code.,
FPNet,plugin evaluation metric,"Measurement of a module’s cross-framework impact, e.g., adding the frequency-perception block to several baselines and reporting average performance uplift.",Reporting a +3% average F-measure boost when embedding the module into five different backbones.,
FPNet,plugin-in module,A self-contained component (like the frequency-perception block) designed to be “plugged” into different backbones or frameworks with minimal engineering effort.,Adding the frequency module to U²-Net and measuring the uplift in COD performance.,
FPNet,polyp segmentation,"A medical imaging task that segments colorectal polyps in endoscopy images, analogous to COD due to low-contrast boundaries and shape variability.",Segmenting small polyps in colonoscopy frames with the same boundary-refinement module.,
FPNet,polyp segmentation,"A medical imaging task that isolates and extracts polyp regions from endoscopy frames. This shares challenges with COD because polyps often have low contrast against surrounding tissue, demanding precise boundary refinement.",Segmenting a small polyp in a colonoscopy image where its color closely matches the surrounding mucosa.,
Octave Convolution,pooling,"An operation (e.g., max or average) that aggregates values over a local window to reduce spatial resolution.",Applying 2×2 average pooling with stride 2 to a feature map reduces its height and width by a factor of 2.,
RFB,pooling operation,A downsampling operation (max or average) over local windows to reduce spatial resolution and build invariance.,VGG16 uses 2×2 max pooling with stride=2 after every two conv layers.,
PVT,pooling operation,"An aggregation mechanism (e.g., average or max pooling) that summarizes local neighborhoods to reduce spatial dimensions.",PVT uses 2×2 average pooling on key/value features in its SRA to compress high-resolution maps before computing attention.,
SAM,pooling operation,"A subsampling or aggregation function (e.g., max, average) that reduces spatial dimensions to produce summaries. CBAM uses two pooling types for channel attention.",2×2 max pooling halves a 32×32 feature map to 16×16.,
RFB,population receptive field (pRF),"The combined RF of many neurons measured e.g. by fMRI, characterizing how a patch of cortex responds to visual stimuli at different eccentricities. It grows with distance from the foveal center.",A pRF in human V1 might cover a 1° radius circle at 2° eccentricity from fixation.,
FPNet,positional encoding,Addition of sinusoidal or learned vectors to patch embeddings so that transformer layers can infer relative spatial arrangement among tokens despite permutation-invariant attention.,Adding 2D sine-cosine vectors to each token so the model knows its patch’s row and column.,
FPNet,positional encoding,Adding sinusoidal or learned vectors to tokens so transformers infer relative spatial arrangement despite attention’s permutation invariance.,Injecting 2D sine-cosine embeddings into patch tokens to encode row/column positions.,
PVT,positional encoding,"A set of vectors added to token embeddings to retain information about the spatial arrangement of patches, often sinusoidal or learned.","PVT learns a 56×56×64 tensor of positional embeddings for Stage 1 and adds it to the 3,136 patch embeddings before attention.",
RFB,positive sample,"A default (anchor) box whose Intersection over Union with a ground-truth box exceeds the positive IoU threshold (e.g., ≥ 0.5), used as a positive example for both classification and regression.",A conv4_3 anchor with IoU = 0.72 against a car’s ground-truth box is marked as a positive sample.,
PVT,post-norm,An alternative where LayerNorm is applied after the residual addition of MHSA or FFN; can be less stable in very deep Transformers.,ViT originally used post-norm; PVT switched to pre-norm for improved convergence on high-resolution tokens.,
FPNet,post-processing pipeline,"Sequence of operations (e.g., morphological closing, CRF) applied after raw predictions to remove artifacts and enforce spatial consistency.",Applying a 3×3 closing operation followed by dense CRF to smooth small holes in the final mask.,
PVT,pre-norm,"A design where LayerNorm is applied before the MHSA and FFN sublayers in each Transformer block, yielding more stable gradients for deep models.",PVT uses pre-norm: it normalizes tokens before feeding them into both the self-attention and FFN in each TF-E module.,
SINet,precision,The ratio of true‐positive pixels to all pixels predicted as foreground. It measures mask accuracy by penalizing false alarms (background pixels mislabeled as object).,"If SINet predicts 1200 pixels of camouflaged leaves and 100 are wrong, precision = (1200−100)/1200 ≈ 0.917.",
RFB,precision score,The fraction of true-positive detections among all predicted positives.,RFBNet300 improved precision at IoU=0.5 from 77% to 79%.,
FPNet,pretrained weights,"Model parameters learned on large source datasets (e.g., ImageNet) before fine-tuning on a target task. Pretraining provides robust feature initialization, accelerating convergence and improving generalization on smaller COD datasets.",Loading ImageNet-pretrained ResNet-50 weights when initializing FPNet’s backbone.,
FPNet,pretrained weights,"Model parameters learned on large datasets (e.g., ImageNet). They accelerate convergence and improve generalization on smaller COD benchmarks.",Loading ImageNet-pretrained PVTv2 weights when training FPNet on COD10K.,
FPNet,prior mask,A coarse binary or probabilistic map from Stage 1 used as a spatial prior in Stage 2 to focus refinement on likely object regions and suppress background.,Feeding the 1/16-scale prior mask of a hidden cat into the refinement module.,
FPNet,prior mask,The coarse binary or probabilistic map from Stage 1 used as a spatial prior in Stage 2 to focus refinement on likely object regions and suppress background.,Feeding a 32×32 rough mask of a hidden turtle into the boundary refinement block.,
FPNet,prior-guided correction (PGC),A mechanism that uses the coarse prediction from the first stage as a spatial prior to adjust high-level features in the refinement stage. This guidance helps the network focus corrections on likely object regions and suppress background noise.,Using a blurred animal heatmap to direct a CRF step that cleans noisy edges.,
FPNet,prior-guided correction (PGC),"A refinement scheme where the coarse mask guides adjustments in deeper feature maps, ensuring that corrections are concentrated around preliminary object proposals.",Using the coarse amphibian mask to gate updates in the decoder’s second block.,
FPNet,prior-guided correction (PGC),"The step in the CFM where the Stage-1 coarse mask is used as a spatial prior to gate and adjust deeper feature maps, focusing refinement on likely object regions and suppressing background noise.",Multiplying deep semantic maps by the upsampled coarse heatmap to zero out background activations before fusion.,
PVT,progressive shrinking pyramid,"A design where the spatial dimension of token sequences is reduced stage-by-stage, lowering computation on later layers while retaining multi-scale outputs.","After Stage 1’s 56×56 grid of tokens, Stage 2 downsamples to 28×28, Stage 3 to 14×14, and Stage 4 to 7×7, halving spatial size each time.",
PVT,progressive shrinking pyramid,"A staged design where token grids are halved in resolution at each stage via patch merging, balancing detail (early stages) with efficiency (later stages).","PVT shrinks from 56×56→28×28→14×14→7×7 grids over four stages, cutting token count by 75% per stage.",
PVT,projection matrix,A learnable linear layer’s weight matrix used to map input vectors into a new feature space. Projection matrices underpin both patch embedding and FFN operations.,"The first FFN linear layer in PVT is a 384×1,536 projection matrix that expands token features.",
FPNet,PVTv1,"The first version of Pyramid Vision Transformer, which introduces progressive patch merging and spatially reduced self-attention. PVTv1 yields richer multi-scale features while keeping compute manageable.",Pretraining PVTv1 on ImageNet and fine-tuning for codorn rodent detection in desert scenes.,
FPNet,PVTv1-Large,The original Pyramid Vision Transformer variant with a large capacity. It merges convolutional patch embedding with hierarchical attention for efficient multi-scale feature extraction.,Using PVTv1-Large pretrained on ImageNet to initialize FPNet’s encoder.,
FPNet,PVTv2,"An improved PVT variant featuring enhanced attention mechanisms and normalization strategies. PVTv2 offers better accuracy-efficiency trade-offs for dense tasks, making it a strong backbone choice for segmentation and detection.",Replacing PVTv1 with PVTv2 in FPNet to gain a 2% mF-measure boost on CAMO.,
FPNet,PVTv2,An improved PVT version featuring enhanced attention modules and normalization strategies. It offers better accuracy-efficiency trade-offs for dense prediction tasks like COD.,Replacing PVTv1 with PVTv2 in FPNet to boost E-measure on CAMO by 2%.,
RFB,pyramid label assignment,Matching ground-truth boxes to default boxes across multiple scales based on object size relative to feature map scale.,Faces <32 px assigned to conv4_3 small-scale anchors.,
FPNet,pyramid pooling module (PPM),"A module that applies spatial pooling at multiple grid sizes (e.g., 1×1, 2×2, 3×3, 6×6) and concatenates the pooled outputs. PPM encodes global and sub-region context, valuable when object/background distinctions are subtle.",Applying PPM to pool a 256×256 image into 4 scales for global-to-local context in scene parsing.,
FPNet,pyramid vision transformer (PVT),"A hybrid backbone combining convolutional patch embedding with hierarchical transformer layers. PVT produces multi-level feature maps with global attention, balancing locality and scalability for dense prediction tasks like COD.","Employing PVTv2 to extract 1/4, 1/8, and 1/16 scale features for camouflaged object localization.",
PVT,pyramid vision transformer (PVT),A convolution-free backbone that injects Transformer encoders into a multi-stage pyramid architecture. It outputs multi-scale feature maps suitable for dense prediction tasks.,"In PVT-Small, four stages of Transformer encoders produce feature maps at strides 4, 8, 16, and 32 for object detection with RetinaNet.",
SAM,PyramidNet,"A network that gradually increases width (filters) across layers, forming a pyramid-shaped channel progression.",A PyramidNet-110 grows channels from 16 to 270 over 110 layers.,
FPNet,qualitative comparison,"Assessing model outputs by visual inspection, highlighting strengths and failure modes in real-world scenarios.",Displaying side-by-side masks from FPNet and a baseline on a hidden lizard image.,
FPNet,qualitative comparison,Visual assessment of segmentation outputs to illustrate strengths and failure modes in real scenes beyond numeric scores.,Displaying side-by-side masks of FPNet and a baseline on a hidden crab image to show improvement.,
FPNet,quantitative comparison,"Rigorously measuring performance via metrics (e.g., MAE, weighted F-measure), enabling objective ranking of models.",Reporting FPNet’s 0.03 MAE vs. a baseline’s 0.045 on the COD10K test set.,
FPNet,quantitative comparison,"Objective evaluation of methods using standardized metrics (MAE, F-measure, E-measure, S-measure) on benchmark datasets.",Reporting FPNet’s 0.028 MAE vs. SINet-V2’s 0.035 on COD10K.,
FPNet,quantitative results,"Numerical performance metrics (MAE, F-measure, E-measure, S-measure) reported in tables.",Listing FPNet’s MAE of 0.025 and weighted F-measure of 0.88 on CAMO in Table 2.,
PVT,query embedding,"A learnable vector (or set of vectors) used as the “query” in cross-attention, typically representing object slots in detection Transformers like DETR.","DETR uses 100 query embeddings; at each decoder layer, these query vectors attend over PVT tokens to propose up to 100 objects.",
PVT,"query, key, value","The three projections of token embeddings used in self-attention: queries attend to keys, and weights are applied to values to produce outputs.","A single token’s embedding is multiplied by three different weight matrices to yield a 96-dim query, 96-dim key, and 96-dim value for each head.",
SAM,r (reduction ratio),The scalar hyperparameter controlling the bottleneck width in CBAM’s shared MLP: hidden size=C/r. Typical choice is 16.,"With C=256 and r=16, the hidden layer size is 16.",
FPNet,random crop,"An augmentation that extracts random sub-regions of an image, forcing the model to learn from partial views and improving robustness to scale and translation variations.",Cropping random 128×128 patches from a 512×512 forest image to train scale-robust COD features.,
SINet,random crop,"Augmentation that extracts random sub-windows of the image and mask, forcing the network to learn from partial views.","To train SINet on varied scales, each image is randomly cropped to a 224×224 patch before feeding into ResNet-50.",
SINet,random flip,A simple augmentation that flips images (and corresponding masks) horizontally or vertically with some probability.,"During COD10K training, SINet flips half of the images horizontally, ensuring robust detection when camouflaged animals face either direction.",
FPNet,random rotation,"Rotating images by a random angle within a set range. This augmentation exposes the model to varied object orientations, helping it detect camouflaged shapes regardless of alignment.",Rotating jungle images up to ±30° during training to handle tilted leaf orientations.,
FPNet,random seed,A fixed initial value controlling data shuffling and weight initialization to ensure reproducibility.,"Setting a random seed of 42 for NumPy, PyTorch, and CUDA to replicate FPNet training runs exactly.",
FPNet,real-time detection,Processing speed (often ≥30 FPS) sufficient for live video applications. COD methods must balance accuracy with throughput to meet real-time constraints.,Achieving 32 FPS on 256×256 frames using a single RTX 2080 Ti.,
RFB,real-time processing,"Inference speed ≥25 frames per second (FPS), suitable for live video applications.",RFBNet300 runs at 83 FPS on a single Titan X GPU.,
SINet,recall,The ratio of true‐positive pixels to all ground‐truth foreground pixels. It measures completeness by penalizing missed detections.,"If the ground‐truth mask has 1000 object pixels and SINet correctly labels 850 of them, recall = 850/1000 = 0.85.",
Octave Convolution,receptive field,The region in the input space that a particular output unit “sees.” Larger receptive fields capture more context but cost more compute if at high resolution.,A 3×3 convolution on a 224×224 map yields a 3×3 receptive field per output; stacking two such layers expands the effective receptive field to 5×5.,
RFB,receptive field (RF),The spatial region in the input image that influences the activation of a single unit in a convolutional feature map. RF size reflects how much context (neighborhood) each unit “sees.”,"In VGG16, a 3×3 convolution at conv3_3 yields an effective RF of 7×7 pixels on the original image.",
NCD,receptive field (RF),"In the context of CNNs, the RF of a feature activation is the region of the input image that influences that activation. SINet’s RF module uses varied convolutional dilation rates to capture both fine detail and global context.",A convolutional layer with dilation 2 in the RF module can “see” a 5×5 pixel area even though its kernel is only 3×3.,
SINet,receptive field (RF) module,"A multi‐branch convolutional block that applies dilated convolutions with varying dilation rates to each backbone feature map. By mixing multiple dilation scales, it enlarges the network’s “field of view” without pooling, capturing both local details and global context.","On a grassland image, the RF module’s branch with dilation 4 sees a 15×15 patch, helping spot faint zebra stripes that span a wider area than a standard 3×3 kernel.",
RFB,receptive field block (RFB),"A custom module combining multi-branch convolutions with varying kernel sizes and dilation rates, then concatenating and merging via 1×1 conv.","The RFB inserted after conv4_3 has three branches with RFs of sizes {1,3,5} and dilations {1,3,5}.",
FPNet,recreational art,"Artistic images using camouflage patterns to hide subjects, providing challenging benchmarks for COD evaluation.",Detecting a hidden human figure painted in trompe-l’œil street art scenes.,
FPNet,recreational art,Artistic images where subjects are purposely hidden in the scene. These provide extreme COD benchmarks due to deliberate camouflage patterns.,Detecting a hidden figure painted in trompe-l’œil street art blending into architectural details.,
SAM,reduction ratio (r),A hyperparameter that shrinks the MLP’s hidden layer size relative to channel count C to control capacity and overhead. CBAM typically uses r=16.,"With C=512 and r=16, the MLP’s hidden size is 512/16=32.",
SINet,reduction ratio (r),"A hyperparameter in channel attention modules that controls the dimensionality reduction of the channel descriptor before expansion. A larger r yields more compact MLP representations, reducing parameters at the cost of capacity.","In SA, SINet sets r=16 to shrink the 256‐channel descriptor into a 16‐dimensional hidden state before expanding back, striking a balance between model complexity and expressive power.",
SAM,refined feature map,The output after CBAM’s sequential channel and spatial attention: F″ ∈ R^{C×H×W}. This refined map is forwarded to subsequent layers.,F″ has the same shape as F but with suppressed background activations.,
FPNet,refined mask prediction,"Final high-resolution segmentation mask after Stage 2’s correction fusion and boundary refinement, achieving precise delineation of camouflaged object contours.",Outputting a 256×256 binary mask perfectly outlining a camouflaged deer’s shape.,
FPNet,region proposal network (RPN),A CNN component that predicts objectness scores and bounding boxes over anchors at multiple scales.,Employing an RPN to suggest likely locations of camouflaged turtles in pond images.,
FPNet,region-aware supervision,"Training signals applied to entire connected object regions (e.g., IoU or Dice loss) to encourage structural coherence in predictions beyond individual pixels.",Computing IoU loss over each predicted lizard mask region.,
FPNet,region-wise supervision,"Supervising predictions at the level of connected regions or entire masks, often via IoU-based losses, to enforce holistic structural consistency beyond individual pixels.",Incorporating IoU loss over the entire predicted fish mask to encourage full-region overlap.,
RFB,regression head,"The branch of the detection head dedicated to predicting bounding-box coordinate offsets relative to default boxes, typically trained with Smooth L1 loss.","On conv10_2, the regression head is a 3×3 conv layer that predicts (Δx, Δy, Δw, Δh) for each of 6 anchors per spatial cell.",
PVT,relative positional encoding,"A scheme where attention bias depends on the relative distance between tokens, rather than absolute position, enabling better generalization across varying input sizes.",PVT could be extended with relative positional encodings so that Stage 2 attention biases by the offset (i–j) between patches instead of their absolute grid coordinates.,
SAM,rectified linear unit (ReLU),"An activation function f(x)=max(0, x) that introduces nonlinearity and mitigates vanishing gradients. CBAM’s MLP uses ReLU between its two FC layers.","After W₀ reduces dimensions, ReLU zeros out negatives before W₁.",
FPNet,ReLU activation,"The Rectified Linear Unit applies f(x)=max(0,x) to introduce non­linearity. It is computationally efficient, alleviates vanishing gradients, and encourages sparse activations, but can suffer from “dying ReLUs” if many units become inactive.",Applying ReLU after each convolution to zero out negative pixel responses.,
SINet,ReLU activation,"A non‐linear function f(x)=max(0,x) applied element‐wise to feature maps, introducing sparsity and avoiding saturation in the positive regime. ReLU accelerates convergence by mitigating vanishing gradients.","After each batch‐normalized convolution in the RF module, SINet applies ReLU to inject non‐linear modeling power, enabling the network to distinguish subtle camouflaged textures.",
SAM,residual attention network,An encoder-decoder style attention network combining residual learning with spatial attention maps.,RA-56 applies attention masks learned via an encoder-decoder to residual blocks.,
SAM,residual block,"A convolutional block that adds its input to its output via a skip connection, easing optimization of deep networks.",A ResNet bottleneck block adds the 1×1–3×3–1×1 conv output to its input.,
Octave Convolution,residual block,"A network module with a skip-connection that adds the block’s input to its output, enabling very deep network training.",The basic ResNet block: two 3×3 convolutions plus an identity shortcut.,
SINet,residual block,"A module that adds the input of two or more convolutional layers directly to their output, forming an identity shortcut. This skip connection mitigates vanishing gradients and enables the training of very deep networks by learning residual mappings instead of full transformations.","In ResNet-50, each residual block in stages 3–5 produces feature maps (C3–C5) for SINet, preserving information flow while extracting increasingly abstract representations.",
PVT,residual bottleneck,"A residual block pattern where the main branch temporarily expands channel dims, processes information, then projects back down, balancing capacity and compute. The bottleneck pattern controls parameter growth.","PVT’s FFN first expands from 384 to 1,536 dims then projects back to 384 in each residual block.",
FPNet,residual connection,"A skip connection that adds a layer’s input directly to its output. Residual connections ease gradient flow, enable very deep networks, and allow the model to learn residual mappings rather than full transformations.",Adding input features to the output of a 3×3 convolution block in ResNet-50.,
FPNet,residual connection,"Bypass link that adds a layer’s input directly to its output, easing gradient flow and enabling very deep architectures by learning residual mappings.",Adding each transformer block’s input tokens to its output to stabilize learning.,
FPNet,residual connection,A shortcut that adds input features to a block’s output. It improves gradient flow and allows very deep network designs.,Adding input tokens directly back to the output of each transformer sub-block.,
PVT,residual feature reuse,"The practice of feeding input features forward via skip connections so later layers can directly access earlier representations, improving gradient flow.","Every PVT encoder block adds its input tokens back to its FFN output, enabling reuse of original features throughout the network.",
SINet,residual learning,"A strategy where networks learn residual (difference) mappings via skip connections, simplifying optimization and enabling deeper architectures.",SINet’s ResNet-50 backbone uses residual blocks to extract robust features without degradation across 50+ layers.,
SAM,ResNet,A deep residual network employing skip connections to enable very deep architectures.,ResNet-50 uses 16 bottleneck blocks with identity shortcuts.,
FPNet,ResNet (residual network),"A deep CNN architecture built from residual blocks leveraging residual connections. ResNets can scale to hundreds of layers with improved training stability, serving as common backbones for segmentation and detection tasks.",Using ResNet-101 pretrained on ImageNet as the base encoder for FPNet.,
FPNet,ResNet backbone,A CNN architecture built from residual blocks with skip connections. Commonly used to extract hierarchical features before adding frequency modules.,Using ResNet-50 pretrained on ImageNet to supply multi-level RGB features for FPNet.,
RFB,ResNet-101,A 101-layer deep convolutional neural network using residual connections to ease gradient flow.,ResNet-101 has a theoretical RF of 483×483 at its deepest layer on a 224×224 input.,
SAM,ResNeXt,A ResNet variant introducing cardinality via grouped convolutions in each block.,ResNeXt-50 (32×4d) uses 32 parallel paths with 4 filters each.,
RFB,RetinaNet,A one-stage detector that uses Focal Loss to address class imbalance and achieves two-stage detector accuracy.,RetinaNet500 (ResNet-101-FPN) achieves 34.4 AP on COCO at 90 ms/image.,
PVT,RetinaNet head,"A detection head with a classification subnet and a box-regression subnet that operates on each level of a feature pyramid, typically using convolutional layers.","When plugged on top of PVT+FPN, the RetinaNet head applies 4 layers of 3×3 convs to each 28×28 feature map to predict anchors and class scores.",
RFB,retinotopic map,A mapping of visual field coordinates to cortical positions where neighboring image locations project to neighboring neurons. It inspired RFB’s spatial arrangement of multi-scale RFs.,Fig. 1 in the paper shows circles of different radii arranged to mimic retinotopic pRF layouts.,
RFB,RFB net,"The full one-stage detector formed by replacing top SSD layers with RFB modules, achieving improved accuracy at real-time speed.",RFBNet300 (input 300×300) runs at 83 FPS on Titan X with 80.7% VOC2007 mAP.,
FPNet,RGB domain,"The standard color-space representation of images using red, green, and blue channels. Most neural networks operate in the RGB domain before any frequency-domain transformation or filtering is applied.","Feeding raw 3-channel (R,G,B) camera data into a CNN for initial feature extraction.",
FPNet,RGB features,"Feature maps derived purely from color-space convolutions or attention layers. Often lack strong cues for low-contrast scenes, hence complemented by frequency features in FPNet.","Convolving the R, G, B channels to detect a red fox against autumn leaves.",
FPNet,ROI align,Improved ROI pooling using bilinear interpolation to avoid quantization errors in proposal coordinates.,Aligning proposals precisely over a hidden frog’s contour to preserve edge details.,
FPNet,ROI pooling,Extracting a fixed-size feature map from each proposed region by pooling activations within its bounds.,Pooling 16×16 pixel features from each RPN box to standardize inputs for the mask head.,
RFB,RoIAlign,A bilinear-interpolation layer that accurately extracts features for region proposals in two-stage detectors.,Mask R-CNN uses RoIAlign instead of RoIPool to avoid misalignments.,
PVT,ROIAlign,"A differentiable pooling operation that extracts fixed-size feature maps (e.g., 7×7) for each Region of Interest by bilinear sampling, avoiding quantization artifacts.","Mask R-CNN+PVT uses ROIAlign to sample 14×14 Stage 3 features for each ground-truth box, producing 7×7 pooled tokens per ROI.",
SINet,S_object,The object-aware component of Structure-measure focusing on global foreground and background mean statistics. It evaluates whether the predicted mask’s coarse shape matches the GT silhouette.,"For a 256×256 camo map, S_object=0.902 if the mean pixel values within the predicted and GT regions closely coincide.",
SINet,S_region,The region-aware component of Structure-measure that subdivides the map into four quadrants and compares local statistics. It captures regional layout consistency.,"A camouflaged frog image yields S_region=0.875, indicating SINet’s mask respects left/right and top/bottom texture distributions in each quadrant.",
FPNet,S-measure,A structural similarity metric for segmentation that considers region-aware and object-aware scores.,Obtaining an Sα of 0.88 on chameleon instances with blurry outlines.,
FPNet,S-measure,A structural similarity metric combining region-aware and object-aware scores. It captures both foreground completeness and boundary alignment.,"Achieving an Sα of 0.88 on CHAMELEON, indicating high structural fidelity of saliency maps.",
FPNet,saliency map,"A grayscale heatmap indicating the “importance” or conspicuity of each pixel in an image. Saliency maps are used in both SOD and COD to guide networks toward regions likely containing objects, although camouflaged objects demand specialized saliency cues.",Highlighting the most contrasting region in an urban scene to spot a hidden pedestrian.,
SINet,salient instance / object detection,A subfield of SOD that outputs both instance-level masks and bounding boxes for each detected salient object. It combines elements of object detection and segmentation gated by human attention cues.,"Salient instance outputs can be used to mine negative examples for SINet’s training: bright, high-contrast objects that should not be flagged as camouflage.",
FPNet,salient object detection (SOD),The task of finding the most visually distinctive objects in an image. COD is a more challenging variant since camouflaged objects are intentionally low-saliency.,Detecting a brightly colored balloon in a cluttered party scene.,
SINet,salient object detection (SOD),"A task that identifies and segments the most attention-grabbing foreground objects in an image. Salient objects contrast strongly with their backgrounds, the opposite of COD’s goal.",Using SOD outputs as negative samples helps SINet learn that high‐contrast regions should be ignored for camouflage detection.,
RFB,sampling distance,"The spacing between taps in a dilated convolution, equal to dilation rate.",Dilation=4 in a 3×3 conv yields a sampling distance of 4 pixels.,
RFB,sampling grid,The pattern of positions (offsets) where convolution kernel taps are applied on the feature map.,"A standard 3×3 grid has offsets {(-1,-1),(-1,0),…, (1,1)}.",
FPNet,scaled dot-product attention,"Core operation in transformers computing similarity between query and key vectors (via dot product), scaled by the root of their dimension, and applied to value vectors to yield attended features.",Computing attention scores between patch tokens to link a camouflaged animal’s head and tail patches.,
PVT,scaling factor,"The constant (1/√d_k) multiplied with dot-product attention scores to prevent them from growing too large, which could push softmax into saturated regimes and harm gradient flow.","In PVT’s attention, dot products are scaled by 1/√64 before softmax when each head dimension is 64.",
NCD,search attention (SA),A channel‐wise attention mechanism that reweights feature channels according to their relevance for spotting camouflaged regions. SA computes global descriptors and generates per-channel scaling factors via a small MLP.,"In the “search” stage, SA boosts channels responding to sharp edges and textures—helping SINet localize a hidden lizard against bark patterns",
SINet,search attention (SA),"A channel-wise attention mechanism that reweights each RF-enhanced feature channel according to its importance for spotting camouflaged regions. SA pools spatially, passes descriptors through a small MLP, and produces per-channel scaling factors applied back to the feature map.","In the Search branch, SA may boost channels responding strongly to edge-like patterns—thereby sharpening SINet’s focus on subtle contours of a concealed bird against tree bark.",
NCD,search identification network (SINet),A two-stage deep network designed specifically for camouflaged object detection. In the search stage it locates approximate regions of potential hidden objects; in the identification stage it refines and segments them.,"On the COD10K test set, SINet first highlights candidate regions containing a camouflaged butterfly, then produces a precise binary mask around its wings.",
SINet,search identification network (SINet),A two‐stage deep convolutional framework for camouflaged object detection. In the Search stage it roughly localizes regions likely to contain hidden objects; in the Identification stage it refines those regions into a precise segmentation mask.,"Applied on a forest image, SINet first flags patches containing a camouflaged deer, then outputs a pixel‐wise mask that tightly follows the deer’s outline.",
SINet,search stage,"The first module in SINet, designed to quickly scan high‐level feature maps and highlight candidate object regions. It produces a coarse “attention” map indicating where a camouflaged object might lie.","On a beach scene, the Search stage might light up areas of sand where a hidden crab’s shell texture stands out from the background.",
PVT,segmentation head,A lightweight network that upsamples and refines feature maps to produce per-pixel class labels for semantic or instance segmentation.,"In Mask R-CNN+PVT, the segmentation head upsamples the 14×14 Stage 3 features via transpose convolutions to output a 224×224 mask.",
FPNet,self-supervised learning,"Pretraining a model on auxiliary tasks that generate their own labels (e.g., rotation prediction).",Using rotation classification of scrambled patches to pretrain FPNet’s transformer backbone before COD fine-tuning.,
FPNet,semantic consistency,"Ensuring that fused features maintain coherent semantic interpretations across layers or modalities, preventing contradictory activations.",Validating that high-frequency texture cues align with semantic object regions in deeper layers.,
SINet,semantic cue,"A high-level signal or pattern that indicates the presence of an object or concept, such as a color distribution or distinctive shape. Semantic cues help networks differentiate objects from backgrounds, even under camouflage.",SINet leverages semantic cues from C5—like the elongated form of a stick insect—to spot camouflaged regions in complex foliage backgrounds.,
FPNet,semantic hierarchy,The organization of feature representations from low-level (pixels/textures) to high-level (object semantics). Frequency-perception in FPNet is driven by tapping into multiple semantic levels for robust coarse positioning.,Visualizing feature pyramid levels showing leaf veins at stage1 and entire tree shapes at stage4.,
FPNet,semantic hierarchy,Layered arrangement of representations from low-level textures to high-level semantics; frequency cues at different depths exploit this hierarchy to inform coarse mask placement.,Fusing low-layer edge maps with deep semantic maps to detect a camouflaged fox silhouette.,
FPNet,semantic segmentation,The task of assigning a class label to each pixel. COD is a specialized segmentation problem with only one foreground class that blends into the background.,Predicting a binary mask that labels every pixel as “background” or “camouflaged object.”,
FPNet,semantic-driven frequency learning,Guidance of frequency separation by semantic hierarchy: deeper layers propose object-level semantics that steer the learnable frequency filters to focus on meaningful texture and contour patterns.,Using class-level features from ResNet’s deeper blocks to adjust frequency splits for detecting camo-snakes.,
FPNet,semantically driven frequency filters,Learnable filters guided by high-level semantic features. They adapt the spectral decomposition to focus on bands most relevant for object/background discrimination.,Using ResNet’s deepest features to adjust octave-conv separation for better lizard detection.,
FPNet,semi-supervised learning,Training with both labeled and unlabeled data to leverage larger datasets.,"Pretraining FPNet on 1,000 unlabeled nature photos with a self-supervised task, then fine-tuning on COD masks.",
PVT,sequence length,"The number of tokens (patch embeddings) input to a Transformer encoder at a given stage, equal to (H/stride × W/stride).","For a 224×224 input with 4×4 patch size, Sequence Length‌1 = 56×56 = 3,136 tokens.",
PVT,sequence length reduction,Shrinking the number of tokens processed in deeper stages to lower computational and memory costs. This reduction targets efficiency while preserving essential information.,"PVT reduces sequence length from 3,136 tokens in Stage 1 down to 49 tokens in Stage 4 through patch merging and pooling.",
PVT,sequence reduction ratio,"The factor by which token count decreases between stages, controlling the computational savings and resolution change.","PVT reduces the number of tokens by a ratio of 4 (from 3,136 to 784 to 196 to 49) across its four stages.",
FPNet,SGD optimizer,"Stochastic Gradient Descent updates model parameters using mini-batch gradient estimates. With carefully tuned momentum and learning rate schedules, SGD often outperforms adaptive methods in final test-set accuracy for large-scale vision models.",Training a segmentation backbone from scratch using SGD with momentum 0.8.,
FPNet,shallow high-resolution features (SHRF),"Early (low-level) network outputs that retain original image resolution and fine boundary details. In FPNet’s refinement stage, these features are fused last to sharpen object edges omitted by deeper, lower-resolution layers.",Fusing the first CNN block’s 1/4-scale feature map to crisply outline leaf margins.,
FPNet,shallow high-resolution features (SHRF),Early network outputs that retain near-input spatial resolution and fine boundary details; fused last to sharpen the predicted mask’s edges.,Concatenating the 1/4-scale output of ResNet’s first block to the final mask for crisp outlines.,
FPNet,shallow high-resolution features (SHRF),"The Stage-2 input from early network layers (e.g., ResNet’s C2) that preserve full or ¼ input resolution and fine boundary details, fused last to refine mask edges.",Concatenating the 128×128 C2 feature map with the corrected semantic map to crisply outline leaf margins,
SAM,shared MLP,"An MLP whose weights are shared between two inputs—here, the average-pooled and max-pooled channel descriptors. This reduces parameter count.","Both pooled vectors pass through the same MLP weights (W₀, W₁).",
FPNet,shifted window,"In some transformer variants, window partitions are shifted between layers to allow cross-window information exchange while retaining low compute.",Offsetting window grids by half a window size in alternating layers to mix neighboring regions.,
SINet,sigmoid,"A pointwise activation σ(x)=1/(1+e^(–x)) that squashes real-valued inputs into [0,1], making it suitable for binary segmentation outputs.","SINet applies sigmoid to the final identification logits, converting raw scores into per-pixel probabilities for camouflaged object presence.",
FPNet,sigmoid activation,"A bounded activation function σ(x)=1/(1+e^(−x)) that maps real values into (0,1). It is suitable for binary classification or mask probability outputs but can saturate during training, leading to vanishing gradients for large input magnitudes.",Producing per-pixel probabilities when predicting a binary camo-object mask.,
SINet,sigmoid activation,"A pointwise function σ(x)=1/(1+e^(−x)) that converts arbitrary real‐valued inputs into the [0,1] range, suitable for modeling probabilities in binary classification tasks such as foreground vs. background segmentation.",SINet applies a sigmoid to the final identification map to produce a per‐pixel probability pi indicating the likelihood that pixel i belongs to a camouflaged object.,
SAM,sigmoid activation function,"A gating function σ(x)=1/(1+e^{-x}) that squashes real-valued inputs into (0,1), useful for attention weights. CBAM applies sigmoid to MLP and convolution outputs.","The channel attention logit vector [2.0, –1.0] becomes [0.88, 0.27] after σ.",
SAM,sigmoid function (σ),"A logistic function used to generate attention weights in [0,1]. In CBAM, σ gates both channel and spatial refined maps.",Weighting a feature activation of 5.0 by σ(5.0)=0.993 yields 4.97.,
FPNet,sigmoid gating,The use of a sigmoid activation to produce weights between 0 and 1 for gating information flow. It allows the network to softly select or suppress features based on learned importance.,Applying sigmoid gating to combine high- and low-frequency streams adaptively.,
SAM,sigmoid gating,"Using a sigmoid to produce values in [0,1], functioning as a soft gate to scale information.",A gating value of 0.6 retains 60% of the original activation.,
SINet,similar color (SC),"An attribute indicating that object and background share very similar color distributions, assessed via histogram distance metrics.","A fish whose scales and water share nearly identical hues is labeled “SC,” challenging SINet’s reliance on color contrast.",
RFB,single shot detector (SSD),A one-stage detector that simultaneously predicts object classes and box offsets on multiple feature maps in a single forward pass.,SSD300 uses six feature maps (38×38 to 1×1) to detect objects from small to large.,
PVT,single-head self-attention,"A simplified attention mechanism with only one head, where queries, keys, and values are projected only once. It reduces computational cost but captures fewer relational subspaces.",A lightweight PVT variant might use single-head attention in Stage 1 to save FLOPs at the expense of representation diversity.,
FPNet,skeletonization,"Reducing a binary mask to its medial axis (one-pixel-wide skeleton), useful for shape analysis and topology preservation.",Skeletonizing a segmented snake mask to analyze its curvature.,
FPNet,skip connection,"A connection that bypasses one or more layers, directly adding or concatenating earlier feature maps to later layers. Skip connections mitigate vanishing gradients, preserve spatial details, and help fuse features at different semantic levels.",Concatenating early CNN features with deep features to maintain edge information in segmentation.,
FPNet,skip connection,A shortcut that bypasses one or more layers by adding or concatenating an earlier feature map to a later layer. It preserves fine spatial details and mitigates vanishing gradients. (Also covered earlier but reiterated for decoder design.),Concatenating the encoder’s C2 map with the decoder’s upsampled map before convolution.,
PVT,skip connection,A residual pathway that bypasses one or more layers by adding the input of a block directly to its output. This helps mitigate vanishing gradients and stabilizes training in deep networks.,"Every Transformer encoder block in PVT adds its input tokens back to the FFN output, ensuring stable gradients and feature reuse across layers.",
SAM,skip connection,"A shortcut linking a block’s input directly to its output, facilitating gradient flow.","In ResNet, identity skip connections bypass conv layers.",
SINet,skip connection,"A pathway that routes signals from an earlier layer directly to a later layer, bypassing intermediate transformations. Skip connections facilitate gradient flow and enable multi‐level feature reuse.","PDC employs skip connections by linking RF outputs of C4 and C5 directly into the decoder, preserving fine details lost in deeper convolutions.",
SINet,small object (SO),"An attribute where the object-to-image area ratio is below 0.1, evaluating detection of tiny, easily overlooked targets.",A small grasshopper in a 512×512 image might occupy only 30×30 pixels and be marked “SO.”,
RFB,smooth L1 loss,"A loss with L2 behavior for small errors and L1 for large errors, preventing gradient explosion.",Error δ = 3.0 yields Smooth L1(δ) =,−0.5 = 2.5.
FPNet,smoothness loss,"A regularization loss that penalizes large mask gradients, encouraging smoother predictions. By discouraging spurious boundary oscillations, it yields more cohesive mask regions in segmentation tasks.",Adding smoothness loss to suppress speckled mask artifacts around a camouflaged rock.,
FPNet,softmax activation,"A normalization function that converts a vector of real-valued scores into a probability distribution. In segmentation, softmax is used for multi-class mask predictions across pixels, ensuring all class probabilities sum to one at each location.","Classifying each pixel into “background,” “tree,” or “animal” categories.",
FPNet,softmax cross-entropy,A loss combining softmax activation with cross-entropy measurement. It penalizes differences between multi-class probability outputs and one-hot ground truths at each pixel.,"Computing softmax cross-entropy over three classes (“bg,” “object1,” “object2”) in multi-instance COD.",
PVT,softmax cross-entropy,"A loss function combining softmax activation with cross-entropy to measure classification error across mutually exclusive classes, widely used in both image classification and segmentation.","During pre-training on ImageNet, PVT optimizes softmax cross-entropy over 1,000 classes on the [CLS] token.",
RFB,softmax loss,The multiclass cross-entropy loss after applying Softmax activation over class logits.,"Given logits [2,1,0], p_class 0 = e²/(e²+e¹+1) then loss = −log(p_true).",
PVT,softmax normalization,A function that converts raw attention scores into a probability distribution by exponentiating and normalizing them so they sum to one. This ensures attention weights are positive and sum to one.,"After computing dot products between queries and keys, PVT applies softmax over each row of the score matrix to obtain attention weights.",
RFB,spatial array,The layout of sampling points (RF centers) over the feature map induced by multi-branch dilated conv.,"Fig. 2’s final array shows points at offsets {0,±3,±5} around center.",
FPNet,spatial attention,"A mechanism that generates a 2D weight map over spatial locations, allowing the network to focus on the most relevant regions when fusing or refining features.",Computing a 64×64 attention mask that highlights the fox’s silhouette area.,
SAM,spatial attention map,"A H×W×1 tensor of values in [0,1] produced by the spatial attention module to weight spatial locations in the refined feature map.",A 7×7 spatial attention map might highlight central pixels over background.,
SAM,spatial attention module,"The second sub-module of CBAM that computes a two-dimensional attention map over spatial locations. It concatenates average-pooled and max-pooled channel descriptors along the channel axis, applies a 7×7 convolution, and a sigmoid activation to focus on “where” to attend.","On a refined feature map of shape 512×7×7, spatial attention highlights foreground pixels corresponding to the object.",
SAM,spatial axis,The two axes indexing height and width in a feature map. CBAM’s spatial attention operates across these.,Axes 1 and 2 in R^{512×14×14} represent H and W.,
SAM,spatial descriptor,The condensed 2D map (H×W) obtained by pooling across channels and summarizing spatial importance.,A 14×14 spatial descriptor used to compute spatial attention.,
SAM,spatial dimension (H×W),The height and width of a feature map’s spatial resolution. CBAM’s spatial attention operates across these dimensions.,A feature map of size 14×14 has H=14 and W=14 spatial locations.,
Octave Convolution,spatial frequency,"A measure of how rapidly pixel or feature values change over space. High frequencies encode fine details, low frequencies encode coarse structures.","Edges in an image correspond to high spatial frequencies, while smooth sky regions correspond to low frequencies.",
SAM,spatial Information,The arrangement and importance of activations over spatial positions. CBAM’s spatial attention highlights salient regions.,Pixels belonging to an object in the center of an image receive higher weights.,
PVT,spatial reduction layer,A specialized attention variant (SRA) that applies pooling to key/value maps before attention to lower token count and computation.,"In PVT’s Stage 2 SRA, keys and values are average-pooled by factor 2, reducing 784 tokens to 196 before dot-product attention.",
RFB,spatial resolution,"The height×width of a feature map, determining localization granularity.","conv4_3 has 38×38 resolution, while conv11_2 is 1×1.",
Octave Convolution,spatial resolution,The height and width size of a feature map tensor. Reducing spatial resolution via pooling or strided operations lowers memory and compute cost.,Down-sampling a 224×224 feature map to 112×112 halves both height and width resolution.,
SINet,spatial resolution,"The height and width dimensions of a tensor, determining the granularity at which spatial details are represented. Higher resolution captures finer details but requires more computation.","On a 256×256 input, SINet’s backbone produces C3 at 64×64, C4 at 32×32, and C5 at 16×16, balancing detail preservation with context aggregation.",
PVT,spatial-reduction attention (SRA),"A variant of multi-head self-attention that applies a pooling or stride operation to key/value maps, reducing the number of tokens and hence memory and computation.","In Stage 2’s SRA, keys and values are pooled by a factor of 2, so attention operates over 196 tokens instead of 784, cutting cost by ~75%.",
PVT,spatial-reduction ratio,"The pooling or stride factor applied to keys/values in SRA, determining how much the token count is reduced before attention.","A spatial-reduction ratio of 2 reduces a 28×28 grid of keys/values to 14×14 for attention in Stage 2, cutting tokens from 784 to 196.",
PVT,spatial-reduction ratio,"The pooling or stride factor applied to keys/values in SRA to reduce token count before attention, trading off granularity for efficiency.","In Stage 3 SRA, a ratio of 2 pools the 14×14 grid to 7×7 keys/values, lowering attention FLOPs by 75%.",
SAM,spatial-wise multiplication,Broadcasting and multiplying a 1×H×W attention map across C channels to reweight spatial positions.,A 1×14×14 map multiplies a 256×14×14 feature map at each pixel.,
SAM,spatial-wise pooling,"Reducing channel dimension to obtain a spatial descriptor, often by average or max over channels. CBAM uses both on F′.",Average pooling across 512 channels at each pixel yields a 1×14×14 map.,
RFB,speed-accuracy tradeoff,The balancing act between inference speed (FPS) and detection performance (mAP).,RFB512 gains 1.6 mAP over SSD512 at the cost of 11 FPS drop.,
FPNet,speed–accuracy trade-off,The balance between model inference speed and detection performance. FPNet designs aim to maximize accuracy with minimal latency increase over baseline backbones.,Reducing FPNet’s refinement iterations to boost FPS from 20 to 28 with a 1% drop in weighted F-measure.,
SAM,squeeze operation,"The reduction along spatial dimensions (H×W) to produce a channel descriptor, often via pooling.",Applying global average pooling to “squeeze” a 512×7×7 map to a 512-vector.,
SAM,squeeze-and-excitation (SE) module,A channel-only attention module that squeezes spatial dimensions via global pooling and excites channels via an MLP.,SE-ResNet-50 adds SE units after each residual block to boost accuracy.,
PVT,stage,"A sequential block in the pyramid, comprising patch embedding, several TF-E modules, and optional downsampling to produce one scale of features.","PVT’s four stages correspond to resolutions: 56×56, 28×28, 14×14, and 7×7, with increasing embedding dims.",
PVT,stage-specific embedding,"The embedding dimension assigned to tokens in a particular pyramid stage, which can vary across stages to balance resolution and feature capacity.","PVT-Base sets stage embeddings to [64, 128, 320, 512] for stages 1–4, respectively.",
SINet,state-of-the-art (SOTA),An acronym indicating the best published performance on a given benchmark or task at the time of reporting. It often serves as a comparison baseline to gauge new model advances.,SINet achieves SOTA segmentation accuracy on COD10K compared to prior salient/object-detection networks repurposed for camouflage.,
FPNet,state-of-the-art (SOTA),The best published performance on a given benchmark.,Reporting FPNet’s lowest MAE and highest weighted F-measure on COD10K compared to all prior methods.,
FPNet,statistical significance test,"A hypothesis test (e.g., paired t-test) applied to metric differences across models. It determines whether observed improvements are unlikely due to chance.",Conducting a paired t-test on S-measure results of FPNet versus SINet-V2 to confirm significance.,
RFB,stochastic gradient descent (SGD),"An optimizer updating weights by computing gradients on mini-batches, optionally with momentum.","Learning rate = 0.001, momentum = 0.9, batch size = 32 yields stable convergence.",
SINet,stochastic gradient descent (SGD),An optimization algorithm that updates model parameters by computing gradients on random minibatches and stepping in the negative gradient direction. SGD may include momentum and weight decay for improved convergence.,"Although SINet uses Adam, one could switch to SGD with momentum (e.g., 0.9) to train the RF and PDC modules on COD10K for potentially different convergence behavior.",
PVT,stride,The factor by which input spatial dimensions are reduced when forming patch tokens; determines how much overlapping or downsampling occurs.,A patch embedding with stride 4 maps a 224×224 image to 56×56 tokens.,
RFB,stride,The step size at which convolution or pooling windows move; larger strides yield more downsampling.,A stride of 2 in a 3×3 conv reduces a 64×64 feature map to 32×32.,
Octave Convolution,strided convolution,"A convolutional operation where the filter moves by more than one pixel (stride >1), effectively down-sampling the spatial dimensions.",A 3×3 convolution with stride 2 halves the height and width of the output feature map compared to the input.,
SINet,strided convolution,"A convolutional layer with stride>1, which both filters and downsamples feature maps in one operation, reducing spatial size while learning.","SINet’s backbone C5 is produced by successive strided convolutions (stride=2) in ResNet-50, shrinking a 64×64 map to 32×32 then to 16×16.",
PVT,strided patch merging,"A downsampling operation that moves the merging window with stride >1, thereby reducing both overlap and resolution in one step. It shrinks token count without additional pooling layers.","PVT applies strided patch merging with stride 2 at each stage transition, halving the spatial dimensions from 28×28 to 14×14, then 7×7 in subsequent stages.",
SINet,structure-measure (Sα),"A metric that evaluates structural similarity between prediction and ground truth, combining object‐aware and region‐aware components. Higher Sα indicates better preservation of object shapes and contextual layout.","On COD10K, SINet achieves Sα=0.890, reflecting its ability to capture the intricate silhouette of a hidden lizard.",
SINet,StructureMeasure,A MATLAB function that wraps S_object and S_region to compute the overall Structure-measure Sα.,"StructureMeasure(pred,gt) returns Sα=0.887, summarizing SINet’s structural performance on edge‐blended moth images.",
SINet,stuff vs. things,"In panoptic segmentation, “stuff” refers to uncountable amorphous regions (e.g., sky, grass) while “things” are countable objects (e.g., cars, animals). This distinction guides different labeling and evaluation protocols.","A camouflaged snake in grass is a “thing,” but the surrounding grass is “stuff” in a panoptic annotation scheme.",
RFB,sub-pixel convolution,Upsampling technique that rearranges feature channels into spatial dimensions (pixel shuffle).,"In super-resolution, a 64×64×256 tensor is reshaped to 256×256×64 by pixel shuffle.",
PVT,task-agnostic backbone,"A feature extractor usable across different vision tasks (classification, detection, segmentation) without task-specific modifications.",PVT is task-agnostic: the same pre-trained PVT weights serve both classification on ImageNet and segmentation on ADE20K.,
PVT,task-specific decoder,"A decoder module (often Transformer-based) tailored to a particular dense prediction task (e.g., DETR for detection, a mask decoder for segmentation).",PVT+DETR uses the DETR decoder as a task-specific decoder for end-to-end object detection.,
SINet,taxonomic system,"The hierarchical classification of camouflaged object categories (e.g., aquatic→fish→seahorse) used in COD10K to organize and analyze dataset distribution.","COD10K’s taxonomic system reveals an imbalance: 1,200 “butterfly” images versus 300 “seahorse” images, guiding data‐augmentation strategies.",
Octave Convolution,tensor,"A multi-dimensional array of numbers, generalizing vectors (1D) and matrices (2D) to higher dimensions. Tensors are the basic data structure in deep learning libraries.","The input to a CNN is often a 4-D tensor of shape (batch_size, channels, height, width).",
SINet,tensor,"A multi-dimensional array of numerical values that holds data in deep learning frameworks. Images, feature maps, and weight parameters are all represented as tensors.","The backbone outputs three tensors—C3 (64×64×512), C4 (32×32×1024), and C5 (16×16×2048)—which the RF module processes in parallel.",
FPNet,texture feature,"A descriptor capturing repeated patterns or local variations in intensity across an image patch. Techniques like Gabor filters or local binary patterns quantify texture richness, which can help distinguish subtle object-background differences.",Differentiating bark from leaves by analyzing local binary pattern histograms on tree textures.,
FPNet,threshold selection,Method for choosing the cutoff used in binarization (fixed or adaptive). It impacts the precision-recall balance of the final mask.,Setting the binarization threshold to maximize F-measure on the validation set.,
SINet,thresholding,"The process of converting continuous-valued outputs (e.g., probabilities) into discrete labels by comparing each value against a cutoff. In segmentation, thresholding yields a binary mask from a probability map.","After inference, SINet thresholds its probability map at 0.5, labeling all pixels with pi≥0.5 as “foreground” in the final camouflage mask.",
FPNet,throughput,Number of samples (or frames) a model can process per second (FPS) during inference.,Achieving 14 FPS when running FPNet on 1024×1024 video streams for wildlife monitoring.,
PVT,throughput,"The number of images (or tokens) processed per second in inference or training, indicating efficiency at scale.","A multi-GPU setup yields ~1,200 images/s throughput when training PVT-Small with batch-size 32 per GPU.",
RFB,top-down architecture,A design that propagates high-level semantic features down to lower-resolution maps to enrich detail.,FPN’s top-down path upsamples conv5_3 to merge with conv4_3.,
FPNet,transfer learning,"A learning paradigm where knowledge from one domain is transferred to another. In vision, transfer learning often involves reusing backbone features for new tasks, reducing data and compute requirements.",Using a segmentation model trained on SOD benchmarks to jump-start COD performance.,
PVT,transfer learning,"Adapting a model trained on one task/domain to another, often by initializing weights from pre-training and then fine-tuning.",PVT transfers learned features from ImageNet classification to segmentation on ADE20K by reusing Stage 1–4 weights.,
RFB,transfer learning,"The practice of reusing knowledge (model weights) learned from one domain (e.g., classification) to improve learning speed and performance on another domain (e.g., object detection).","RFBNet leverages ImageNet VGG16 as its backbone rather than training from scratch, boosting mAP and accelerating convergence.",
FPNet,transformer backbone,"A vision transformer (ViT or variant) used to extract hierarchical features from the RGB input. Transformers capture long-range dependencies via self-attention, which benefits detecting low-contrast, spatially distributed camouflaged patterns.",Employing ViT-B/16 to encode global context for detecting a camouflaged bird on a tree.,
FPNet,transformer backbone,A network composed of self-attention and MLP layers that processes tokenized image patches. It captures long-range dependencies beneficial for low-contrast detection.,Employing PVTv2 to encode both local leaf vein textures and global tree canopy structure.,
PVT,transformer encoder (TF-E),"The core block of self-attention layers followed by a feed-forward network, wrapped with residual connections and layer normalization.","PVT’s Stage 2 stacks 3 TF-E modules, each computing self-attention over a sequence of 128×128 patch tokens.",
FPNet,transformer token,A flattened vector representation of a patch or region used as input to self-attention layers; carries both spatial and spectral information once embedded.,Converting each 16×16 image patch into a 768-dim token for the ViT backbone.,
FPNet,transposed convolution,Also known as deconvolution; a learnable upsampling layer that inverts the spatial reduction of standard convolution. It reconstructs higher-resolution feature maps by learning how to distribute coarse activations.,Using a transposed conv to upsample a 32×32 coarse mask back to 64×64 in the decoder stage.,
FPNet,transposed convolution,A learnable upsampling operation that reverses spatial reduction. It redistributes coarse activations to reconstruct higher-resolution maps.,Using a 3×3 transposed conv with stride 2 to double the mask resolution from 128×128 to 256×256.,
RFB,two-stage detector,"An object detector that first generates region proposals, then classifies and refines them (e.g. Faster R-CNN).",Faster R-CNN with ResNet-101 backbone runs at 7 FPS with 80.5 mAP on VOC2007.,
FPNet,two-stage model,An architecture that processes input in two sequential steps (here: coarse localization then fine refinement). Each stage can leverage different cues—frequency for initial object search and semantic/RGB features for detailed delineation.,"Similar to Faster R-CNN’s proposal/classification split, but for segmentation tasks.",
FPNet,two-stage strategy,"A detection paradigm splitting processing into: (1) a coarse search stage (frequency-guided) to propose object regions, and (2) a fine recognition stage (RGB-guided refinement) to sharpen boundaries and suppress background.","First generating a rough heatmap of hidden frogs, then refining with high-res color cues for exact masks.",
FPNet,U-Net,A popular encoder-decoder network originally proposed for biomedical image segmentation. Its hallmark is symmetric downsampling and upsampling paths with lateral skip connections that fuse high-resolution features from the encoder into the decoder for precise localization.,Segmenting cell nuclei in microscopy images with crisp boundary delineation.,
FPNet,unconstrained face recognition,"Identifying faces under arbitrary conditions (e.g., low light, occlusion), which can leverage camouflage detection ideas to suppress background interference.",Recognizing obscured faces in crowd surveillance videos by first masking out background clutter.,
FPNet,unconstrained face recognition,"Identifying faces under varied lighting, occlusion, or pose conditions. Suppressing background noise via COD techniques can improve detection in cluttered scenes.",Recognizing a partially obscured face in a crowded subway station photograph.,
FPNet,upsampling,"The process of increasing spatial resolution, often via interpolation (bilinear) or transposed convolution. Up-sampling reconstructs feature maps to the original image size for pixel-wise predictions but may introduce artifacts if not properly regularized.",Using bilinear interpolation to double the size of a coarse river-shore segmentation mask.,
Octave Convolution,upsampling,Increasing a tensor’s spatial resolution by interpolation or transposed convolution to match another tensor’s size.,Using nearest-neighbor interpolation to enlarge a 112×112 low-frequency map back to 224×224 so it can be summed with the high-frequency map.,
RFB,upsampling,"Increasing spatial resolution via interpolation (nearest, bilinear) or learned deconvolution.",ASPP sometimes upsamples low-res feature maps back to input size via bilinear.,
SINet,upsampling,"Increasing the spatial resolution of feature maps, commonly via interpolation methods (e.g., bilinear) or learnable transpose convolutions. Upsampling restores low‐resolution predictions to match input size for pixel‐level tasks.",SINet upsamples the 16×16 search attention map by a factor of 16 using bilinear interpolation to obtain a 256×256 map aligned with the original image for refined segmentation.,
SINet,upsampling,"Increasing the spatial resolution of feature maps, typically via bilinear interpolation or transposed convolution. SINet upsamples low-res maps back to full image size for identification.","After computing a 32×32 identification map, SINet upsamples it by factor 8 with bilinear interpolation to yield a 256×256 prediction aligned with the original input.",
SAM,user study (interpretability evaluation),A human subject evaluation where participants judge the quality or relevance of visual explanations. CBAM authors conducted such a study to quantify interpretability gains.,Participants rated CBAM-Grad-CAM heatmaps as more accurate 78% of the time.,
FPNet,user-defined threshold,A probability cutoff set on grayscale prediction maps to binarize masks; adjusting this threshold trades off precision versus recall.,Binarizing masks at a 0.5 confidence threshold to compute F-measure.,
Octave Convolution,vanilla convolution,The standard convolution operation over full-resolution feature maps (without frequency splitting).,A 3×3 vanilla convolution applied to a 64-channel 224×224 tensor produces another 64-channel 224×224 tensor.,
FPNet,vertical flip,"A less common augmentation that flips an image top to bottom. It is useful when objects can appear in inverted orientations, but must be used cautiously if the domain has consistent gravity direction.","Inverting an aerial drone image so the sky appears below and ground above, testing orientation invariance.",
RFB,VGG16,A CNN backbone with 16 weight layers (13 conv + 3 fully connected) using only 3×3 kernels and 2×2 pooling.,SSD retains VGG16 up to conv7 (two fc layers converted to conv) as its base.,
SAM,VGGNet,A deep CNN architecture using repeated 3×3 convolutional layers with increasing depth.,VGG16 stacks thirteen 3×3 conv layers before three FC layers.,
FPNet,vision transformer (ViT),A neural network architecture adapted from NLP Transformers to process image patches with self-attention. Captures global context more effectively than local-only CNNs.,Training ViT-L/32 on 32×32 patches to classify camouflaged drone images.,
FPNet,visual examples,Qualitative input–output pairs presented in the paper to illustrate model behavior.,"Showing side-by-side images of a hidden owl, the ground truth, and FPNet’s predicted mask.",
SAM,W (Width),The spatial width dimension of a feature map. CBAM’s spatial attention map has width W.,"In ResNet-50 conv4, the feature map has W=28.",
SAM,W₀ (MLP weight matrix 1),The first weight matrix in CBAM’s shared MLP of shape (C/r)×C that reduces channel descriptor dimensionality.,"For C=512, r=16, W₀ is a 32×512 matrix.",
SAM,W₁ (MLP weight matrix 2),The second weight matrix in the shared MLP of shape C×(C/r) that restores the reduced descriptor to original channel size.,"For C=512, r=16, W₁ is a 512×32 matrix.",
FPNet,weakly-supervised learning,"Learning from coarse labels (e.g., image-level tags) instead of detailed masks.",Training a variant of FPNet using only “contains animal” tags rather than pixel-precise annotations.,
FPNet,weight decay,"A regularization technique that adds an L2 penalty on network weights to the loss. By discouraging large weights, weight decay reduces overfitting and improves generalization, especially in deep models prone to memorizing noise.",Applying a weight decay of 1e-4 during Adam optimization to prevent overfitting.,
RFB,weight decay,An L2 regularization term adding λ·‖w‖² to loss to discourage large weights and prevent overfitting.,Weight decay set to 5×10⁻⁴ penalizes large conv filter weights.,
SINet,weight decay,"An L2 regularization term added to the loss, penalizing large weights to prevent overfitting.",A weight decay of 1e–5 ensures SINet’s RF filters don’t become overly tuned to specific camouflage patterns in the training set.,
PVT,weight matrix,"A set of trainable parameters organized as a matrix, used in linear layers to transform input features (queries, keys, values, embeddings). Each such matrix is optimized during training via backpropagation.","In PVT’s MHSA, separate weight matrices W_q, W_k, and W_v each learn to project token embeddings into query, key, and value spaces respectively.",
SAM,"weight matrix (W₀, W₁)","The learned parameters of an FC layer. In CBAM, W₀ ∈ R^{C/r×C} reduces dimensions, and W₁ ∈ R^{C×C/r} restores them.",W₀ is a 32×512 matrix when C=512 and r=16.,
PVT,weight sharing,"Reusing the same parameters (e.g., Transformer block weights) across multiple layers or heads to reduce model size and enforce consistency. Weight sharing trades off expressiveness for parameter efficiency.","Unlike some compact Transformers, PVT does not share weights between stages, giving each stage its own encoder parameters for maximum flexibility.",
Octave Convolution,weight tensor,"A learnable multi-dimensional array of convolutional filter weights, e.g., of shape (out_channels, in_channels, k, k).","In OctConv, you have four weight tensors: W_H2H for high-to-high, W_H2L for high-to-low, W_L2H for low-to-high, and W_L2L for low-to-low filtering.",
SINet,weighted BCE (WBCE),"A variant of BCE that multiplies positive and negative terms by different weights (e.g., β for foreground, 1–β for background). It addresses class imbalance when camouflaged pixels are far fewer than background.","In SINet’s identification loss, foreground pixels (small insect wings) are upweighted by β=0.7 so that missing those tiny regions hurts more than occasional background leaks.",
FPNet,weighted F-measure,A blend of precision and recall that weights more heavily on true positives. Used in segmentation tasks to balance detection completeness and precision.,Achieving a weighted Fβ of 0.85 when optimizing for recall on COD10K.,
FPNet,weighted F-measure,A variant of the F-measure that weights precision and recall to emphasize true positives. It balances detection completeness against false alarms in camouflaged contexts.,Scoring a weighted Fβ of 0.85 on CAMO when prioritizing recall of hidden animals.,
SINet,weighted F-measure (WFβ),"A variant of F-measure that assigns pixel‐level weights based on proximity to object boundaries, penalizing errors near edges more severely. It better reflects segmentation quality in regions critical to human perception.",SINet’s WF₁ score of 0.862 demonstrates robustness in recovering fine boundary details of a camouflaged butterfly.,
SAM,WideResNet,A variant of ResNet that decreases depth but increases width (filters per layer) for efficiency.,WideResNet-28-10 has 28 layers and 10× wider blocks than ResNet-28.,
SAM,width,The number of filters (channels) per layer or block. A wider network can learn richer features.,WideResNet-28-10 uses 10× more filters per block than ResNet-28.,
FPNet,window-based self-attention,"An attention variant that restricts computations to non-overlapping local windows, reducing complexity from quadratic to linear in sequence length.",Applying 7×7 window attention within each image region to limit compute.,
SINet,word cloud distribution,"A visualization that displays object-category frequencies in COD10K, where word size corresponds to image count, aiding exploratory data analysis.","A word cloud might show “butterfly” in large font and “seahorse” in small font, prompting targeted data-augmentation for underrepresented classes.",
RFB,Xavier initialization,"A weight initialization scheme drawing from a distribution with variance 2/(fan_in + fan_out), which preserves signal variance across layers and stabilizes gradients in early training.",All newly added RFB module convolutions are initialized with Xavier so their outputs neither explode nor vanish during the first few epochs.,
SAM,Xception,A depthwise separable convolutional architecture extending Inception’s ideas to extreme factorization.,Xception replaces Inception modules with depthwise separable conv layers.,
RFB,YOLO,A one-stage detector that divides the image into grid cells and directly regresses bounding boxes and class probabilities.,YOLOv2 (Darknet-19) runs at 40 FPS but scores 21.6 AP on COCO.,
FPNet,zero-shot transfer,Deploying a model to detect categories it never saw during training.,Using FPNet to segment a new species of camouflaged insect without any retraining.,
SINet,α (loss weight factor),"A scalar hyperparameter balancing the contributions of the Search-stage and Identification-stage losses in the total training objective. Typically set to 1.0 in SINet, it ensures neither coarse localization nor fine segmentation dominates the gradient.","SINet uses α=1.0 so that L_total = L_search + α·L_identification, causing both stages to learn in tandem.",
SAM,σ (sigma),The sigmoid activation function used for gating in both channel and spatial attention.,"σ(0)=0.5, σ(2)=0.88, σ(–2)=0.12.",