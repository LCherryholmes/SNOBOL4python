{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT9eYD6et20w",
        "outputId": "fc4ec066-f1f0-40ad-cb63-b95951fcc79e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/mydrive\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "from pyspark import SparkContext\n",
        "from google.colab import drive\n",
        "sc = SparkContext.getOrCreate()\n",
        "drive.mount('/content/mydrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATAFILE_PATTERN = '^(.+),\"(.+)\",(.*),(.*),(.*)'\n",
        "def remove_quotes(s):\n",
        "    return ''.join(i for i in s if i != '\"')"
      ],
      "metadata": {
        "id": "xSOtoppcvD1k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data_file_line(data_file_line):\n",
        "    match = re.search(DATAFILE_PATTERN, data_file_line)\n",
        "    if match is None:\n",
        "        print('Invalid datafile line: %s' % data_file_line)\n",
        "        return (data_file_line, -1)\n",
        "    elif match.group(1) == '\"id\"':\n",
        "        print('Header datafile line: %s' % data_file_line)\n",
        "        return (data_file_line, 0)\n",
        "    else:\n",
        "        product = '%s %s %s' % (match.group(2), match.group(3), match.group(4))\n",
        "        return ((remove_quotes(match.group(1)), product), 1)"
      ],
      "metadata": {
        "id": "Si3rUMTAvBXq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = {\n",
        "  'Amazon':                 \"/content/mydrive/My Drive/CSCI-573/Amazon.csv\"\n",
        ", 'Google':                 \"/content/mydrive/My Drive/CSCI-573/Google.csv\"\n",
        ", 'gold_standard':          \"/content/mydrive/My Drive/CSCI-573/Amazon_Google_perfectMapping.csv\"\n",
        ", 'stopwords':              \"/content/mydrive/My Drive/CSCI-573/stopwords.txt\"\n",
        "}"
      ],
      "metadata": {
        "id": "B9Nbpl-Puin4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data(filename):\n",
        "    return sc.textFile(filename, 4, use_unicode=True).map(parse_data_file_line)"
      ],
      "metadata": {
        "id": "SL7472ndu6-W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filekey):\n",
        "    raw = parse_data(files[filekey]).cache()\n",
        "    failed = raw.filter(lambda s: s[1] == -1).map(lambda s: s[0])\n",
        "    for line in failed.take(10):\n",
        "        print('%s - Invalid datafile line: %s' % (files[filekey], line))\n",
        "    valid = raw.filter(lambda s: s[1] == 1).map(lambda s: s[0]).cache()\n",
        "    print('%s - Read %d lines, successfully parsed %d lines, failed to parse %d lines'\n",
        "        % (files[filekey], raw.count(), valid.count(), failed.count()))\n",
        "    assert(failed.count() == 0)\n",
        "    assert(raw.count() == (valid.count() + 1))\n",
        "    return valid"
      ],
      "metadata": {
        "id": "OMlDAbkSu0fC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "google = load_data('Google')\n",
        "amazon = load_data('Amazon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSE76jGguv02",
        "outputId": "f687fb4e-e423-42a2-d7d8-0d5fe6d85312"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mydrive/My Drive/CSCI-573/Google.csv - Read 3227 lines, successfully parsed 3226 lines, failed to parse 0 lines\n",
            "/content/mydrive/My Drive/CSCI-573/Amazon.csv - Read 1364 lines, successfully parsed 1363 lines, failed to parse 0 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_regex = r'\\W+'\n",
        "stopwords = set(sc.textFile(files['stopwords'], 1, use_unicode=True).collect())\n",
        "\n",
        "def tokenize(string):\n",
        "    return [w for w in re.split(split_regex, string.lower())\n",
        "               if w != '' and w not in stopwords]"
      ],
      "metadata": {
        "id": "DZCGyFj8wUh3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (4a) Tokenize the full dataset\n",
        "amazon_full_rec_to_token = amazon.map(lambda line: (line[0], tokenize(line[1])))\n",
        "google_full_rec_to_token = google.map(lambda line: (line[0], tokenize(line[1])))\n",
        "print('Amazon full dataset is %s products, Google full dataset is %s products'\n",
        "    % (amazon_full_rec_to_token.count(), google_full_rec_to_token.count()))\n",
        "# Amazon full dataset is 1363 products, Google full dataset is 3226 products"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKkxKDcft8gK",
        "outputId": "7736d96c-d81c-41ff-aba7-4b00cac6c884"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon full dataset is 1363 products, Google full dataset is 3226 products\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def idfs(corpus):\n",
        "    N = corpus.count()\n",
        "    unique_tokens = corpus.map(lambda x: {t for t in x[1]})\n",
        "    token_count_pair_tuple = unique_tokens.flatMap(lambda s: [(t, 1) for t in s])\n",
        "    token_sum_pair_tuple = token_count_pair_tuple.reduceByKey(lambda total, count: total + count);\n",
        "    return token_sum_pair_tuple.map(lambda x: (x[0], N / x[1]))"
      ],
      "metadata": {
        "id": "tKuHRx1zyBAN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf(tokens):\n",
        "    TF = dict()\n",
        "    for token in tokens:\n",
        "        if token not in TF:\n",
        "            TF[token] = 1\n",
        "        else: TF[token] += 1\n",
        "    for token in TF:\n",
        "        TF[token] = TF[token] / len(tokens)\n",
        "    return TF"
      ],
      "metadata": {
        "id": "67Vk9Y6mzWTU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf(tokens, idfs):\n",
        "    tfs = tf(tokens)\n",
        "    tf_idf_dict = {t: TF * idfs[t] for t, TF in tfs.items()}\n",
        "    return tf_idf_dict"
      ],
      "metadata": {
        "id": "_jwVmo4czEi2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (4b) Compute IDFs and TF-IDFs for the full datasets\n",
        "RDD_full_corpus = amazon_full_rec_to_token.union(google_full_rec_to_token)\n",
        "idfs_full = idfs(RDD_full_corpus)\n",
        "idfs_full_count = idfs_full.count()\n",
        "print('There are %s unique tokens in the full datasets.' % idfs_full_count)\n",
        "# There are 17078 unique tokens in the full datasets.\n",
        "# Recompute IDFs for full dataset\n",
        "idfs_full_weights = idfs_full.collectAsMap()\n",
        "idfs_full_broadcast = sc.broadcast(idfs_full_weights)\n",
        "# Pre-compute TF-IDF weights. Build mappings from record ID weight vector.\n",
        "RDD_amazon_weights = amazon_full_rec_to_token.map(lambda x: (x[0], tfidf(x[1], idfs_full_weights)))\n",
        "RDD_google_weights = google_full_rec_to_token.map(lambda x: (x[0], tfidf(x[1], idfs_full_weights)))\n",
        "print('There are %s Amazon weights and %s Google weights.'\n",
        "      % (RDD_amazon_weights.count(), RDD_google_weights.count()))\n",
        "# There are 1363 Amazon weights and 3226 Google weights."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqbTbOjEw7Yb",
        "outputId": "a3e6af29-6e1c-46cd-84fe-2ca639f0a0ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 17078 unique tokens in the full datasets.\n",
            "There are 1363 Amazon weights and 3226 Google weights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def dotprod(a, b):\n",
        "    sum = 0.0\n",
        "    for t, at in a.items():\n",
        "        if t in b:\n",
        "            sum += at * b[t]\n",
        "    return sum\n",
        "def norm(a):\n",
        "    sum = 0.0\n",
        "    for t, at in a.items():\n",
        "        sum += at * at\n",
        "    return math.sqrt(sum)\n",
        "def cossim(a, b):\n",
        "    return dotprod(a, b) / (norm(a) * norm(b))"
      ],
      "metadata": {
        "id": "HjQy1X8t6Ht6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(string1, string2, idfs_dictionary):\n",
        "    w1 = tfidf(tokenize(string1), idfs_dictionary)\n",
        "    w2 = tfidf(tokenize(string2), idfs_dictionary)\n",
        "    return cossim(w1, w2)"
      ],
      "metadata": {
        "id": "QrdPbP_hRvE9"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (4c) Compute Norms for the weights from the full datasets\n",
        "amazon_norms = RDD_amazon_weights.map(lambda x: (x[0], norm(x[1]))).collectAsMap()\n",
        "amazon_norms_broadcast = sc.broadcast(amazon_norms)\n",
        "google_norms = RDD_google_weights.map(lambda x: (x[0], norm(x[1]))).collectAsMap()\n",
        "google_norms_broadcast = sc.broadcast(google_norms)\n",
        "print(len(amazon_norms_broadcast.value))\n",
        "#1363\n",
        "print(len(google_norms_broadcast.value))\n",
        "#3226"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXTcdHH45DO_",
        "outputId": "c2e62b6b-6839-4c33-8cf7-c7a63f7f0a4c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1363\n",
            "3226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4d) Create inverted indicies from the full datasets\n",
        "def invert(record):\n",
        "    return [(k, record[0]) for k in record[1].keys()]\n",
        "print(invert((1, {'foo': 2, 'bar': 3})))\n",
        "#[('foo', 1), ('bar', 1)]\n",
        "RDD_amazon_inv_pairs = RDD_amazon_weights.flatMap(lambda x: invert(x)).cache()\n",
        "RDD_google_inv_pairs = RDD_google_weights.flatMap(lambda x: invert(x)).cache()\n",
        "print('There are %s Amazon inverted pairs and %s Google inverted pairs.'\n",
        "     % (RDD_amazon_inv_pairs.count(), RDD_google_inv_pairs.count()))\n",
        "#There are 111387 Amazon inverted pairs and 77678 Google inverted pairs."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwHVz6qW9qdX",
        "outputId": "427a3c3a-69a3-4ec7-a676-5ff93bee4550"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('foo', 1), ('bar', 1)]\n",
            "There are 111387 Amazon inverted pairs and 77678 Google inverted pairs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4e) Identify common tokens from the full dataset\n",
        "from pyspark.rdd import portable_hash\n",
        "def swap(record): return (record[1], record[0])\n",
        "common_tokens = ( RDD_amazon_inv_pairs #.partitionBy(64, lambda k: portable_hash(k[0]))\n",
        "                  .join(RDD_google_inv_pairs) #.partitionBy(64, lambda k: portable_hash(k[0]))\n",
        "                  .map(lambda x: swap(x))\n",
        "                  .groupByKey()\n",
        "                  .cache()\n",
        "                )\n",
        "print('Found %d common tokens' % common_tokens.count())\n",
        "#Found 2441100 common tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR8zwGdoA1YI",
        "outputId": "52acb585-ac78-4392-df50-ac593a218f31"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2441100 common tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (4f) Identify common tokens from the full dataset (cont.)\n",
        "amazon_weights_broadcast = sc.broadcast(RDD_amazon_weights.collectAsMap())\n",
        "google_weights_broadcast = sc.broadcast(RDD_google_weights.collectAsMap())\n",
        "def fast_cosine_similarity(record):\n",
        "    amazon_rec = record[0][0]\n",
        "    google_rec = record[0][1]\n",
        "    tokens = record[1]\n",
        "    value = 0.0\n",
        "    for token in tokens:\n",
        "        value +=  ( amazon_weights_broadcast.value[amazon_rec][token]\n",
        "                  * google_weights_broadcast.value[google_rec][token]\n",
        "                  )\n",
        "    value /= amazon_norms_broadcast.value[amazon_rec]\n",
        "    value /= google_norms_broadcast.value[google_rec]\n",
        "    key = (amazon_rec, google_rec)\n",
        "    return (key, value)\n",
        "RDD_similarities_full = common_tokens.map(lambda x: fast_cosine_similarity(x)).cache()\n",
        "pprint(RDD_similarities_full.count())\n",
        "#2441100\n",
        "similarity_test = RDD_similarities_full.filter(\n",
        "    lambda x: x[0][0] == 'b00005lzly'\n",
        "          and x[0][1] == 'http://www.google.com/base/feeds/snippets/13823221823254120257'\n",
        ").collect()\n",
        "pprint(similarity_test)\n",
        "#[(('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), 4.286548413995203e-06)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEoS3kZxKfvN",
        "outputId": "4d2c2eb9-66dc-49a7-dd2c-1dfad2893443"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2441100\n",
            "[(('b00005lzly',\n",
            "   'http://www.google.com/base/feeds/snippets/13823221823254120257'),\n",
            "  4.2865484139952024e-06)]\n"
          ]
        }
      ]
    }
  ]
}