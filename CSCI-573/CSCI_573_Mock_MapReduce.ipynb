{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MapReduce: Simplified Data Processing on Large Clusters by Jeffrey Dean and Sanjay Ghemawat at Google, Inc.\n",
        "\n",
        "Interactive Article - Mock simulation of MapReduce for interactive demonstration\n",
        "* Section 2.1 Example\n",
        "* Section 3.1 Execution Overview."
      ],
      "metadata": {
        "id": "TiGzxaKV0MAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Q2.1: Example\n",
        "# Q2.1.1(a): Consider the problem of counting the number of occurrences of each word in a large collection of documents.\n",
        "from pprint import pprint\n",
        "re_word = re.compile(r'\\b[A-Za-z][a-z]*\\b')\n",
        "documents = [\n",
        "  \"Eagles eat snakes, lizards, and insects.\" #1\n",
        ", \"Snakes eat lizards and insects.\" #2\n",
        ", \"Snakes eat frogs and insects.\" #3\n",
        ", \"Snakes eat fish and insects.\" #4\n",
        ", \"Frogs eat lizards, fish and insects.\" #5\n",
        ", \"Lizards eat insects.\" #6\n",
        ", \"Fish eat insects.\" #7\n",
        ", \"Insects eat insects.\" #8\n",
        "]\n",
        "# Q2.1.1(b): The user would write code similar to the following pseudo-code:\n",
        "# Q2.1.2(a): The map function emits each word plus an associated count of occurrences (just ‘1’ in this simple example).\n",
        "def Map(doc_id, sentence):\n",
        "    return [(word.lower(), 1) for word in re.findall(re_word, sentence)]\n",
        "def Reduce(word, count_list): # Q2.1.2(b): The reduce function sums together all counts emitted for a particular word.\n",
        "    total = 0\n",
        "    for count in count_list:\n",
        "        total += count\n",
        "    return total\n",
        "# Q2.1.2(c): In addition, the user writes code to fill in a mapreduce specification object ...\n",
        "# Q2.1.2(c): ... with the names of the input and output files, and optional tuning parameters.\n",
        "# Q2.1.2(d): The user then invokes the MapReduce function, passing it the specification object.\n",
        "# Q2.1.2(e): The user’s code is linked together with the MapReduce library (implemented in C++)."
      ],
      "metadata": {
        "id": "XV9Vy4XtW9Bu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input data is split into M pieces for parallel \"Map\" processing of M tasks on N machines. The intermediate data is divided into R segments using a Partition function for parallel \"Reduce\" processing of R tasks on N machines. The default Partition function is hash(key) modulo R.\n",
        "\n",
        "Guidelines for readbaility:\n",
        "* **N**umber of machines, N, n\n",
        "* **M**ap tasks, M, m\n",
        "* **R**educe tasks, R, r\n",
        "* **key1**, **value1** versus k1, v1 in the article\n",
        "* **key2**, **value2** versus k2, v2 in the article\n",
        "\n",
        "Components provided by the user\n",
        "* map(k1, v1) → list(k2, v2)\n",
        "* reduce(k2, list(v2)) → aggregate(v2)\n",
        "* partition(k2) → hash(k2) mod R"
      ],
      "metadata": {
        "id": "sZCOyCWRykAN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E6KLMHEWcsni"
      },
      "outputs": [],
      "source": [
        "M = 4 # Q3.1(a): The Map invocations are distributed across multiple machines ...\n",
        "      # Q3.1(a): ... by automatically partitioning the input data into a set of M splits.\n",
        "N = 2 # Q3.1(b): The input splits can be processed in parallel by different machines.\n",
        "R = 13 # Q3.1(c): Reduce invocations are distributed by partitioning the intermediate key space into R pieces ...\n",
        "       # Q3.1(c): ... using a partitioning function (e.g., hash(key) mod R).\n",
        "def Initialize(): # Initialize N machines with R intermediate files\n",
        "    global slice_size, machine\n",
        "    slice_size = len(documents) // M\n",
        "    machine = [None] * N\n",
        "    for n in range(N):\n",
        "        machine[n] = [None] * R\n",
        "        for r in range(R):\n",
        "            machine[n][r] = list()\n",
        "# Q3.1(d): The number of partitions (R) and the partitioning function are specified by the user.\n",
        "def Hash(word): return ord(word[0]) - ord('a')\n",
        "# Q3.1.1(a): The MapReduce library in the user program first splits the input files into M pieces ...\n",
        "# Q3.1.1(a): ... of typically 16 megabytes to 64 megabytes (MB) per piece ...\n",
        "# Q3.1.1(a): ... (controllable by the user via an optional parameter).\n",
        "# Q3.1.1(b): It then starts up many copies of the program on a cluster of machines.\n",
        "def Partition(word): return Hash(word) % R"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Map tasks. Each Map worker writes to R partitioned files on that machine\n",
        "machine = None\n",
        "slice_size = None\n",
        "Initialize() # Q3.1.2(a): One of the copies of the program is special - the master.\n",
        "# Q3.1.2(b): The rest are workers that are assigned work by the master.\n",
        "# Q3.1.2(c): There are M map tasks and R reduce tasks to assign.\n",
        "for m in range(M): # mock running Map tasks in parallel on N machines\n",
        "    n = m % N # Q3.1.2(d): The master picks idle workers and assigns each one a map task or a reduce task.\n",
        "    # Q3.1.3(a): A worker who is assigned a map task reads the contents of the corresponding input split.\n",
        "    slice_begin = m * slice_size\n",
        "    slice_end = slice_begin + slice_size\n",
        "    for doc_id in range(slice_begin, slice_end):\n",
        "    # Q3.1.3(b): It parses key/value pairs out of the input data and passes each pair to the user-defined Map function.\n",
        "        key1 = doc_id\n",
        "        value1 = documents[doc_id-1]\n",
        "        for (key2, value2) in Map(key1, value1):\n",
        "             # Q3.1.3(c): The intermediate key/value pairs produced by the Map function are buffered in memory.\n",
        "            machine[n][Partition(key2)].append((key2, value2))\n",
        "            # Q3.1.4(a): Periodically, the buffered pairs are written to local disk, ...\n",
        "            # Q3.1.4(a): ... partitioned into R regions by the partitioning function.\n",
        "            # Q3.1.4(b): The locations of these buffered pairs on the local disk are passed back to the master, ...\n",
        "            # Q3.1.4(b): ... who is responsible for forwarding these locations to the reduce workers."
      ],
      "metadata": {
        "id": "CtaZmjwNfNLF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show intermediate file contents\n",
        "pprint(machine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2QEoJTMqzpn",
        "outputId": "f7f698dd-c785-46fb-c17c-669d5138cca1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[('and', 1), ('and', 1), ('and', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [],\n",
            "  [('eat', 1), ('eagles', 1), ('eat', 1), ('eat', 1), ('eat', 1)],\n",
            "  [('snakes', 1), ('snakes', 1), ('fish', 1), ('frogs', 1), ('fish', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [('insects', 1),\n",
            "   ('insects', 1),\n",
            "   ('insects', 1),\n",
            "   ('insects', 1),\n",
            "   ('insects', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [('lizards', 1), ('lizards', 1)],\n",
            "  []],\n",
            " [[('and', 1), ('and', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [],\n",
            "  [('eat', 1), ('eat', 1), ('eat', 1), ('eat', 1)],\n",
            "  [('snakes', 1), ('snakes', 1), ('frogs', 1), ('fish', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [('insects', 1), ('insects', 1), ('insects', 1), ('insects', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [('lizards', 1), ('lizards', 1)],\n",
            "  []]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Reduce tasks.\n",
        "output = [dict() for r in range(R)]\n",
        "for r in range(R): # mock executing Reduce tasks in parallel on N machines\n",
        "    memory = list() # mock gathering data in local memory\n",
        "    this_machine = r % N # mock scheduling task on selected machine\n",
        "    for n in range(N): # mock for all reported ready partitions\n",
        "        # Q3.1.5(a): When a reduce worker is notified by the master about these locations, ...\n",
        "        # Q3.1.5(a): ... it uses remote procedure calls to read the buffered data ...\n",
        "        # Q3.1.5(a): ... from the local disks of the map workers.\n",
        "        if n != this_machine:\n",
        "            partition = (globals()['machine'][n])[r] # mock remote machine access\n",
        "        else: partition = machine[n][r] # mock local machine access\n",
        "        for (key2, value2) in partition:\n",
        "            memory.append((key2, value2))\n",
        "    # Q3.1.5(b): When a reduce worker has read all intermediate data, ...\n",
        "    # Q3.1.5(b): ... it sorts it by the intermediate keys ...\n",
        "    # Q3.1.5(b): ... so that all occurrences of the same key are grouped together.\n",
        "    sorted_memory = dict() # mock sorting local memory\n",
        "    for (key2, value2) in memory:\n",
        "        if key2 not in sorted_memory:\n",
        "            sorted_memory[key2] = [value2]\n",
        "        else: sorted_memory[key2].append(value2)\n",
        "    # Q3.1.6(a): The reduce worker iterates over the sorted intermediate data ...\n",
        "    # Q3.1.6(a): ... and for each unique intermediate key encountered, ...\n",
        "    # Q3.1.6(a): ... it passes the key and the corresponding set of intermediate values to the user’s Reduce function.\n",
        "    # Q3.1.6(b): The output of the Reduce function is appended to a final output file for this reduce partition.\n",
        "    for (key2, list_value2) in sorted_memory.items():\n",
        "        output[r][key2] = Reduce(key2, list_value2) # notice the list passed to Reduce has only values (no keys)\n",
        "    # Q3.1.5(c): The sorting is needed because typically many different keys map to the same reduce task.\n",
        "    # Q3.1.5(d): If the amount of intermediate data is too large to fit in memory, an external sort is used.\n",
        "# Q3.1.7(a): When all map tasks and reduce tasks have been completed, the master wakes up the user program.\n",
        "# Q3.1.7(b): At this point, the MapReduce call in the user program returns back to the user code."
      ],
      "metadata": {
        "id": "7beim1Cf8MbT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3.1.8(a): After successful completion, the output of the mapreduce execution is available in the R output files ...\n",
        "# Q3.1.8(a): ... (one per reduce task, with file names as specified by the user).\n",
        "# Q3.1.8(b): Typically, users do not need to combine these R output files into one file - ...\n",
        "# Q3.1.8(b): ... they often pass these files as input to another MapReduce call, ...\n",
        "# Q3.1.8(b): ... or use them from another distributed application that is able to deal with input ...\n",
        "# Q3.1.8(b): ... that is partitioned into multiple files.\n",
        "pprint(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNyWe9WzoRDi",
        "outputId": "b1509c59-57b5-4159-8067-506bba273dff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'and': 5},\n",
            " {},\n",
            " {},\n",
            " {},\n",
            " {'eagles': 1, 'eat': 8},\n",
            " {'fish': 3, 'frogs': 2, 'snakes': 4},\n",
            " {},\n",
            " {},\n",
            " {'insects': 9},\n",
            " {},\n",
            " {},\n",
            " {'lizards': 4},\n",
            " {}]\n"
          ]
        }
      ]
    }
  ]
}