{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Interactive Article, Section 3.1 Execution Overview.\n",
        "MapReduce: Simplified Data Processing on Large Clusters by Jeffrey Dean and Sanjay Ghemawat at Google, Inc."
      ],
      "metadata": {
        "id": "TiGzxaKV0MAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mock MapReduce, mock simulation of MapReduce for interactive demonstration\n",
        "import re\n",
        "from pprint import pprint\n",
        "re_word = re.compile(r'[A-Za-z][a-z]*')\n",
        "documents = [\n",
        "  \"Eagles eat snakes, lizards, and insects.\" #1\n",
        ", \"Snakes eat lizards and insects.\" #2\n",
        ", \"Snakes eat frogs and insects.\" #3\n",
        ", \"Snakes eat fish and insects.\" #4\n",
        ", \"Frogs eat lizards, fish and insects.\" #5\n",
        ", \"Lizards eat insects.\" #6\n",
        ", \"Fish eat insects.\" #7\n",
        ", \"Insects eat insects.\" #8\n",
        "]"
      ],
      "metadata": {
        "id": "A389bRhBeT26"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input data is split into M pieces for parallel Map processing. The intermediate data is divided into R segments using a Partition function for parallel Reduce processing."
      ],
      "metadata": {
        "id": "sZCOyCWRykAN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E6KLMHEWcsni"
      },
      "outputs": [],
      "source": [
        "# Components provided by the user\n",
        "#   map(k1, v1) => list(k2, v2)\n",
        "#   reduce(k2, list(v2)) => aggregate(v2)\n",
        "#   partition(k2) => hash(k2) mod R\n",
        "X = 2 # multiplier, how many more machines than map tasks, typically 100\n",
        "N = 2 # machines, kept small for demonstration, typically 2_000\n",
        "M = N * X # number of Map tasks, typically 200_000\n",
        "R = 13 # number of Reduce tasks, typically 5_000\n",
        "def Hash(word): return ord(word[0]) - ord('a') # 26 unique values\n",
        "def Partition(word): return Hash(word) % R\n",
        "def Map(doc_id, sentence): return [(word.lower(), 1) for word in re.findall(re_word, sentence)]\n",
        "def Reduce(word, count_list):\n",
        "    total = 0\n",
        "    for count in count_list:\n",
        "        total += count\n",
        "    return total"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize N machines with R intermediate files\n",
        "def Initialize():\n",
        "    global slice_size, machine\n",
        "    slice_size = len(documents) // M\n",
        "    machine = [None] * N\n",
        "    for n in range(N):\n",
        "        machine[n] = [None] * R\n",
        "        for r in range(R):\n",
        "            machine[n][r] = list()"
      ],
      "metadata": {
        "id": "qRAsevfZVcGh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Map tasks. Each Map worker writes to R partitioned files on that machine\n",
        "machine = None\n",
        "slice_size = None\n",
        "Initialize()\n",
        "for m in range(M): # mock execute all Map tasks in parallel on N machines\n",
        "    n = m % N # mock schedule task on selected machine\n",
        "    slice_begin = m * slice_size # select proper slice of input\n",
        "    slice_end = slice_begin + slice_size\n",
        "    for doc_id in range(slice_begin, slice_end):\n",
        "        key1 = doc_id\n",
        "        value1 = documents[doc_id-1]\n",
        "        key_values = Map(key1, value1) # execute Map function on machine\n",
        "        for (key2, value2) in key_values:\n",
        "            machine[n][Partition(key2)].append((key2, value2))"
      ],
      "metadata": {
        "id": "CtaZmjwNfNLF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show intermediate file contents\n",
        "pprint(machine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2QEoJTMqzpn",
        "outputId": "827bd00d-ae91-44d5-b121-33470521ac6b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[('and', 1), ('and', 1), ('and', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [],\n",
            "  [('eat', 1), ('eagles', 1), ('eat', 1), ('eat', 1), ('eat', 1)],\n",
            "  [('snakes', 1), ('snakes', 1), ('fish', 1), ('frogs', 1), ('fish', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [('insects', 1),\n",
            "   ('insects', 1),\n",
            "   ('insects', 1),\n",
            "   ('insects', 1),\n",
            "   ('insects', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [('lizards', 1), ('lizards', 1)],\n",
            "  []],\n",
            " [[('and', 1), ('and', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [],\n",
            "  [('eat', 1), ('eat', 1), ('eat', 1), ('eat', 1)],\n",
            "  [('snakes', 1), ('snakes', 1), ('frogs', 1), ('fish', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [('insects', 1), ('insects', 1), ('insects', 1), ('insects', 1)],\n",
            "  [],\n",
            "  [],\n",
            "  [('lizards', 1), ('lizards', 1)],\n",
            "  []]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Reduce tasks.\n",
        "results = dict()\n",
        "for r in range(R): # mock execute Reduce tasks in parallel on N machines\n",
        "    memory = dict() # mock sorted local memory\n",
        "    this_machine = r % N # mock scheduling task on selected machine\n",
        "    for n in range(N): # mock for all reported ready partitions\n",
        "        if n != this_machine:\n",
        "            partition = (globals()['machine'][n])[r] # mock remote machine access\n",
        "        else: partition = machine[n][r] # mock local machine access\n",
        "        for (key2, value2) in partition:\n",
        "            if key2 not in memory:\n",
        "                memory[key2] = list()\n",
        "            memory[key2].append(value2)\n",
        "    for (key2, list_value2) in memory.items():\n",
        "        results[key2] = Reduce(key2, list_value2)"
      ],
      "metadata": {
        "id": "7beim1Cf8MbT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNyWe9WzoRDi",
        "outputId": "6802e2eb-efcd-4e15-e88a-f109d1b6cee3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'and': 5,\n",
            " 'eagles': 1,\n",
            " 'eat': 8,\n",
            " 'fish': 3,\n",
            " 'frogs': 2,\n",
            " 'insects': 9,\n",
            " 'lizards': 4,\n",
            " 'snakes': 4}\n"
          ]
        }
      ]
    }
  ]
}