-,Abstract,-
1,Introduction,"challenges of processing vast data sets, motivates a new abstraction for scalable, distributed computing"
2,Programming Model,a simple programming model built on user-defined map and reduce functions to express parallel computations intuitively
2.0.1,Map function,"map(k1, v1) => list(k2, v2), the function that processes individual key/value pairs to generate intermediate key/value pairs"
2.0.2,Reduce function,"reduce(k2, list(v2)) => list(v2), the function that aggregates all intermediate values sharing the same key into a condensed output"
2.1,Example (word count),a word count example that illustrates the transformation and aggregation steps using map and reduce
2.2,Types,various computational tasks can be framed into the map and reduce paradigm
2.3,More Examples,-
2.3.1,Distributed Grep,a distributed text search across massive datasets
2.3.2,Count of URL Access Frequency,"summarizing and counting URL accesses, useful for traffic analysis"
2.3.3,Reverse Web-link Graph,build graphs of referencing URLs
2.3.4,Term-Vector Per Host,generating term vectors for each host to support content analysis
2.3.5,Inverted Index,constructing an inverted index for efficient retrieval in text search applications
2.3.6,Distributed Sort,sorting of large datasets in a distributed environment
3,Implementation,architectural components and interactions that enable execution of jobs on clusters
3.0.1,user program,the client code containing user-defined map and reduce routines that drive the computation
3.0.2,master process,the role of the central coordinator that schedules tasks and manages the overall operation
3.0.3,worker process,the nodes that execute individual map and reduce tasks as assigned by the master
3.0.4,input files (splits),input data is partitioned into manageable splits to facilitate parallel processing
3.0.5,itermediate files (local disks),temporary results are stored locally on each node before being shuffled for reduction
3.0.6,output files,the final computation results are written to output files after reduction
3.1,Execution Overview,"the data flow from input splitting through mapping, shuffling, and reducing"
3.1.1,M splits of the input,the initial division of input data into M splits for concurrent processing by map tasks
3.1.2,M map tasks,the M concurrent execution of map tasks across the input splits
3.1.3,R reduce tasks,the R reduce tasks work on groups of intermediate data to produce the final results
3.1.4,R regions (partitioning function),the partitioning function assigns intermediate data to R regions corresponding to reduce tasks
3.1.5,R output files,each reduce task produces a distinct output file forming part of the final output
3.2,Master Data Structures,the internal data structures used by the master to manage the state of tasks during execution
3.2.1,idle task state,tasks that have not yet been assigned to any worker and are waiting in the queue
3.2.2,in-progress task state,tasks that are currently being executed by worker processes
3.2.3,completed task state,tasks that have successfully finished execution and contributed to the final output
3.3,Fault Tolerance,design strategies allow handling of failures gracefully on large clusters
3.3.1,Worker Failure,the system detects worker failures and reallocates their tasks to ensure completion
3.3.2,Master Failure,aborts the computation if the master fails
3.3.3,Semantics in the Presence of Failures,"guarantees preserve computation correctness despite failures, and better when map and reduce are deterministic functions"
3.4,Locality,optimizing performance by processing data on or near its storage location to reduce network overhead
3.5,Task Granularity,partition work into fine-grained tasks to facilitate load balancing and scalability
3.5.1,subdivisions,M and R should be much larger than the number of worker machines
3.5.2,scheduling decisions,"O(M + R), task scheduling is influenced by data locality and resource availability"
3.5.3,memory states,O(M * R) intermediate data is managed in memory during the map and reduce operations
3.5.4,typical configuration,"M=200_000, R=5_000, and 2_000 machines"
3.6,Backup Tasks,straggler tasks
4,Refinements,various enhancements and optional features
4.1,Partitioning Function,strategies to partition intermediate data efficiently for balanced load distribution
4.1.1,default,"the built-in, default partitioning function: hash(key) mod R"
4.1.2,provided,"supply custom partitioning functions, for example: hash(Hostname(urlkey)) mod R"
4.2,Ordering Guarantees,the model ensures an ordered output where required by specific applications
4.3,Combiner Function,a mechanism for partial local aggregation that reduces the volume of data transferred across the network
4.4,Input/Output Types,the support for various data type formats for both input and output streams
4.4.1,built-in,text
4.4.2,built-in,key-value
4.4.3,customized,custom input/output types to handle specialized data formats using a reader intrerface
4.5,Side Effects,the potential and management of side effects
4.6,Skipping Bad Records,detects and skips over corrupt or unreadable input records during processing
4.7,Local Execution,running jobs locally for debugging and testing purposes
4.8,Status Information,the framework continuously provides status updates and progress information during execution
4.9,Counters,"built-in counters that help monitor various metrics such as progress, successes, and failures during a job"
5,Performance,benchmarks and performance measurements demonstrating the scalability and efficiency
5.1,Cluster Configuration,the hardware and network settings used in performance tests to illustrate the modelâ€™s scalability
5.2,Grep,the performance of running distributed text search tasks
5.3,Sort,the efficiency and speed of distributed sorting tasks
5.4,Effect of Backup Tasks,the improvement in job completion times when backup tasks are employed to mitigate slow workers
5.5,Machine Failures,minimizes performance penalties even when nodes fail during processing
6,Experience,practical insights and lessons learned from deploying at Google for large-scale computations
6.0.1,large-scale machine learning problems,has been utilized to solve extensive machine learning challenges at scale
6.0.2,clustering problems,clustering tasks that enhance Google News and Froogle product features
6.0.3,extraction of data from popular queries,the extraction processes generate reports on frequently issued queries like Google Zeitgeist
6.0.4,large-scale graph computations,the model supports the processing of complex graph structures on massive datasets
6.1,Large Scale Indexing,building and updating large-scale web indices efficiently
7,Related Work,other distributed frameworks and research efforts that address similar challenges in large-scale data processing
8,Conclusion,"simplicity, scalability, and effectiveness"
-,Acknowledgements,-
-,References,-