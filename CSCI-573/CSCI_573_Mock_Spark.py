#===============================================================================
class RDD: # §2: Resilient Distributed Datasets (RDDs)
# §2(i): This section provides an overview of RDDs.
    def __init__(): # §2(ii): We first define RDDs (§2.1) ...
# §2(ii): ... and introduce their programming interface in Spark (§2.2).
# §2(iii): We then compare RDDs ...
# §2(iii): ... with finer-grained shared memory abstractions (§2.3).
# §2(iv): Finally, we discuss limitations of the RDD model (§2.4).
Spark = None
#-------------------------------------------------------------------------------
# §2.1: RDD Abstraction
# §2.1(i): Formally, an RDD is a read-only, partitioned collection of records.
# §2.1(ii): RDDs can only be created through deterministic operations on ...
# §2.1(ii): ... either (1) data in stable storage ...
# §2.1(ii): ... or (2) other RDDs.
# §2.1(iii): We call these operations transformations ...
# §2.1(iii): ... to differentiate them from other operations on RDDs.
# §2.1(iv): Examples of transformations include map, filter, and join.
# §2.1(v): RDDs do not need to be materialized at all times.
# §2.1(vi): Instead, an RDD has enough information about how it was derived ...
# §2.1(vi): ... from other datasets (its lineage) to compute its partitions ...
# §2.1(vi): ... from data in stable storage.
# §2.1(vii): This is a powerful property: in essence, a program cannot ...
# §2.1(vii): ... reference an RDD that it cannot reconstruct after a failure.
# §2.1(viii): Finally, users can control two other aspects of RDDs: ...
# §2.1(viii): ... persistence and partitioning.
# §2.1(ix): Users can indicate which RDDs they will reuse and ...
# §2.1(ix): ... choose a storage strategy for them (e.g., in-memory storage).
# §2.1(x): They can also ask that an RDD’s elements be partitioned ...
# §2.1(x): ... across machines based on a key in each record.
# §2.1(xi): This is useful for placement optimizations, such as ...
# §2.1(xi): ... ensuring that two datasets that will be joined together ...
# §2.1(xi): ... are hash-partitioned in the same way.
records = None
machines = None
partitions = None
#===============================================================================
# §2.2: Spark Programming Interface
#-------------------------------------------------------------------------------
# §2.2.0.1(i): Spark exposes RDDs through a language-integrated API similar ...
# §2.2.0.1(i): ... to DryadLINQ [31] and FlumeJava [8], ...
# §2.2.0.1(i): ... where each dataset is represented as an object and ...
# §2.2.0.1(i): ... transformations are invoked using methods on these objects.
#-------------------------------------------------------------------------------
# §2.2.0.2(i): Programmers start by defining one or more RDDs ...
# §2.2.0.2(i): ... through transformations on data ...
# §2.2.0.2(i): ... in stable storage (e.g., map and filter).
# §2.2.0.2(ii): They can then use these RDDs in actions, which are ...
# §2.2.0.2(ii): ... operations that return a value to the application or ...
# §2.2.0.2(ii): ... export data to a storage system.
# §2.2.0.2(iii): Examples of actions include ...
count = None   # §2.2.0.2(iii): ... count (which returns the number ...
               # §2.2.0.2(iii): ... of elements in the dataset), ...
collect = None # §2.2.0.2(iii): ... collect (which returns the elements themselves), and ...
save = None    # §2.2.0.2(iii): ... save (which outputs the dataset to a storage system).
# §2.2.0.2(iv): Like DryadLINQ, Spark computes RDDs lazily the first time ...
# §2.2.0.2(iv): ... they are used in an action, so that it can pipeline ...
# §2.2.0.2(iv): ... transformations.
transformations = None
actions = None
#-------------------------------------------------------------------------------
# §2.2.0.3(i): In addition, programmers can call a persist method to ...
# §2.2.0.3(i): ... indicate which RDDs they want to reuse in future operations.
# §2.2.0.3(ii): Spark keeps persistent RDDs in memory by default, but it can ...
# §2.2.0.3(ii): ... spill them to disk if there is not enough RAM.
# §2.2.0.3(iii): Users can also request other persistence strategies, ...
# §2.2.0.3(iii): ... such as storing the RDD only on disk or ...
# §2.2.0.3(iii): ... replicating it across machines, through flags to persist.
# §2.2.0.3(iv): Finally, users can set a persistence priority on each RDD ...
# §2.2.0.3(iv): ... to specify which in-memory data should spill to disk first.
persistence_priority = None
in_memory = None
#===============================================================================
# §2.2.1: Example: Console Log Mining
#-------------------------------------------------------------------------------
# §2.2.1.1(i): Suppose that a web service is experiencing errors and an ...
# §2.2.1.1(i): ... operator wants to search terabytes of logs ...
# §2.2.1.1(i): ... in the Hadoop filesystem (HDFS) to find the cause.
# §2.2.1.1(ii): Using Spark, the operator can load just the error messages ...
# §2.2.1.1(ii): ... from the logs into RAM across a set of nodes and ...
# §2.2.1.1(ii): ... query them interactively.
# §2.2.1.1(iii): She would first type the following Scala code:
lines = spark.textFile("hdfs://...") # §2.2.1.1(iv): Line 1 defines an RDD ...
# §2.2.1.1(iv): ... backed by an HDFS file (as a collection of lines of ...
# §2.2.1.1(iv): ... text), while line 2 derives a filtered RDD from it.
errors = lines.filter(lambda line: line.startsWith("ERROR"))
errors.persist() # §2.2.1.1(v): Line 3 then asks for errors to persist ...
                 # §2.2.1.1(v): ... in memory so that it can be shared ...
                 # §2.2.1.1(v): ... across queries.
# §2.2.1.1(vi): Note that the argument to filter is Scala syntax for a closure.
#-------------------------------------------------------------------------------
clusters = None # §2.2.1.2(i): At this point, no work has been performed on the cluster.
# §2.2.1.2(ii): However, the user can now use the RDD in actions, ...
errors.count() # §2.2.1.2(ii): ... e.g., to count the number of messages.
#-------------------------------------------------------------------------------
# §2.2.1.3(i): The user can also perform further transformations on the RDD ...
# §2.2.1.3(i): ... and use their results, as in the following lines ...
# Count errors mentioning MySQL:
errors.filter(lambda line: line.contains("MySQL")).count()
# Return the time fields of errors mentioning HDFS as an array
# (assuming time is field number 3 in a tab-separated format):
errors.filter(lambda line: line.contains("HDFS")) \
      .map(lambda line: line.split('\t')[3]) \
      .collect()
#-------------------------------------------------------------------------------
# §2.2.1.4(i): After the first action involving errors runs, Spark will ...
# §2.2.1.4(i): ... store the partitions of errors in memory, ...
# §2.2.1.4(i): ... greatly speeding up subsequent computations on it.
# §2.2.1.4(ii): Note that the base RDD, lines, is not loaded into RAM.
# §2.2.1.4(iii): This is desirable because the ...
# §2.2.1.4(iii): ... error messages might only be a small ...
# §2.2.1.4(iii): ... fraction of the data (small enough to fit into memory).
#===============================================================================
# §3: Spark Programming Interface
#-------------------------------------------------------------------------------
# §3.0.1(i): Spark provides the RDD abstraction through a language- ...
# §3.0.1(i): ... integrated API similar to DryadLINQ [31] in Scala [2], a ...
# §3.0.1(i): ... statically typed functional programming language ...
# §3.0.1(i): ... for the Java VM.
# §3.0.1(ii): We chose Scala due to its combination of ...
# §3.0.1(ii): ... conciseness (which is convenient for interactive use) ...
# §3.0.1(ii): ... and efficiency (due to static typing).
# §3.0.1(iii): However, nothing about the RDD abstraction ...
# §3.0.1(iii): ... requires a functional language.
Scala = None
#-------------------------------------------------------------------------------
# §3.0.2(i): To use Spark, developers write a driver program ...
# §3.0.2(i): ... that connects to a cluster of workers, as shown in Figure 2.
# §3.0.2(ii): The driver defines one or more RDDs and invokes actions on them.
# §3.0.2(iii): Spark code on the driver also tracks the RDDs lineage.
# §3.0.2(iv): The workers are long-lived processes ...
# §3.0.2(iv): ... that can store RDD partitions in RAM across operations.
driver = None
worker = None
#-------------------------------------------------------------------------------
# §3.0.3(i): As we showed in the log mining example in Section 2.2.1, ...
# §3.0.3(i): ... users provide arguments to RDD operations like map ...
# §3.0.3(i): ... by passing closures (function literals).
# §3.0.3(ii): Scala represents each closure as a Java object, and these ...
# §3.0.3(ii): ... objects can be serialized and loaded on another node ...
# §3.0.3(ii): ... to pass the closure across the network.
# §3.0.3(iii): Scala also saves any variables bound in the closure ...
# §3.0.3(iii): ... as fields in the Java object.
x = 5                     # §3.0.3(iv): For example, one can write code like ...
rdd.map(lambda i: i + x)  # §3.0.3(iv): ... to add 5 to each element of an RDD.
#-------------------------------------------------------------------------------
# §3.0.4(i): RDDs themselves are statically typed ...
# §3.0.4(i): ... objects parametrized by an element type.
# §3.0.4(ii): For example, RDD[Int] is an RDD of integers.
# §3.0.4(iii): However, most of our examples ...
# §3.0.4(iii): ... omit types since Scala supports type inference.
#-------------------------------------------------------------------------------
# §3.0.5(i): Although our method of exposing RDDs in Scala is ...
# §3.0.5(i): ... conceptually simple, we had to work around issues with ...
# §3.0.5(i): ... Scala’s closure objects using reflection [33].
# §3.0.5(ii): We also needed more work to make Spark usable from the Scala ...
# §3.0.5(ii): ... interpreter, as we shall discuss in Section 5.2.
# §3.0.5(iii): Nonetheless, we did not have to modify the Scala compiler.
#===============================================================================
# Table 2(i): Transformations and actions available on RDDs in Spark.
# Table 2(ii): Seq[T] denotes a sequence of elements of type T.
# Table 2.1: ----- Transformations: --------------------------------------------
# Table 2.1(i):    map(f: T => U):                              RDD[T] => RDD[U]
# Table 2.1(ii):   filter(f: T => Bool):                        RDD[T] => RDD[T]
# Table 2.1(iii):  flatMap(f: T => Seq[U]):                     RDD[T] => RDD[U]
# Table 2.1(iv):   sample(fraction: Float):                     RDD[T] => RDD[T]
# Table 2.1(iv):                                        (Deterministic sampling)
# Table 2.1(v):    groupByKey():                 RDD[(K, V)] => RDD[(K, Seq[V])]
# Table 2.1(vi):   reduceByKey(f: (V, V) => V):       RDD[(K, V)] => RDD[(K, V)]
# Table 2.1(vii):  union():                           (RDD[T], RDD[T]) => RDD[T]
# Table 2.1(viii): join():                            (RDD[(K, V)], RDD[(K, W)])
# Table 2.1(viii):                                           => RDD[(K, (V, W))]
# Table 2.1(ix):   cogroup():                         (RDD[(K, V)], RDD[(K, W)])
# Table 2.1(ix):                                   => RDD[(K, (Seq[V], Seq[W]))]
# Table 2.1(x):    crossProduct():               (RDD[T], RDD[U]) => RDD[(T, U)]
# Table 2.1(xi):   mapValues(f: V => W):              RDD[(K, V)] => RDD[(K, W)]
# Table 2.1(xi):                                        (Preserves partitioning)
# Table 2.1(xii):  sort(c: Comparator[K]):            RDD[(K, V)] => RDD[(K, V)]
# Table 2.1(xiii): partitionBy(p: Partitioner[K]):    RDD[(K, V)] => RDD[(K, V)]
# Table 2.2: ----- Actions: ----------------------------------------------------
# Table 2.2(i):    count():                                       RDD[T] => Long
# Table 2.2(ii):   collect():                                   RDD[T] => Seq[T]
# Table 2.2(iii):  reduce(f: (T, T) => T):                           RDD[T] => T
# Table 2.2(iv):   lookup(k: K):                           RDD[(K, V)] => Seq[V]
# Table 2.2(iv):                                (On hash/range partitioned RDDs)
# Table 2.2(v):    save(path: String): Outputs RDD to a storage system, HDFS
#===============================================================================
def map(): pass
def filter(): pass
def flatMap(): pass
def sample(): pass
def groupByKey(): pass
def reduceByKey(): pass
def union(): pass
def join(): pass
def cogroup(): pass
def crossProduct(): pass
def mapValues(): pass
def sort(): pass
def partitionBy(): pass
def count(): pass
def collect(): pass
def reduce(): pass
def lookup(): pass
def save(): pass
#===============================================================================
# §3.1 RDD Operations in Spark
#-------------------------------------------------------------------------------
# §3.1.1(i): Table 2 lists the main RDD ...
# §3.1.1(i): ... transformations and actions available in Spark.
# §3.1.1(ii): We give the signature of each operation, ...
# §3.1.1(ii): ... showing type parameters in square brackets.
# §3.1.1(iii): Recall that transformations are lazy operations that define ...
# §3.1.1(iii): ... a new RDD, while actions launch a computation to return ...
# §3.1.1(iii): ... a value to the program or write data to external storage.
#-------------------------------------------------------------------------------
# §3.1.2(i): Note that some operations, such as join, ...
# §3.1.2(i): ... are only available on RDDs of key-value pairs.
# §3.1.2(ii): Also, our function names are chosen to match other APIs ...
# §3.1.2(ii): ... in Scala and other functional languages;
# §3.1.2(iii): for example, map is a one-to-one mapping, ...
# §3.1.2(iii): ... while flatMap maps each input value ...
# §3.1.2(iii): ... to one or more outputs (similar to the map in MapReduce).
#-------------------------------------------------------------------------------
# §3.1.3(i): In addition to these operators, users can ask for ...
# §3.1.3(i): ... an RDD to persist.
# §3.1.3(ii): Furthermore, users can get an RDD’s partition order, which is ...
# §3.1.3(ii): ... represented by a Partitioner class, and ...
# §3.1.3(ii): ... partition another dataset according to it.
# §3.1.3(iii): Operations such as groupByKey, reduceByKey and sort ...
# §3.1.3(iii): ... automatically result in a hash or range partitioned RDD.
#===============================================================================
# §3.2: Example Applications
#-------------------------------------------------------------------------------
# §3.2.0(i): We complement the data mining example in Section 2.2.1 with ...
# §3.2.0(i): ... two iterative applications: logistic regression and PageRank.
# §3.2.0(ii): The latter also showcases how ...
# §3.2.0(ii): ... control of RDDs’ partitioning can improve performance.
#-------------------------------------------------------------------------------
# §3.2.1: Logistic Regression
#-------------------------------------------------------------------------------
# §3.2.1.1(i): Many machine learning algorithms are iterative in nature ...
# §3.2.1.1(i): ... because they run iterative optimization procedures, ...
# §3.2.1.1(i): ... such as gradient descent, to maximize a function.
# §3.2.1.1(ii): They can thus run much faster by keeping their data in memory.
#-------------------------------------------------------------------------------
# §3.2.1.2(i): As an example, the following program implements ...
# §3.2.1.2(i): ... logistic regression [14], a common classification ...
# §3.2.1.2(i): ... algorithm that searches for a hyperplane w that ...
# §3.2.1.2(i): ... best separates two sets of points ...
# §3.2.1.2(i): ... (e.g., spam and non-spam emails).
# §3.2.1.2(ii): The algorithm uses gradient descent: it starts w at a random ...
# §3.2.1.2(ii): ... value, and on each iteration, it sums a function of w ...
# §3.2.1.2(ii): ... over the data to move w in a direction that improves it.
def parsePoint(): pass
points = spark.textFile("...").map(parsePoint).persist()
w = None # random initial vector
for i in range(1, ITERATIONS+1):
    gradient = points.map(
        lambda p: p.x * (1 / (1 + exp(-p.y * (dot(w, p.x)))) - 1) * p.y
    ).reduce(lambda a, b: a + b)
    w -= gradient
#-------------------------------------------------------------------------------
# §3.2.1.3(i): We start by defining a persistent RDD called points as the ...
# §3.2.1.3(i): ... result of a map transformation on a text file that ...
# §3.2.1.3(i): ... parses each line of text into a Point object.
# §3.2.1.3(ii): We then repeatedly run map and reduce on points to compute ...
# §3.2.1.3(ii): ... the gradient at each step by ...
# §3.2.1.3(ii): ... summing a function of the current w.
# §3.2.1.3(iii): Keeping points in memory across iterations can yield a ...
# §3.2.1.3(iii): ... 20x speedup, as we show in Section 6.1.
#-------------------------------------------------------------------------------
# §3.2.2: PageRank
#-------------------------------------------------------------------------------
# §3.2.2.1(i): A more complex pattern of data sharing occurs in PageRank [6].
# §3.2.2.1(ii): The algorithm iteratively updates a rank for each document ...
# §3.2.2.1(ii): ... by adding up contributions from documents that link to it.
# §3.2.2.1(iii): On each iteration, each document sends a contribution of ...
# §3.2.2.1(iii): ... "r / n" to its neighbors, where ...
# §3.2.2.1(iii): ... r is its rank and ...
# §3.2.2.1(iii): ... n is its number of neighbors.
# §3.2.2.1(iv): It then updates its rank to ...
# §3.2.2.1(iv): ... "alpha / N + (1 - alpha) * sum(c[i])", where the ...
# §3.2.2.1(iv): ... sum is over the contributions it received and ...
# §3.2.2.1(iv): ... N is the total number of documents.
# §3.2.2.1(v): We can write PageRank in Spark as follows ...
# Load graph as an RDD of (URL, outlinks) pairs
links = spark.textFile("...").map("...").persist()
ranks = None # RDD of (URL, rank) pairs
for i in range(1, ITERATIONS+1):
    # Build an RDD of (targetURL, float) pairs
    # with the contributions sent by each page
    contribs = links.join(ranks).flatMap(
        lambda url, (links, rank):
            links.map(lambda dest: (dest, rank / links.size))
    )
    # Sum contributions by URL and get new ranks
    ranks = contribs.reduceByKey(lambda x, y: x + y)
                    .mapValues(lambda sum: a/N + (1 - a) * sum)
#-------------------------------------------------------------------------------
# §3.2.2.2(i): This program leads to the RDD lineage graph in Figure 3.
# §3.2.2.2(ii): On each iteration, we create a new ranks dataset based ...
# §3.2.2.2(ii): ... on the contribs and ranks from the previous iteration ...
# §3.2.2.2(ii): ... and the static links dataset.
# §3.2.2.2(iii): One interesting feature of this graph is ...
# §3.2.2.2(iii): ... that it grows longer with the number of iterations.
# §3.2.2.2(iv): Thus, in a job with many iterations, it may be necessary ...
# §3.2.2.2(iv): ... to reliably replicate some of the versions of ranks ...
# §3.2.2.2(iv): ... to reduce fault recovery times [20].
# §3.2.2.2(v): The user can call persist with a RELIABLE flag to do this.
# §3.2.2.2(vi): However, note that the links dataset does not need to be ...
# §3.2.2.2(vi): ... replicated, because partitions of it can be rebuilt ...
# §3.2.2.2(vi): ... efficiently by rerunning a map on blocks of the input file.
# §3.2.2.2(vii): This dataset will typically be ...
# §3.2.2.2(vii): ... much larger than ranks, because each document has ...
# §3.2.2.2(vii): ... many links but only one number as its rank, so ...
# §3.2.2.2(vii): ... recovering it using lineage saves time over systems ...
# §3.2.2.2(vii): ... that checkpoint a program’s entire in-memory state.
#-------------------------------------------------------------------------------
# §3.2.2.3(i): Finally, we can optimize communication in PageRank ...
# §3.2.2.3(i): ... by controlling the partitioning of the RDDs.
# §3.2.2.3(ii): If we specify a partitioning for links (e.g., hash-partition ...
# §3.2.2.3(ii): ... the link lists by URL across nodes), we can ...
# §3.2.2.3(ii): ... partition ranks in the same way and ensure that the ...
# §3.2.2.3(ii): ... join operation between links and ranks ...
# §3.2.2.3(ii): ... requires no communication (as each URL’s rank will be ...
# §3.2.2.3(ii): ... on the same machine as its link list).
# §3.2.2.3(iii): We can also write a custom Partitioner class to group pages ...
# §3.2.2.3(iii): ... that link to each other together (e.g., partition the ...
# §3.2.2.3(iii): ... URLs by domain name).
# §3.2.2.3(iv): Both optimizations can be expressed by ...
# §3.2.2.3(iv): ... calling partitionBy when we define links.
def myPartFunc(): pass
links = spark.textFile("...").map("...").partitionBy(myPartFunc).persist()
#===============================================================================
# §4: Representing RDDs
#-------------------------------------------------------------------------------
# §4.1(i): One of the challenges in providing RDDs as an abstraction is ...
# §4.1(i): ... choosing a representation for them that can ...
# §4.1(i): ... track lineage across a wide range of transformations.
# §4.1(ii): Ideally, a system implementing RDDs should provide as rich a set ...
# §4.1(ii): ... of transformation operators as possible (e.g., the ones ...
# §4.1(ii): ... in Table 2), and let users compose them in arbitrary ways.
# §4.1(iii): We propose a simple graph-based representation for RDDs ...
# §4.1(iii): ... that facilitates these goals.
# §4.1(iv): We have used this representation in Spark to support a wide ...
# §4.1(iv): ... range of transformations without adding special logic to the ...
# §4.1(iv): ... scheduler for each one, which greatly simplified the ...
# §4.1(iv): ... system design.
#-------------------------------------------------------------------------------
# §4.2(i): In a nutshell, we propose representing each RDD through a ...
# §4.2(i): ... common interface that exposes five pieces of information: ...
partitions = None       # §4.2(i): ... a set of partitions, which are atomic pieces of the dataset; ...
dependencies = None     # §4.2(i): ... a set of dependencies on parent RDDs; ...
compute_function = None # §4.2(i): ... a function for computing the dataset based on its parents; ...
metadata = None         # §4.2(i): ... and metadata about its partitioning scheme ...
data_placement = None   # §4.2(i): ... and data placement.
# §4.2(ii): For example, an RDD representing an HDFS file has a partition ...
# §4.2(ii): ... for each block of the file and knows ...
# §4.2(ii): ... which machines each block is on.
# §4.2(iii): Meanwhile, the result of a map on this RDD has ...
# §4.2(iii): ... the same partitions, but applies the map function ...
# §4.2(iii): ... to the parent’s data when computing its elements.
# §4.2(iv): We summarize this interface in Table 3.
file_block = None
#-------------------------------------------------------------------------------
# Table 3: Interface used to represent RDDs in Spark
# Table 3(i): Operation: Meaning
def partitions(): pass      # Table 3(ii):  partitions(): Return a list of Partition objects
def preferredLocations():   # Table 3(iii): preferredLocations(p): ...
    pass                    # Table 3(iii): ... List nodes where partition p can be accessed faster ...
                            # Table 3(iii): ... due to data locality
def dependencies(): pass    # Table 3(iv):  dependencies(): Return a list of dependencies
def iterator(): pass        # Table 3(v):   iterator(p, parentIters): ...
                            # Table 3(v):   ... Compute the elements of partition p ...
                            # Table 3(v):   ... given iterators for its parent partitions
def partitioner(): pass     # Table 3(vi):  partitioner(): ...
                            # Table 3(vi):  ... Return metadata specifying ...
                            # Table 3(vi):  ... whether the RDD is hash/range partitioned
#-------------------------------------------------------------------------------
# §4.3(i): The most interesting question in designing this interface ...
# §4.3(i): ... is how to represent dependencies between RDDs.
# §4.3(ii): We found it both sufficient and useful to ...
# §4.3(ii): ... classify dependencies into two types: ...
# §4.3(ii): ... narrow dependencies, where each partition of the parent RDD ...
# §4.3(ii): ... is used by at most one partition of the child RDD, ...
# §4.3(ii): ... wide dependencies, where multiple child partitions ...
# §4.3(ii): ... may depend on it.
# §4.3(iii): For example, map leads to a narrow dependency, ...
# §4.3(iii): ... while join leads to to wide dependencies ...
# §4.3(iii): ... (unless the parents are hash-partitioned).
# §4.3(iv): Figure 4 shows other examples.
narrow_dependencies = None
wide_dependencies = None
#-------------------------------------------------------------------------------
# §4.4(i): This distinction is useful for two reasons.
# §4.4(ii): First, narrow dependencies allow for pipelined execution on ...
# §4.4(ii): ... one cluster node, which can compute all the parent partitions.
# §4.4(iii): For example, one can apply a map followed by a filter ...
# §4.4(iii): ... on an element-by-element basis.
# §4.4(iv): In contrast, wide dependencies require data from all ...
# §4.4(iv): ... parent partitions to be available and to be shuffled ...
# §4.4(iv): ... across the nodes using a MapReduce-like operation.
# §4.4(v): Second, recovery after a node failure is more efficient with a ...
# §4.4(v): ... narrow dependency, as only the lost parent partitions need to ...
# §4.4(v): ... be recomputed, and they can be recomputed ...
# §4.4(v): ... in parallel on different nodes.
# §4.4(vi): In contrast, in a lineage graph with wide dependencies, a single ...
# §4.4(vi): ... failed node might cause the loss of some partition from ...
# §4.4(vi): ... all the ancestors of an RDD, requiring a complete re-execution.
#===============================================================================
# §4.5(i): This common interface for RDDs made it possible to implement ...
# §4.5(i): ... most transformations in Spark in less than 20 lines of code.
# §4.5(ii): Indeed, even new Spark users have implemented new ...
# §4.5(ii): ... transformations (e.g., sampling and various types of joins) ...
# §4.5(ii): ... without knowing the details of the scheduler.
# §4.5(iii): We sketch some RDD implementations below.
#-------------------------------------------------------------------------------
def textFile(): pass # §4.5.1(i): HDFS files: The input RDDs in our samples have been files in HDFS.
# §4.5.1(ii): For these RDDs, partitions returns one partition for each ...
# §4.5.1(ii): ... block of the file (with the block’s offset stored in each ...
# §4.5.1(ii): ... Partition object), preferredLocations gives the nodes the ...
# §4.5.1(ii): ... block is on, and iterator reads the block.
#-------------------------------------------------------------------------------
def map(): pass # §4.5.2(i): map: Calling map on any RDD returns a MappedRDD object.
# §4.5.2(ii): This object has the same partitions and preferred locations ...
# §4.5.2(ii): ... as its parent, but applies the function passed to map ...
# §4.5.2(ii): ... to the parent’s records in its iterator method.
#-------------------------------------------------------------------------------
def union(): pass # §4.5.3(i): union: Calling union on two RDDs returns an RDD ...
# §4.5.3(i): ... whose partitions are the union of those of the parents.
# §4.5.3(ii): Each child partition is computed through a narrow dependency ...
# §4.5.3(ii): ... on the corresponding parent.
#-------------------------------------------------------------------------------
def sample(): pass # §4.5.4: sample: Sampling is similar to mapping, except that the RDD ...
# §4.5.4: ... stores a random number generator seed for each partition ...
# §4.5.4: ... to deterministically sample parent records.
#-------------------------------------------------------------------------------
def join(): pass # §4.5.5(i): join: Joining two RDDs may lead to either ...
# §4.5.5(i): ... two narrow dependencies (if they are both hash/range ...
# §4.5.5(i): ... partitioned with the same partitioner), ...
# §4.5.5(i): ... two wide dependencies, or a mix (if one parent has a ...
# §4.5.5(i): ... partitioner and one does not).
# §4.5.5(ii): In either case, the output RDD has a partitioner (either one ...
# §4.5.5(ii): ... inherited from the parents or a default hash partitioner).
#===============================================================================
# §5: Implementation
#-------------------------------------------------------------------------------
# §5.0.1(i): We have implemented Spark in about 14,000 lines of Scala.
Spark = None
Mesos = None # §5.0.1(ii): The system runs over the Mesos cluster manager ...
Mesos_cluster_manager = None
# §5.0.1(ii): ... [17], allowing it to share resources with Hadoop, ...
# §5.0.1(ii): ... MPI and other applications.
# §5.0.1(iii): Each Spark program runs as a separate Mesos application, ...
Spark_program = None
Mesos_application = None
# §5.0.1(iii): ... with its own driver (master) and workers, and resource ...
# §5.0.1(iii): ... sharing between these applications is handled by Mesos.
#-------------------------------------------------------------------------------
# §5.0.2: Spark can read data from any Hadoop input source (e.g., HDFS ...
# §5.0.2: ... or HBase) using Hadoop’s existing input plugin APIs, and ...
# §5.0.2: ... runs on an unmodified version of Scala.
#-------------------------------------------------------------------------------
# §5.0.3: We now sketch several of the technically interesting parts of ...
# §5.0.3: ... the system: our job scheduler (§5.1), our ...
# §5.0.3: ... Spark interpreter allowing interactive use (§5.2), ...
# §5.0.3: ... memory management (§5.3), and support for ...
# §5.0.3: ... checkpointing (§5.4).
#===============================================================================
# §5.1: Job Scheduling
#-------------------------------------------------------------------------------
# §5.1.1: Spark’s scheduler uses our representation of RDDs, ...
# §5.1.1: ... described in Section 4.
#-------------------------------------------------------------------------------
# §5.1.2(i): Overall, our scheduler is similar to Dryad’s [19], ...
# §5.1.2(i): ... but it additionally takes into account ...
# §5.1.2(i): ... which partitions of persistent RDDs are available in memory.
# §5.1.2(ii): Whenever a user runs an action (e.g., count or save) on ...
# §5.1.2(ii): ... an RDD, the scheduler examines that RDD’s lineage graph to ...
# §5.1.2(ii): ... build a DAG of stages to execute, as illustrated in Figure 5.
# §5.1.2(iii): Each stage contains as many pipelined transformations ...
# §5.1.2(iii): ... with narrow dependencies as possible.
# §5.1.2(iv): The boundaries of the stages are the shuffle operations ...
# §5.1.2(iv): ... required for wide dependencies, or any already computed ...
# §5.1.2(iv): ... partitions that can short-circuit the ...
# §5.1.2(iv): ... computation of a parent RDD.
# §5.1.2(v): The scheduler then launches tasks to compute missing partitions ...
# §5.1.2(v): ... from each stage until it has computed the target RDD.
scheduler = None
tasks = None
#-------------------------------------------------------------------------------
# §5.1.2(i): Our scheduler assigns tasks to machines ...
# §5.1.2(i): ... based on data locality using delay scheduling [32].
# §5.1.2(ii): If a task needs to process a partition that is available ...
# §5.1.2(ii): ... in memory on a node, we send it to that node.
# §5.1.2(iii): Otherwise, if a task processes a partition for which the ...
# §5.1.2(iii): ... containing RDD provides preferred locations (e.g., an ...
# §5.1.2(iii): ... HDFS file), we send it to those.
#-------------------------------------------------------------------------------
# §5.1.3: For wide dependencies (i.e., shuffle dependencies), we currently ...
# §5.1.3: ... materialize intermediate records on the nodes holding parent ...
# §5.1.3: ... partitions to simplify fault recovery, much like MapReduce ...
# §5.1.3: ... materializes map outputs.
#-------------------------------------------------------------------------------
# §5.1.4(i): If a task fails, we re-run it on another node ...
# §5.1.4(i): ... as long as its stage’s parents are still available.
# §5.1.4(ii): If some stages have become unavailable (e.g., because an ...
# §5.1.4(ii): ... output from the “map side” of a shuffle was lost), we ...
# §5.1.4(ii): ... resubmit tasks to compute the missing partitions in parallel.
# §5.1.4(iii): We do not yet tolerate scheduler failures, though ...
# §5.1.4(iii): ... replicating the RDD lineage graph would be straightforward.
#-------------------------------------------------------------------------------
# §5.1.5(i): Finally, although all computations in Spark currently run ...
# §5.1.5(i): ... in response to actions called in the driver program, ...
# §5.1.5(i): ... we are also experimenting with letting tasks on the cluster ...
# §5.1.5(i): ... (e.g., maps) call the lookup operation, which provides ...
# §5.1.5(i): ... random access to elements of hash-partitioned RDDs by key.
# §5.1.5(ii): In this case, tasks would need to tell the scheduler ...
# §5.1.5(ii): ... to compute the required partition if it is missing.
#===============================================================================
# §5.2: Interpreter Integration
#-------------------------------------------------------------------------------
# 31 flavors of patterns to choose from ...
from SNOBOL4python import GLOBALS, TRACE, ε, σ, π, λ, Λ, ζ, θ, Θ, φ, Φ, α, ω
from SNOBOL4python import ABORT, ANY, ARB, ARBNO, BAL, BREAK, BREAKX, FAIL
from SNOBOL4python import FENCE, LEN, MARB, MARBNO, NOTANY, POS, REM, RPOS
from SNOBOL4python import RTAB, SPAN, SUCCEED, TAB
from SNOBOL4python import ALPHABET, DIGITS, UCASE, LCASE
from SNOBOL4python import PATTERN, STRING, NULL
from pprint import pformat, pprint
#-------------------------------------------------------------------------------
# §5.2.1(i): Scala includes an interactive shell ...
# §5.2.1(i): ... similar to those of Ruby and Python.
# §5.2.1(ii): Given the low latencies attained with in-memory data, ...
# §5.2.1(ii): ... we wanted to let users run Spark interactively ...
# §5.2.1(ii): ... from the interpreter to query big datasets.
#-------------------------------------------------------------------------------
def interp(t):
    match t[0]:
        case 'id':      #
                        if t[1] in globals():
                            return globals()[t[1]]
                        else: return None
        case 'int':     return t[1]
        case 'str':     return t[1]
        case '=':       globals()[t[1]] = interp(t[2])
        case '+=':      globals()[t[1]] += interp(t[2])
        case '-=':      globals()[t[1]] -= interp(t[2])
        case '*':       return interp(t[1]) * interp(t[2])
        case '/':       return interp(t[1]) / interp(t[2])
        case '+':       # positive and addition
                        if len(t) == 2: return +interp(t[1])
                        elif len(t) == 3: return interp(t[1]) + interp(t[2])
        case '-':       # negative and subtraction
                        if len(t) == 2: return -interp(t[1])
                        elif len(t) == 3: return interp(t[1]) - interp(t[2])
        case 'eval':    return interp(t[1])
        case 'print':   return print(interp(t[1]))
        case 'for':     # looping
                        for n in range(interp(t[2]), interp(t[3])):
                            globals()[t[1]] = n
                            for s in t[4:]: interp(s)
        case 'scala':   # interpret each statement
                        for s in t[1:]: interp(s)
        case _:         raise Exception(f"interp: {t}")
#-------------------------------------------------------------------------------
# §5.2.2(i): The Scala interpreter normally operates by compiling a class ...
# §5.2.2(i): ... for each line typed by the user, loading it into the JVM, ...
# §5.2.2(i): ... and invoking a function on it.
# §5.2.2(ii): This class includes a singleton object that contains ...
# §5.2.2(ii): ... the variables or functions on that line and ...
# §5.2.2(ii): ... runs the line’s code in an initialize method.
#-------------------------------------------------------------------------------
def init():     return λ(f"scala = None; stack = []")   # + λ(f"pprint(stack)")
def push(v):    return λ(f"stack.append([{v}])")        # + λ(f"pprint(stack)")
def inject(v):  return ( λ(f"top = stack[-1].pop()")    # + λ(f"pprint(stack)")
                       + λ(f"stack.append([{v}, top])") # + λ(f"pprint(stack)")
                       )
def item(v):    return λ(f"stack[-1].append({v})")      # + λ(f"pprint(stack)")
def pop():      return ( λ(f"top = tuple(stack.pop())") # + λ(f"pprint(stack)")
                       + λ(f"stack[-1].append(top)")    # + λ(f"pprint(stack)")
                       )
def fini():     return λ(f"scala = tuple(stack.pop())") # + λ(f"pprint(stack)")
#-------------------------------------------------------------------------------
# §5.2.2(iii): For example, if the user types var x = 5 followed by ...
# §5.2.2(iii): ... println(x), the interpreter defines a class called Line1 ...
# §5.2.2(iii): ... containing x and causes the second line to ...
# §5.2.2(iii): ... compile to println(Line1.getInstance().x).
#-------------------------------------------------------------------------------
η           =   ARBNO(SPAN(' \t\r\n') | σ('//') + BREAK('\n'))
def ς(s):       return η + σ(s)
#-------------------------------------------------------------------------------
integer     =   η + (SPAN(DIGITS)) @ "OUTPUT" % "txtint"
string      =   η + (σ('"') + BREAK('"') + σ('"')) @ "OUTPUT" % "txtstr"
identifier  =   ( η
                + ( ANY(UCASE+LCASE)
                  + (SPAN(DIGITS+UCASE+'_'+LCASE) | ε())
                  ) @ "OUTPUT" % "id"
                )
#-------------------------------------------------------------------------------
parameters  =   ( identifier + item("id")
                + ARBNO(ς(',') + identifier + item("id"))
                )
function    =   ( push("'lambda'")
                + parameters
                + ς('=>')
                + ζ(lambda: expression)
                + pop()
                )
reference   =   ( identifier + push("'id'") + item("id") + pop()
                | identifier + push("id")
                + ς('(') + ζ(lambda: arguments) + ς(')') + pop()
                )
#-------------------------------------------------------------------------------
element     =   ( integer + push("'int'") + item("eval(txtint)") + pop()
                | string  + push("'str'") + item("eval(txtstr)") + pop()
                | reference
                | ς('(') + ζ(lambda: expression) + ς(')')
                )
factor      =   ( ς('+') + push("'+'") + ζ(lambda: factor) + pop()
                | ς('-') + push("'-'") + ζ(lambda: factor) + pop()
                | element
                + ARBNO(
                    ς('.')
                  + inject("'.'")
                  + reference
                  + pop()
                  )
                )
term        =   ( factor
                + ( ς('*') + inject("'*'") + ζ(lambda: term) + pop()
                  | ς('/') + inject("'/'") + ζ(lambda: term) + pop()
                  | ε()
                  )
                )
expression  =   ( term
                + ( ς('+') + inject("'+'") + ζ(lambda: expression) + pop()
                  | ς('-') + inject("'-'") + ζ(lambda: expression) + pop()
                  | ς('dot') + inject("'dot'") + ζ(lambda: expression) + pop()
                  | ε()
                  )
                )
#-------------------------------------------------------------------------------
argument    =   ( function
                | expression
                + (ς('until') + inject("'until'") + expression + pop() | ε())
                | ( ς('new') + push("'new'")
                  + identifier + item("id")
                  + ς('(') + ζ(lambda: arguments) + ς(')')
                  + pop()
                  )
                )
arguments   =   argument + ARBNO(ς(',') + argument) | ε()
#-------------------------------------------------------------------------------
assignment  =   ( (ς('var') | ς('val') | ε())
                + identifier
                + ( ς('=') + push("'='") + item("id")
                  | ς('+=') + push("'+='") + item("id")
                  | ς('-=') + push("'-='") + item("id")
                  )
                + expression
                + pop()
                )
#-------------------------------------------------------------------------------
loop        =   ( ς('for') + push("'for'")
                + ς('(') + identifier + item("id")
                + ς('<-') + expression + (ς('to') + expression | ε())
                + ς(')') + ς('{')
                + ζ(lambda: statements)
                + ς('}')
                + pop()
                )
#-------------------------------------------------------------------------------
statement   =   ( loop
                | assignment + ς(';')
                | push("'eval'") + expression + ς(';') + pop()
                )
statements  =   ARBNO(statement)
program     =   ( POS(0)
                + init()
                + push("'scala'")
                + statements
                + fini()
                + η + RPOS(0)
                )
#-------------------------------------------------------------------------------
# §5.2.3: We made two changes to the interpreter in Spark:
#-------------------------------------------------------------------------------
# §5.2.3.1: Class shipping: To let the worker nodes fetch the bytecode ...
# §5.2.3.1: ... for the classes created on each line, ...
# §5.2.3.1: ... we made the interpreter serve these classes over HTTP.
#-------------------------------------------------------------------------------
# §5.2.3.2(i): Modified code generation: Normally, the singleton object ...
# §5.2.3.2(i): ... created for each line of code is accessed through ...
# §5.2.3.2(i): ... a static method on its corresponding class.
# §5.2.3.2(ii): This means that when we serialize a closure referencing a ...
# §5.2.3.2(ii): ... variable defined on a previous line, such as Line1.x ...
# §5.2.3.2(ii): ... in the example above, Java will not trace through the ...
# §5.2.3.2(ii): ... object graph to ship the Line1 instance wrapping around x.
# §5.2.3.2(iii): Therefore, the worker nodes will not receive x.
# §5.2.3.2(iv): We modified the code generation logic ...
# §5.2.3.2(iv): ... to reference the instance of each line object directly.
#-------------------------------------------------------------------------------
# §5.2.4: Figure 6 shows how the interpreter translates a set of lines ...
# §5.2.4: ... typed by the user to Java objects after our changes.
#-------------------------------------------------------------------------------
# §5.2.5(i): We found the Spark interpreter to be useful in processing ...
# §5.2.5(i): ... large traces obtained as part of our research and ...
# §5.2.5(i): ... exploring datasets stored in HDFS.
# §5.2.5(i): We also plan to use ...
# §5.2.5(i): ... to run higher-level query languages interactively, e.g., SQL.
#===============================================================================
GLOBALS(globals())
TRACE(40)
program_source = """\
x = 0;
for (i <- 0 to 10) {
    x += 1;
    print(x);
}
"""
if program_source in program:
    pprint(scala)
    pprint(interp(scala))
else: print("Boo!")
#===============================================================================
# §5.3: Memory Management
memory_management = None
#-------------------------------------------------------------------------------
# §5.3.1(i): Spark provides three options for storage of persistent RDDs: ...
# §5.3.1(i): ... in-memory storage as deserialized Java objects, ...
# §5.3.1(i): ... in-memory storage as serialized data, ...
# §5.3.1(i): ... and on-disk storage.
# §5.3.1(ii): The first option provides the fastest performance, ...
# §5.3.1(ii): ... because the Java VM can access each RDD element natively.
# §5.3.1(iii): The second option lets users choose a more memory-efficient ...
# §5.3.1(iii): ... representation than Java object graphs ...
# §5.3.1(iii): ... when space is limited, at the cost of lower performance.
# §5.3.1(iv): The third option is useful for RDDs that are too large ...
# §5.3.1(iv): ... to keep in RAM but costly to recompute on each use.
#-------------------------------------------------------------------------------
# §5.3.2(i): To manage the limited memory available, ...
# §5.3.2(i): ... we use an LRU eviction policy at the level of RDDs.
# §5.3.2(ii): When a new RDD partition is computed but there ...
# §5.3.2(ii): ... is not enough space to store it, we ...
# §5.3.2(ii): ... evict a partition from the least recently accessed RDD, ...
# §5.3.2(ii): ... unless this is the same RDD as the one with the new partition.
# §5.3.2(iii): In that case, we keep the old partition in memory ...
# §5.3.2(iii): ... to prevent cycling partitions from the same RDD in and out.
# §5.3.2(iv): This is important because most operations will run tasks ...
# §5.3.2(iv): ... over an entire RDD, so it is quite likely that the ...
# §5.3.2(iv): ... partition already in memory will be needed in the future.
# §5.3.2(v): We found this default policy to work well in all our ...
# §5.3.2(v): ... applications so far, but we also give users further ...
# §5.3.2(v): ... control via a “persistence priority” for each RDD.
#-------------------------------------------------------------------------------
# §5.3.3(i): Finally, each instance of Spark on a cluster ...
# §5.3.3(i): ... currently has its own separate memory space.
# §5.3.3(ii): In future work, we plan to investigate sharing RDDs ...
# §5.3.3(ii): ... across instances of Spark through a unified memory manager.
#===============================================================================
checkpoint = None # §5.4: Support for Checkpointing
#-------------------------------------------------------------------------------
# §5.4.1(i): Although lineage can always be used to recover RDDs ...
# §5.4.1(i): ... after a failure, such recovery may be time-consuming ...
# §5.4.1(i): ... for RDDs with long l-ineage chains.
# §5.4.1(ii): Thus, it can be helpful to checkpoint some RDDs to stable storage.
#-------------------------------------------------------------------------------
# §5.4.2(i): In general, checkpointing is useful for RDDs with long ...
# §5.4.2(i): ... lineage graphs containing wide dependencies, such as the ...
# §5.4.2(i): ... rank datasets in our PageRank example (§3.2.2).
# §5.4.2(ii): In these cases, a node failure in the cluster may result in ...
# §5.4.2(ii): ... the loss of some slice of data from each parent RDD, ...
# §5.4.2(ii): ... requiring a full recomputation [20].
# §5.4.2(iii): In contrast, for RDDs with narrow dependencies on data in ...
# §5.4.2(iii): ... stable storage, such as the points in our ...
# §5.4.2(iii): ... logistic regression example (§3.2.1) and the link lists ...
# §5.4.2(iii): ... in PageRank, checkpointing may never be worthwhile.
# §5.4.2(iv): If a node fails, lost partitions from these RDDs can be ...
# §5.4.2(iv): ... recomputed in parallel on other nodes, at a ...
# §5.4.2(iv): ... fraction of the cost of replicating the whole RDD.
#-------------------------------------------------------------------------------
# §5.4.3(i): Spark currently provides an API for checkpointing (a ...
# §5.4.3(i): ... REPLICATE flag to persist), but leaves the decision of ...
# §5.4.3(i): ... which data to checkpoint to the user.
# §5.4.3(ii): However, we are also investigating how to ...
# §5.4.3(ii): ... perform automatic checkpointing.
# §5.4.3(iii): Because our scheduler knows the size of each dataset ...
# §5.4.3(iii): ... as well as the time it took to first compute it, ...
# §5.4.3(iii): ... it should be able to select an optimal set of RDDs ...
# §5.4.3(iii): ... to checkpoint to minimize system recovery time [30].
#-------------------------------------------------------------------------------
# §5.4.4(i): Finally, note that the read-only nature of RDDs ...
# §5.4.4(i): ... makes them simpler to checkpoint than general shared memory.
# §5.4.4(ii): Because consistency is not a concern, RDDs ...
# §5.4.4(ii): ... can be written out in the background without requiring ...
# §5.4.4(ii): ... program pauses or distributed snapshot schemes.
#===============================================================================
