{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language ToolKit\n",
    "\n",
    "#### Basic operations\n",
    "* Tokenization\n",
    "* Stemming and lemmatization\n",
    "* POS tagging\n",
    "* Syntactic parsing\n",
    "* Edit distance\n",
    "\n",
    "#### Advanced\n",
    "* Named entity recognition\n",
    "* Synonymy and antonymy\n",
    "* Sentiment analysis \n",
    "\n",
    "#### References\n",
    "* https://likegeeks.com/nlp-tutorial-using-python-nltk/\n",
    "* https://www.nltk.org/book/ch01.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "Advantages: \n",
    "* very comprehensive\n",
    "* Functions easily defined\n",
    "* Comes with a rich collection of corpora\n",
    "* A lot of material available\n",
    "\t\n",
    "\n",
    "Disadvantages:\n",
    "* Tends to get slow for production-level applications (spaCy is more popular)\n",
    "* Limitations of syntactic parsing\n",
    "* Little incorporation of latest developments in machine learning (e.g deep learning applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import nltk, pprint \n",
    "from urllib import request\n",
    "import re\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462073"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download a text\n",
    "url = \"http://www.gutenberg.org/cache/epub/174/pg174.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization: \n",
    "* Splits sentence on white spaces\n",
    "* Removes diacritics\n",
    "* Segments out punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', 'by', 'Oscar', 'Wilde', 'This', 'eBook', 'is', 'for', 'the', 'use']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(text)\n",
    "text[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dent',\n",
       " 'dent',\n",
       " 'dent',\n",
       " 'dent',\n",
       " 'dent',\n",
       " 'dent',\n",
       " 'dent',\n",
       " 'dent',\n",
       " 'dent',\n",
       " 'dent',\n",
       " 'dent',\n",
       " 'dent',\n",
       " 'dent']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"dent\", str(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "\n",
    "#### Stemming: \n",
    "* removing inflectional elements from a form\n",
    "\n",
    "#### Lemmatization: \n",
    "* modifying the token to its canonical form, normally Nominative singular. Inflections are NOT dropped, e.g. \n",
    "\n",
    "* Example from Russian: \n",
    "* \"sobake\"/dog-n,Genitive Singular --> lemmatizes to \"sobaka\"; -a is an inflection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEMMING\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "#stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'syllabi'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"syllabi\")\n",
    "#try: irregular, rare and ambiguous nouns: \"oxen\", syllabi\", \"paintings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people\n"
     ]
    }
   ],
   "source": [
    "#LEMMATIZATION\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "L = WordNetLemmatizer()\n",
    "\n",
    "#lemmatizing individual words\n",
    "print(L.lemmatize(\"peopled\", pos = \"v\"))\n",
    "\n",
    "#compare to: print(L.lemmatize(\"paintings\", pos = \"v\"))    \n",
    "#try print(L.lemmatize(\"peopled\")) with and without \"pos\" = v    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      "by\n",
      "Oscar\n",
      "Wilde\n",
      "This\n",
      "eBook\n",
      "is\n",
      "for\n",
      "the\n",
      "use\n",
      "of\n",
      "anyone\n",
      "anywhere\n",
      "at\n",
      "no\n",
      "cost\n",
      "and\n",
      "with\n",
      "almost\n",
      "no\n",
      "restriction\n",
      "whatsoever\n",
      ".\n",
      "You\n",
      "may\n",
      "copy\n",
      "it\n",
      ",\n",
      "give\n",
      "it\n",
      "away\n",
      "or\n",
      "re-use\n",
      "it\n",
      "under\n",
      "the\n",
      "term\n",
      "of\n",
      "the\n",
      "Project\n",
      "Gutenberg\n",
      "License\n",
      "included\n",
      "with\n",
      "this\n",
      "eBook\n",
      "or\n",
      "online\n",
      "at\n",
      "www.gutenberg.net\n",
      "Title\n",
      ":\n",
      "The\n",
      "Picture\n",
      "of\n",
      "Dorian\n",
      "Gray\n",
      "Author\n",
      ":\n",
      "Oscar\n",
      "Wilde\n",
      "Release\n",
      "Date\n",
      ":\n",
      "June\n",
      "9\n",
      ",\n",
      "2008\n",
      "[\n",
      "EBook\n",
      "#\n",
      "174\n",
      "]\n",
      "[\n",
      "This\n",
      "file\n",
      "last\n",
      "updated\n",
      "on\n",
      "July\n",
      "2\n",
      ",\n",
      "2011\n",
      "]\n",
      "[\n",
      "This\n",
      "file\n",
      "last\n",
      "updated\n",
      "on\n",
      "July\n",
      "23\n",
      ",\n",
      "2014\n",
      "]\n",
      "Language\n",
      ":\n",
      "English\n",
      "***\n",
      "START\n",
      "OF\n",
      "THIS\n",
      "PROJECT\n",
      "GUTENBERG\n",
      "EBOOK\n",
      "THE\n",
      "PICTURE\n",
      "OF\n",
      "DORIAN\n",
      "GRAY\n",
      "***\n",
      "Produced\n",
      "by\n",
      "Judith\n",
      "Boss\n",
      ".\n",
      "HTML\n",
      "version\n",
      "by\n",
      "Al\n",
      "Haines\n",
      ".\n",
      "The\n",
      "Picture\n",
      "of\n",
      "Dorian\n",
      "Gray\n",
      "by\n",
      "Oscar\n",
      "Wilde\n",
      "THE\n",
      "PREFACE\n",
      "The\n",
      "artist\n",
      "is\n",
      "the\n",
      "creator\n",
      "of\n",
      "beautiful\n",
      "thing\n",
      ".\n",
      "To\n",
      "reveal\n",
      "art\n",
      "and\n",
      "conceal\n",
      "the\n",
      "artist\n",
      "is\n",
      "art\n",
      "'s\n",
      "aim\n",
      ".\n",
      "The\n",
      "critic\n",
      "is\n",
      "he\n",
      "who\n",
      "can\n",
      "translate\n",
      "into\n",
      "another\n",
      "manner\n",
      "or\n",
      "a\n",
      "new\n",
      "material\n",
      "his\n",
      "impression\n",
      "of\n",
      "beautiful\n",
      "thing\n",
      ".\n",
      "The\n",
      "highest\n",
      "a\n",
      "the\n",
      "lowest\n",
      "form\n",
      "of\n",
      "criticism\n",
      "is\n",
      "a\n",
      "mode\n",
      "of\n",
      "autobiography\n",
      ".\n",
      "Those\n",
      "who\n",
      "find\n"
     ]
    }
   ],
   "source": [
    "#lemmatizing text\n",
    "for word in text[10:200]:\n",
    "    print(L.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "Penn Tree bank reference:\n",
    "* CC coordinating conjunction\n",
    "* CD cardinal digit\n",
    "* DT determiner\n",
    "* EX existential there (like: “there is” … think of it like “there exists”)\n",
    "* FW foreign word\n",
    "* IN preposition/subordinating conjunction\n",
    "* JJ adjective ‘big’\n",
    "* JJR adjective, comparative ‘bigger’\n",
    "* JJS adjective, superlative ‘biggest’\n",
    "* LS list marker 1)\n",
    "* MD modal could, will\n",
    "* NN noun, singular ‘desk’\n",
    "* NNS noun plural ‘desks’\n",
    "* NNP proper noun, singular ‘Harrison’\n",
    "* NNPS proper noun, plural ‘Americans’\n",
    "* PDT predeterminer ‘all the kids’\n",
    "* POS possessive ending parent’s\n",
    "* PRP personal pronoun I, he, she\n",
    "* PRP\\$ possessive pronoun my, his, hers\n",
    "* RB adverb very, silently\n",
    "* RBR adverb, comparative better\n",
    "* RBS adverb, superlative best\n",
    "* RP particle give up\n",
    "* TO, to go ‘to’ the store.\n",
    "* UH interjection, errrrrrrrm\n",
    "* VB verb, base form take\n",
    "* VBD verb, past tense took\n",
    "* VBG verb, gerund/present participle taking\n",
    "* VBN verb, past participle taken\n",
    "* VBP verb, sing. present, non-3d take\n",
    "* VBZ verb, 3rd person sing. present takes\n",
    "* WDT wh-determiner which\n",
    "* WP wh-pronoun who, what\n",
    "* WP$ possessive wh-pronoun whose\n",
    "* WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'donated', 'the', 'painting', 'to', 'the', 'museum']\n"
     ]
    }
   ],
   "source": [
    "sentence = nltk.word_tokenize(\"he donated the painting to the museum\")\n",
    "#try ambiguous cases, e.g. \"the old man the boat\"\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('painting', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#print(nltk.pos_tag(sentence))\n",
    "\n",
    "#try: \n",
    "#print(nltk.pos_tag(\"painting\"))\n",
    "#print(nltk.pos_tag(list(\"painting\")))\n",
    "print(nltk.pos_tag([\"painting\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "... S -> NP VP\n",
    "... PP -> P NP\n",
    "... NP -> Det N | Det N PP | 'I'\n",
    "... VP -> V NP | VP PP\n",
    "... Det -> 'a' | 'an' | 'my'\n",
    "... N -> 'book' | 'library' |'branch' | 'tree' \n",
    "... V -> 'borrowed' | 'pruned'\n",
    "... P -> 'from'\n",
    "... \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nltk.word_tokenize(\"I pruned a branch from my tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V pruned) (NP (Det a) (N branch)))\n",
      "    (PP (P from) (NP (Det my) (N tree)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V pruned)\n",
      "    (NP (Det a) (N branch) (PP (P from) (NP (Det my) (N tree))))))\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK continued\n",
    "* Edit distance\n",
    "* Sentiment analysis\n",
    "* Working with lexicons (synonymy and antonymy)\n",
    "* Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit distance \n",
    "* Known as the Levenshtein Distance \n",
    "* Jaccard distance an alternative metric\n",
    "* Covered in J&M Ch4\n",
    "* Measured by the number of deletions, insertions or substitutions\n",
    "* Common applications: plagiarism detection and spell checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Edit distance: the fewer edit steps the smaller the distance\n",
    "w1 = \"dog\"\n",
    "w2 = \"painting\"\n",
    "nltk.edit_distance(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Jaccard distance: measures dissimilarity; the more similar the forms the smaller the value\n",
    "w1 = set(\"dog\")\n",
    "w2 = set(\"painting\")\n",
    "nltk.jaccard_distance(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree 7\n",
      "linear 5\n",
      "war 5\n",
      "nominal 5\n",
      "night 6\n",
      "nocturnal 5\n",
      "fridge 7\n",
      "swallow 6\n",
      "novel 5\n",
      "blue 6\n",
      "navel 5\n",
      "nocturnal 5\n",
      "nucleus 4\n",
      "nuclear 2\n"
     ]
    }
   ],
   "source": [
    "#Spell checker: \n",
    "# given a misspelling, find the correct form with the shortest edit distance\n",
    "misspelling = \"nucular\"  \n",
    "correction = [\"tree\", \"linear\", \"war\", \"nominal\", \"night\", \"nocturnal\", \"fridge\", \"swallow\", \"novel\", \"blue\", \"navel\", \"nocturnal\", \"nucleus\", \"nuclear\"]\n",
    "for word in correction: \n",
    "    distance = nltk.edit_distance(misspelling, word)\n",
    "    print(word, distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plagiarism detection\n",
    "# compare source and target texts as to the edit distance at word-level\n",
    "# texts with the greatest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Edit Distance between sent1 and sent2\n",
      "84 Edit Distance between sent1 and sent3\n",
      "96 Edit Distance between sent1 and sent4\n",
      "95 Edit Distance between sent1 and sent5\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"Olive Kitteridge is a 2014 four-hour miniseries based on the 2008 novel of the same name by Elizabeth Strout\"\n",
    "sent2 = \"Olive Kitteridge is a 2014 4-hour long miniseries based on the 2008 novel of the same name by the writer Elizabeth Strout\"\n",
    "sent3 = \"Following a 2008 novel Olive Kitteridge by Elizabeth Strout, a four-hour miniseries by the same name were made in 2014\"\n",
    "sent4 = \"Elizabeth Strout's'novel, Olive Kitteridge published in 2008, became the basis for the four-hour miniseries released in 2014\"\n",
    "sent5 = \"I love Python programming.\"\n",
    " \n",
    "ed_sent_1_2 = nltk.edit_distance(sent1, sent2)\n",
    "ed_sent_1_3 = nltk.edit_distance(sent1, sent3)\n",
    "ed_sent_1_4 = nltk.edit_distance(sent1, sent4)\n",
    "ed_sent_1_5 = nltk.edit_distance(sent1, sent5)\n",
    " \n",
    " \n",
    "print(ed_sent_1_2, 'Edit Distance between sent1 and sent2')\n",
    "print(ed_sent_1_3, 'Edit Distance between sent1 and sent3')\n",
    "print(ed_sent_1_4, 'Edit Distance between sent1 and sent4')\n",
    "print(ed_sent_1_5, 'Edit Distance between sent1 and sent5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.3, 'neu': 0.7, 'pos': 0.0, 'compound': -0.4585}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = SentimentIntensityAnalyzer()\n",
    "S.polarity_scores(\"I have not had any pleasure reading the book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great place to be when you are in Barcelona.\n",
      "neg: 0.0, neu: 0.661, pos: 0.339, compound: 0.6249, \n",
      "The place was being renovated when I visited so the seating was limited.\n",
      "neg: 0.147, neu: 0.853, pos: 0.0, compound: -0.2263, \n",
      "Loved the ambience, loved the food\n",
      "neg: 0.0, neu: 0.339, pos: 0.661, compound: 0.8316, \n",
      "The food is delicious but not over the top.\n",
      "neg: 0.168, neu: 0.623, pos: 0.209, compound: 0.1184, \n",
      "Service - Little slow, probably because too many people.\n",
      "neg: 0.0, neu: 1.0, pos: 0.0, compound: 0.0, \n",
      "The place is not easy to locate\n",
      "neg: 0.286, neu: 0.714, pos: 0.0, compound: -0.3412, \n",
      "Mushroom fried rice was tasty\n",
      "neg: 0.0, neu: 1.0, pos: 0.0, compound: 0.0, \n"
     ]
    }
   ],
   "source": [
    "hotel_rev = ['Great place to be when you are in Barcelona.', \n",
    "             'The place was being renovated when I visited so the seating was limited.', \n",
    "             'Loved the ambience, loved the food', \n",
    "             'The food is delicious but not over the top.',\n",
    "             'Service - Little slow, probably because too many people.', \n",
    "             'The place is not easy to locate', \n",
    "             'Mushroom fried rice was tasty']\n",
    "  \n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in hotel_rev:\n",
    "     print(sentence)\n",
    "     ss = sid.polarity_scores(sentence)\n",
    "     for k in ss:\n",
    "         print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the lexicon\n",
    "* Wordnet is a package in NLTK\n",
    "* Wordnet is a popular lexical resource, built as a collection of sets of synonyms, aka synsets\n",
    "* Can invoke Wordnet to retrieve synonyms, antonyms, definitions and examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wordnet\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most common medium of exchange; functions as legal tender\n",
      "['we tried to collect the money he owed us']\n"
     ]
    }
   ],
   "source": [
    "#obtain a definition and example for \"pain\"\n",
    "synsets = wordnet.synsets(\"money\")\n",
    "#synsets\n",
    "print(synsets[0].definition())\n",
    "print(synsets[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'direct', 'trail', 'rail', 'coach', 'condition', 'geartrain', 'school', 'develop', 'educate', 'civilize', 'prepare', 'power_train', 'caravan', 'cultivate', 'wagon_train', 'railroad_train', 'string', 'take', 'gear', 'groom', 'aim', 'train', 'discipline', 'take_aim', 'civilise', 'check', 'gearing'}\n"
     ]
    }
   ],
   "source": [
    "#obtain synonyms\n",
    "synonyms = [] #declare an empty list of synonyms\n",
    "\n",
    "for syn in wordnet.synsets(\"train\"):       # get all synsets for \"tank\"\n",
    "    for lemma in syn.lemmas():            # obtain a lemma from each synset\n",
    "        synonyms.append(lemma.name())     # populate the list of synonyms \n",
    "print(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thicken', 'full', 'gain', 'thick', 'thickly', 'fat'}\n"
     ]
    }
   ],
   "source": [
    "#obtain antonyms\n",
    "antonyms = []  #declare an empty list of synonyms\n",
    "for syn in wordnet.synsets(\"thin\"):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition \n",
    "* NER is a common NLP task\n",
    "* Normal entity breakdown: person, geolocation, organization, conceptual work, event, currency, time, date, etc.\n",
    "* NLTK's NER is embedded into a pre-processing function which also performs POS tagging and \"chunking\" (a form of shallow parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare an input text\n",
    "txt = \"The present mayor is Bill de Blasio, the first Democrat since 1993.[572] He was elected in 2013 with over 73% of the vote, and assumed office on January 1, 2014. The Democratic Party holds the majority of public offices. As of April 2016, 69% of registered voters in the city are Democrats and 10% are Republicans.[573] New York City has not been carried by a Republican in a statewide or presidential election since President Calvin Coolidge won the five boroughs in 1924. In 2012, Democrat Barack Obama became the first presidential candidate of any party to receive more than 80% of the overall vote in New York City, sweeping all five boroughs. Party platforms center on affordable housing, education, and economic development, and labor politics are of importance in the city.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  present/JJ\n",
      "  mayor/NN\n",
      "  is/VBZ\n",
      "  (PERSON Bill/NNP de/NNP Blasio/NNP)\n",
      "  ,/,\n",
      "  the/DT\n",
      "  first/JJ\n",
      "  Democrat/NNP\n",
      "  since/IN\n",
      "  1993/CD\n",
      "  ./.\n",
      "  [/VB\n",
      "  572/CD\n",
      "  ]/NN\n",
      "  He/PRP\n",
      "  was/VBD\n",
      "  elected/VBN\n",
      "  in/IN\n",
      "  2013/CD\n",
      "  with/IN\n",
      "  over/IN\n",
      "  73/CD\n",
      "  %/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  vote/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  assumed/VBD\n",
      "  office/NN\n",
      "  on/IN\n",
      "  January/NNP\n",
      "  1/CD\n",
      "  ,/,\n",
      "  2014/CD\n",
      "  ./.\n",
      "  The/DT\n",
      "  (ORGANIZATION Democratic/JJ Party/NNP)\n",
      "  holds/VBZ\n",
      "  the/DT\n",
      "  majority/NN\n",
      "  of/IN\n",
      "  public/JJ\n",
      "  offices/NNS\n",
      "  ./.\n",
      "  As/IN\n",
      "  of/IN\n",
      "  April/NNP\n",
      "  2016/CD\n",
      "  ,/,\n",
      "  69/CD\n",
      "  %/NN\n",
      "  of/IN\n",
      "  registered/JJ\n",
      "  voters/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  city/NN\n",
      "  are/VBP\n",
      "  (ORGANIZATION Democrats/NNPS)\n",
      "  and/CC\n",
      "  10/CD\n",
      "  %/NN\n",
      "  are/VBP\n",
      "  Republicans/NNPS\n",
      "  ./.\n",
      "  [/VB\n",
      "  573/CD\n",
      "  ]/JJ\n",
      "  (GPE New/NNP York/NNP City/NNP)\n",
      "  has/VBZ\n",
      "  not/RB\n",
      "  been/VBN\n",
      "  carried/VBN\n",
      "  by/IN\n",
      "  a/DT\n",
      "  (ORGANIZATION Republican/NNP)\n",
      "  in/IN\n",
      "  a/DT\n",
      "  statewide/NN\n",
      "  or/CC\n",
      "  presidential/JJ\n",
      "  election/NN\n",
      "  since/IN\n",
      "  President/NNP\n",
      "  (PERSON Calvin/NNP Coolidge/NNP)\n",
      "  won/VBD\n",
      "  the/DT\n",
      "  five/CD\n",
      "  boroughs/NNS\n",
      "  in/IN\n",
      "  1924/CD\n",
      "  ./.\n",
      "  In/IN\n",
      "  2012/CD\n",
      "  ,/,\n",
      "  (PERSON Democrat/NNP Barack/NNP Obama/NNP)\n",
      "  became/VBD\n",
      "  the/DT\n",
      "  first/JJ\n",
      "  presidential/JJ\n",
      "  candidate/NN\n",
      "  of/IN\n",
      "  any/DT\n",
      "  party/NN\n",
      "  to/TO\n",
      "  receive/VB\n",
      "  more/JJR\n",
      "  than/IN\n",
      "  80/CD\n",
      "  %/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  overall/JJ\n",
      "  vote/NN\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP City/NNP)\n",
      "  ,/,\n",
      "  sweeping/VBG\n",
      "  all/DT\n",
      "  five/CD\n",
      "  boroughs/NNS\n",
      "  ./.\n",
      "  Party/NNP\n",
      "  platforms/NNS\n",
      "  center/NN\n",
      "  on/IN\n",
      "  affordable/JJ\n",
      "  housing/NN\n",
      "  ,/,\n",
      "  education/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  economic/JJ\n",
      "  development/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  labor/NN\n",
      "  politics/NNS\n",
      "  are/VBP\n",
      "  of/IN\n",
      "  importance/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  city/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#tokenize, pos-tag and NER-tag input\n",
    "tokens = nltk.word_tokenize(txt)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(entities)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
