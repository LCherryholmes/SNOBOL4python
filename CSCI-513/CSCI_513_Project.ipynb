{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fko1aGQ3buMA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pprint import pprint, pformat\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zsrnv7WtVqV"
      },
      "outputs": [],
      "source": [
        "#!pip install Cython\n",
        "%load_ext cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaCMt0K9p5BY"
      },
      "outputs": [],
      "source": [
        "%%cython\n",
        "\n",
        "cdef int pos\n",
        "cdef bytes subject\n",
        "\n",
        "cdef void factor():\n",
        "    global pos, subject\n",
        "    if   subject[pos] == ord('+'): pos += 1; factor()\n",
        "    elif subject[pos] == ord('-'): pos += 1; factor()\n",
        "    elif subject[pos] == ord('('):\n",
        "        pos += 1\n",
        "        expr()\n",
        "        if subject[pos] == ord(')'): pos += 1\n",
        "        else: raise Exception(pos)\n",
        "    elif subject[pos] == ord('x'): pos += 1\n",
        "    elif subject[pos] == ord('y'): pos += 1\n",
        "    elif subject[pos] == ord('0'): pos += 1\n",
        "    elif subject[pos] == ord('1'): pos += 1\n",
        "    else: raise Exception(pos)\n",
        "\n",
        "cdef void term():\n",
        "    global pos, subject\n",
        "    factor()\n",
        "    while subject[pos] == ord('*') or subject[pos] == ord('/'):\n",
        "        pos += 1\n",
        "        term()\n",
        "\n",
        "cdef void expr():\n",
        "    global pos, subject\n",
        "    term()\n",
        "    while subject[pos] == ord('+') or subject[pos] == ord('-'):\n",
        "        pos += 1\n",
        "        expr()\n",
        "\n",
        "cdef void stmt():\n",
        "    global pos, subject\n",
        "    expr()\n",
        "    if subject[pos] != ord('\\n'):\n",
        "        raise Exception(pos)\n",
        "\n",
        "def parse_expression(s):\n",
        "    global pos; pos = 0\n",
        "    global subject; subject = f\"{s}\\n\".encode('ascii')\n",
        "    try: stmt()\n",
        "    except Exception as e:\n",
        "      return e.args[0]+1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgUwnme7YjFe"
      },
      "outputs": [],
      "source": [
        "total = None\n",
        "tokens = \"(x*-1+0/y)\"\n",
        "def expressions(length):\n",
        "    global total; total = 1\n",
        "    for toks in product(tokens, repeat=length):\n",
        "        expression = \"\".join(toks)\n",
        "        yield (parse_expression(expression), expression)\n",
        "        total += 1\n",
        "    total -= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g2lu8aYNeIJ"
      },
      "outputs": [],
      "source": [
        "for length in range(1, 8):\n",
        "    start = time.time_ns() // 1000\n",
        "    classes = [0] * (length+2)\n",
        "    for expr in expressions(length):\n",
        "        classes[expr[0]] += 1\n",
        "    finish = time.time_ns() // 1000\n",
        "    classes = [(c * 100.0 / total) for c in classes]\n",
        "    pprint([length, total, (finish - start) / 1000000.0, classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRzNj9C4UBhu"
      },
      "outputs": [],
      "source": [
        "token_map = {'(': 1, 'x': 2, '0': 3, '+': 4, '*': 5, '/': 6, '-': 7, '1': 8, 'y': 9, ')': 10}\n",
        "def EXPRESSIONS(length):\n",
        "    XS = []\n",
        "    for expr in expressions(length):\n",
        "        XS.append([expr[0]] + [token_map[x] for x in expr[1]])\n",
        "    return XS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X4k0PuhGqfC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "results = {}\n",
        "for length in range(1, 6):\n",
        "    columns = ['target'] + [f\"x{i}\" for i in range(1, length+1)]\n",
        "    data = pd.DataFrame(EXPRESSIONS(length), columns=columns)\n",
        "    X = data.iloc[:, 1:length+1]\n",
        "    y = data.iloc[:, 0]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=53, stratify=y\n",
        "    )\n",
        "    classifiers = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
        "        \"Support Vector Classifier\": SVC(),\n",
        "        \"Random Forest\": RandomForestClassifier(n_estimators=100)\n",
        "    }\n",
        "    results[length] = {}\n",
        "    for name, model in classifiers.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        results[length][name] = {\n",
        "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "            \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
        "            \"Recall\": recall_score(y_test, y_pred, average='weighted'),\n",
        "            \"F1 Score\": f1_score(y_test, y_pred, average='weighted'),\n",
        "            \"Classification Report\": classification_report(y_test, y_pred)\n",
        "        }\n",
        "    comparison = pd.DataFrame(\n",
        "    { model: {\n",
        "          \"Accuracy\": metrics[\"Accuracy\"],\n",
        "          \"Precision\": metrics[\"Precision\"],\n",
        "          \"Recall\": metrics[\"Recall\"],\n",
        "          \"F1 Score\": metrics[\"F1 Score\"]\n",
        "      } for model, metrics in results[length].items()\n",
        "    }).T\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(comparison)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    if False:\n",
        "        for model, metrics in reports[length].items():\n",
        "            print(f\"Classification Report: {model}:\")\n",
        "            print(metrics[\"Classification Report\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "hOzjReG-NKgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "for length in range(1, 7):\n",
        "    data = np.array(EXPRESSIONS(length))\n",
        "    X = data[:, 1:].astype(np.int64) # tokens (shape: [n_samples, 5])\n",
        "    y = data[:, 0].astype(np.int64) # target column\n",
        "    num_classes = int(y.max()) + 1\n",
        "    seq_length = X.shape[1] # should be 5 as specified\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=53)\n",
        "    batch_size = 32\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.tensor(X_train, dtype=torch.long),\n",
        "        torch.tensor(y_train, dtype=torch.long))\n",
        "    test_dataset = TensorDataset(\n",
        "        torch.tensor(X_test, dtype=torch.long),\n",
        "        torch.tensor(y_test, dtype=torch.long))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    vocab_size = 11 # tokens 1-10\n",
        "    embedding_dim = 8 # arbitrary embedding dimension\n",
        "    hidden_dim = 32 # used by hidden layers / RNN\n",
        "    LEARNING_RATE = 0.001\n",
        "    EPOCHS = 20\n",
        "\n",
        "    # Model 1: Logistic Regression (a single linear layer after embedding+flattening)\n",
        "    class LogisticRegressionModel(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, num_classes, seq_length):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "            self.fc = nn.Linear(seq_length * embedding_dim, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # x: [batch_size, seq_length]\n",
        "            x = self.embedding(x) # shape: [batch_size, seq_length, embedding_dim]\n",
        "            x = x.view(x.size(0), -1) # flatten to [batch_size, seq_length*embedding_dim]\n",
        "            out = self.fc(x)\n",
        "            return out\n",
        "\n",
        "    # Model 2: Feedforward network with one hidden layer\n",
        "    class FeedforwardNN(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, num_classes, seq_length, hidden_dim):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "            self.fc1 = nn.Linear(seq_length * embedding_dim, hidden_dim)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.embedding(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.relu(self.fc1(x))\n",
        "            out = self.fc2(x)\n",
        "            return out\n",
        "\n",
        "    # Model 3: Deeper network with two hidden layers\n",
        "    class DeeperNN(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, num_classes, seq_length, hidden_dim):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "            self.fc1 = nn.Linear(seq_length * embedding_dim, hidden_dim)\n",
        "            self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
        "            self.relu = nn.ReLU()\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.embedding(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.relu(self.fc1(x))\n",
        "            x = self.relu(self.fc2(x))\n",
        "            out = self.fc3(x)\n",
        "            return out\n",
        "\n",
        "    # Model 4: RNN using an LSTM (the hidden state of the last time step is used for classification)\n",
        "    class RNNModel(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, num_classes, hidden_dim, num_layers=1):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "            self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.embedding(x) # [batch, seq_length, embedding_dim]\n",
        "            out, (hn, cn) = self.lstm(x) # hn: [num_layers, batch, hidden_dim]\n",
        "            out = self.fc(hn[-1]) # use last layer’s hidden state for classification\n",
        "            return out\n",
        "\n",
        "    # Model 5: Transformer-based Model (using a single encoder layer)\n",
        "    class TransformerModel(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, num_classes, seq_length, nhead=2, num_encoder_layers=1, dim_feedforward=64):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "            # Learnable positional encoding (for simplicity)\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(1, seq_length, embedding_dim))\n",
        "            encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dim_feedforward=dim_feedforward)\n",
        "            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "            self.fc = nn.Linear(seq_length * embedding_dim, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.embedding(x) + self.pos_embedding # [batch, seq_length, embedding_dim]\n",
        "            x = x.transpose(0, 1) # Transformer expects: [seq_length, batch, embedding_dim]\n",
        "            x = self.transformer_encoder(x)\n",
        "            x = x.transpose(0, 1) # back to [batch, seq_length, embedding_dim]\n",
        "            x = x.reshape(x.size(0), -1) # flatten\n",
        "            out = self.fc(x)\n",
        "            return out\n",
        "\n",
        "    # Model 6: Convolutional Neural Network (CNN) with 1D Convolutions\n",
        "    class CNNModel(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, num_classes, seq_length, num_filters=16, kernel_size=2):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "            # For conv1d, we expect input of shape (batch, channels, seq_length)\n",
        "            self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=kernel_size)\n",
        "            conv_output_size = seq_length - kernel_size + 1\n",
        "            self.fc = nn.Linear(num_filters * conv_output_size, num_classes)\n",
        "            self.relu = nn.ReLU()\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.embedding(x) # [batch, seq_length, embedding_dim]\n",
        "            x = x.transpose(1, 2) # [batch, embedding_dim, seq_length]\n",
        "            x = self.conv1(x) # [batch, num_filters, new_seq_length]\n",
        "            x = self.relu(x)\n",
        "            x = x.view(x.size(0), -1) # flatten\n",
        "            out = self.fc(x)\n",
        "            return out\n",
        "\n",
        "    def train_model(model, train_loader, criterion, optimizer, device, epochs=20):\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            for batch_x, batch_y in train_loader:\n",
        "                batch_x = batch_x.to(device)\n",
        "                batch_y = batch_y.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_loss = total_loss / len(train_loader)\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def evaluate_model(model, test_loader, device):\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in test_loader:\n",
        "                batch_x = batch_x.to(device)\n",
        "                outputs = model(batch_x)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.append(preds.cpu())\n",
        "                all_labels.append(batch_y)\n",
        "        all_preds = torch.cat(all_preds).numpy()\n",
        "        all_labels = torch.cat(all_labels).numpy()\n",
        "        return all_preds, all_labels\n",
        "\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegressionModel(vocab_size, embedding_dim, num_classes, seq_length),\n",
        "        \"Feedforward NN\": FeedforwardNN(vocab_size, embedding_dim, num_classes, seq_length, hidden_dim),\n",
        "        \"Deeper NN\": DeeperNN(vocab_size, embedding_dim, num_classes, seq_length, hidden_dim),\n",
        "        \"RNN (LSTM)\": RNNModel(vocab_size, embedding_dim, num_classes, hidden_dim),\n",
        "        \"Transformer\": TransformerModel(vocab_size, embedding_dim, num_classes, seq_length),\n",
        "        \"CNN\": CNNModel(vocab_size, embedding_dim, num_classes, seq_length)\n",
        "    }\n",
        "\n",
        "    results_dict = {}\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"Training model: {model_name}\")\n",
        "        model.to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "        train_model(model, train_loader, criterion, optimizer, device, epochs=EPOCHS)\n",
        "        preds, labels = evaluate_model(model, test_loader, device)\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        prec = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "        rec = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "        report = classification_report(labels, preds, zero_division=0)\n",
        "        results_dict[model_name] = {\n",
        "            \"Accuracy\": acc,\n",
        "            \"Precision\": prec,\n",
        "            \"Recall\": rec,\n",
        "            \"F1 Score\": f1,\n",
        "            \"Classification Report\": report\n",
        "        }\n",
        "        print(f\"Classification report for {model_name}:\\n{report}\")\n",
        "\n",
        "    summary_data = []\n",
        "    for name, metrics in results_dict.items():\n",
        "        summary_data.append({\n",
        "            \"Model\": name,\n",
        "            \"Accuracy\": metrics[\"Accuracy\"],\n",
        "            \"Precision\": metrics[\"Precision\"],\n",
        "            \"Recall\": metrics[\"Recall\"],\n",
        "            \"F1 Score\": metrics[\"F1 Score\"]\n",
        "        })\n",
        "\n",
        "    summary = pd.DataFrame(summary_data)\n",
        "    print(\"Summary of Evaluation Metrics:\")\n",
        "    print(summary)"
      ],
      "metadata": {
        "id": "h7PbdwwzZQSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}